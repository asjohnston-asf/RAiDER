{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"RAiDER Raytracing Atmospheric Delay Estimation for RADAR RAiDER-tools is a package in Python which contains tools to calculate tropospheric corrections for Radar using a raytracing implementation. Its development was funded under the NASA Sea-level Change Team (NSLCT) program, the Earth Surface and Interior (ESI) program, and the NISAR Science Team (NISAR-ST) (NTR-51433). U.S. Government sponsorship acknowledged. Copyright (c) 2019-2022, California Institute of Technology (\"Caltech\"). All rights reserved. THIS IS RESEARCH CODE PROVIDED TO YOU \"AS IS\" WITH NO WARRANTIES OF CORRECTNESS. USE AT YOUR OWN RISK. Contents Getting Started Installing With Conda Using the Docker Image Installing from Source Setup of third party weather model access Running RAiDER and Documentation Citing Development Contributors 1. Getting Started RAiDER has been tested on the following systems: - Ubuntu v.16 and up - Mac OS v.10 and up RAiDER does not currently run on arm64 processors on Mac. We will update this note once the build becomes available. Installing With Conda RAiDER is available on conda-forge . Conda is a cross-platform way to use Python that allows you to setup and use \"virtual environments.\" These can help to keep dependencies for different sets of code separate. We recommend using Miniforge , a conda environment manager that uses conda-forge as its default code repo. Alternatively,see here for help installing Anaconda and here for installing Miniconda. Installing RAiDER: conda env create --name RAiDER -c conda-forge raider conda activate RAiDER Using the Docker image RAiDER provides a docker container image with all the necessary dependencies pre-installed. To get the latest released version: docker pull ghcr.io/dbekaert/raider:latest a specific release version (>=v0.2.0 only): docker pull ghcr.io/dbekaert/raider:0.2.0 or the current development version: docker pull ghcr.io/dbekaert/raider:test To run the container and jump into a bash shell inside: docker run -it --rm ghcr.io/dbekaert/raider:latest To mount your current directory inside the container so that files will be written back to your local machine: docker run -it -v ${PWD}:/home/raider/work --rm ghcr.io/dbekaert/raider:latest cd work For more docker run options, see: https://docs.docker.com/engine/reference/run/ . 2. Setup of third party weather model access RAiDER has the ability to download weather models from third-parties; some of which require license agreements. See here for details. 3. Running RAiDER and Documentation For detailed documentation, examples, and Jupyter notebooks see the RAiDER-docs repository . We welcome contributions of other examples on how to leverage the RAiDER (see here for instructions). raiderDelay.py -h provides a help menu and list of example commands to get started. The RAiDER scripts are highly modularized in Python and allows for building your own processing workflow. 4. Citation TODO 5. Development Contributions are welcome and heartily encourage! See our contributing guide . Development install For development, we recommend installing directly from source. git clone https://github.com/dbekaert/RAiDER.git cd RAiDER conda create -f environment.yml conda activate RAiDER python -m pip install -e . For more details on installing from source see here . Contributors David Bekaert Jeremy Maurer Raymond Hogenson Piyush Agram (Descartes Labs) Yang Lei Rohan Weeden Simran Sangha other community members We welcome community contributions! For instructions see here .","title":"Home page"},{"location":"#raider","text":"Raytracing Atmospheric Delay Estimation for RADAR RAiDER-tools is a package in Python which contains tools to calculate tropospheric corrections for Radar using a raytracing implementation. Its development was funded under the NASA Sea-level Change Team (NSLCT) program, the Earth Surface and Interior (ESI) program, and the NISAR Science Team (NISAR-ST) (NTR-51433). U.S. Government sponsorship acknowledged. Copyright (c) 2019-2022, California Institute of Technology (\"Caltech\"). All rights reserved. THIS IS RESEARCH CODE PROVIDED TO YOU \"AS IS\" WITH NO WARRANTIES OF CORRECTNESS. USE AT YOUR OWN RISK.","title":"RAiDER"},{"location":"#contents","text":"Getting Started Installing With Conda Using the Docker Image Installing from Source Setup of third party weather model access Running RAiDER and Documentation Citing Development Contributors","title":"Contents"},{"location":"#1-getting-started","text":"RAiDER has been tested on the following systems: - Ubuntu v.16 and up - Mac OS v.10 and up RAiDER does not currently run on arm64 processors on Mac. We will update this note once the build becomes available.","title":"1. Getting Started"},{"location":"#installing-with-conda","text":"RAiDER is available on conda-forge . Conda is a cross-platform way to use Python that allows you to setup and use \"virtual environments.\" These can help to keep dependencies for different sets of code separate. We recommend using Miniforge , a conda environment manager that uses conda-forge as its default code repo. Alternatively,see here for help installing Anaconda and here for installing Miniconda. Installing RAiDER: conda env create --name RAiDER -c conda-forge raider conda activate RAiDER","title":"Installing With Conda"},{"location":"#using-the-docker-image","text":"RAiDER provides a docker container image with all the necessary dependencies pre-installed. To get the latest released version: docker pull ghcr.io/dbekaert/raider:latest a specific release version (>=v0.2.0 only): docker pull ghcr.io/dbekaert/raider:0.2.0 or the current development version: docker pull ghcr.io/dbekaert/raider:test To run the container and jump into a bash shell inside: docker run -it --rm ghcr.io/dbekaert/raider:latest To mount your current directory inside the container so that files will be written back to your local machine: docker run -it -v ${PWD}:/home/raider/work --rm ghcr.io/dbekaert/raider:latest cd work For more docker run options, see: https://docs.docker.com/engine/reference/run/ .","title":"Using the Docker image"},{"location":"#2-setup-of-third-party-weather-model-access","text":"RAiDER has the ability to download weather models from third-parties; some of which require license agreements. See here for details.","title":"2. Setup of third party weather model access"},{"location":"#3-running-raider-and-documentation","text":"For detailed documentation, examples, and Jupyter notebooks see the RAiDER-docs repository . We welcome contributions of other examples on how to leverage the RAiDER (see here for instructions). raiderDelay.py -h provides a help menu and list of example commands to get started. The RAiDER scripts are highly modularized in Python and allows for building your own processing workflow.","title":"3. Running RAiDER and Documentation"},{"location":"#4-citation","text":"TODO","title":"4. Citation"},{"location":"#5-development","text":"Contributions are welcome and heartily encourage! See our contributing guide .","title":"5. Development"},{"location":"#development-install","text":"For development, we recommend installing directly from source. git clone https://github.com/dbekaert/RAiDER.git cd RAiDER conda create -f environment.yml conda activate RAiDER python -m pip install -e . For more details on installing from source see here .","title":"Development install"},{"location":"#contributors","text":"David Bekaert Jeremy Maurer Raymond Hogenson Piyush Agram (Descartes Labs) Yang Lei Rohan Weeden Simran Sangha other community members We welcome community contributions! For instructions see here .","title":"Contributors"},{"location":"Installing_from_source/","text":"Common Installation Issues This package uses GDAL and g++, both of which can be tricky to set up correctly. GDAL in particular will often break after installing a new program If you receive error messages such as the following: ImportError: ~/anaconda3/envs/RAiDER/lib/python3.7/site-packages/matplotlib/../../../libstdc++.so.6: version `CXXABI_1.3.9' not found (required by ~/anaconda3/envs/RAiDER/lib/python3.7/site-packages/matplotlib/ft2font.cpython-37m-x86_64-linux-gnu.so) ImportError: libtiledb.so.1.6.0: cannot open shared object file: No such file or directory ***cmake: ~/anaconda3/envs/RAiDER/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by cmake)*** try running the following commands within your RAiDER conda environment: conda update --force-reinstall libstdcxx-ng conda update --force-reinstall gdal libgdal This package requires both C++ and C headers, and the system headers are used for some C libraries. If running on a Mac computer, and \"python setup.py build\" results in a message stating that some system library header file is missing, try the following steps, and accept the various licenses and step through the installation process. Try re-running the build step after each update: xcode-select --install open /Library/Developer/CommandLineTools/Packages/macOS_SDK_headers_for_macOS_10.14.pkg Testing your installation To test the installation was successfull you can run the following tests: py.test test/ raiderDelay.py -h To enable automatic CircleCI Tests from a pull requests You will need to make sure that CircleCI is an authorized OAuth application from Github. Simply sign in here using your github account.","title":"Installing from source"},{"location":"Installing_from_source/#common-installation-issues","text":"This package uses GDAL and g++, both of which can be tricky to set up correctly. GDAL in particular will often break after installing a new program If you receive error messages such as the following: ImportError: ~/anaconda3/envs/RAiDER/lib/python3.7/site-packages/matplotlib/../../../libstdc++.so.6: version `CXXABI_1.3.9' not found (required by ~/anaconda3/envs/RAiDER/lib/python3.7/site-packages/matplotlib/ft2font.cpython-37m-x86_64-linux-gnu.so) ImportError: libtiledb.so.1.6.0: cannot open shared object file: No such file or directory ***cmake: ~/anaconda3/envs/RAiDER/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by cmake)*** try running the following commands within your RAiDER conda environment: conda update --force-reinstall libstdcxx-ng conda update --force-reinstall gdal libgdal This package requires both C++ and C headers, and the system headers are used for some C libraries. If running on a Mac computer, and \"python setup.py build\" results in a message stating that some system library header file is missing, try the following steps, and accept the various licenses and step through the installation process. Try re-running the build step after each update: xcode-select --install open /Library/Developer/CommandLineTools/Packages/macOS_SDK_headers_for_macOS_10.14.pkg","title":"Common Installation Issues"},{"location":"Installing_from_source/#testing-your-installation","text":"To test the installation was successfull you can run the following tests: py.test test/ raiderDelay.py -h","title":"Testing your installation"},{"location":"Installing_from_source/#to-enable-automatic-circleci-tests-from-a-pull-requests","text":"You will need to make sure that CircleCI is an authorized OAuth application from Github. Simply sign in here using your github account.","title":"To enable automatic CircleCI Tests from a pull requests"},{"location":"WeatherModels/","text":"Accessing weather model data RAiDER has built-in support for a number of different weather models. RAiDER provides all the interfacing to data servers required to access data for the different weather models, although some weather models require a license agreement and accounts to be set-up. Instructions for accessing data, including license-limited data, are provided below. It is the user's responsibility to accept license agreements for whatever model is desired. In addition, RAiDER provides functionality for adding additional weather models. See the RAiDER-docs repository page on how to do this. We would love to expand the suite of supported models, and welcome any contributions. Please see the contributing guidelines or reach out through an issue ticket for help. 1. Usage RAiDER . processWM . prepareWeatherModel ( weather_model , time = None , wmLoc = None , ll_bounds = None , download_only = False , makePlots = False , force_download = False ) Parse inputs to download and prepare a weather model grid for interpolation Parameters: Name Type Description Default weather_model WeatherModel - instantiated weather model object required time datetime - Python datetime to request. Will be rounded to nearest available time None wmLoc str str - file path to which to write weather model file(s) None ll_bounds List [ float ] list of float - bounding box to download in [S, N, W, E] format None download_only bool bool - False if preprocessing weather model data False makePlots bool bool - whether to write debug plots False force_download bool bool - True if you want to download even when the weather model exists False Returns: Name Type Description str str filename of the netcdf file to which the weather model has been written Source code in RAiDER/processWM.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def prepareWeatherModel ( weather_model , time = None , wmLoc : str = None , ll_bounds : List [ float ] = None , download_only : bool = False , makePlots : bool = False , force_download : bool = False , ) -> str : \"\"\"Parse inputs to download and prepare a weather model grid for interpolation Args: weather_model: WeatherModel - instantiated weather model object time: datetime - Python datetime to request. Will be rounded to nearest available time wmLoc: str - file path to which to write weather model file(s) ll_bounds: list of float - bounding box to download in [S, N, W, E] format download_only: bool - False if preprocessing weather model data makePlots: bool - whether to write debug plots force_download: bool - True if you want to download even when the weather model exists Returns: str: filename of the netcdf file to which the weather model has been written \"\"\" # Ensure the file output location exists if wmLoc is None : wmLoc = os . path . join ( os . getcwd (), 'weather_files' ) os . makedirs ( wmLoc , exist_ok = True ) # check whether weather model files are supplied or should be downloaded download_flag = True if weather_model . files is None : if time is None : raise RuntimeError ( 'prepareWeatherModel: Either a file or a time must be specified' ) weather_model . filename ( time , wmLoc ) if os . path . exists ( weather_model . files [ 0 ]): if not force_download : logger . warning ( 'Weather model already exists, please remove it (\" %s \") if you want ' 'to download a new one.' , weather_model . files ) download_flag = False else : download_flag = False # if no weather model files supplied, check the standard location if download_flag : weather_model . fetch ( * weather_model . files , ll_bounds , time ) else : time = getTimeFromFile ( weather_model . files [ 0 ]) weather_model . setTime ( time ) containment = weather_model . checkContainment ( ll_bounds ) if not containment : logger . warning ( 'The weather model passed does not cover all of the input ' 'points; you may need to download a larger area.' ) # If only downloading, exit now if download_only : logger . warning ( 'download_only flag selected. No further processing will happen.' ) return None # Otherwise, load the weather model data f = weather_model . load ( wmLoc , ll_bounds = ll_bounds , ) if f is not None : logger . warning ( 'The processed weather model file already exists,' ' so I will use that.' ) return f # Logging some basic info logger . debug ( 'Number of weather model nodes: {} ' . format ( np . prod ( weather_model . getWetRefractivity () . shape ) ) ) shape = weather_model . getWetRefractivity () . shape logger . debug ( f 'Shape of weather model: { shape } ' ) logger . debug ( 'Bounds of the weather model: %.2f / %.2f / %.2f / %.2f (SNWE)' , np . nanmin ( weather_model . _ys ), np . nanmax ( weather_model . _ys ), np . nanmin ( weather_model . _xs ), np . nanmax ( weather_model . _xs ) ) logger . debug ( 'Weather model: %s ' , weather_model . Model ()) logger . debug ( 'Mean value of the wet refractivity: %f ' , np . nanmean ( weather_model . getWetRefractivity ()) ) logger . debug ( 'Mean value of the hydrostatic refractivity: %f ' , np . nanmean ( weather_model . getHydroRefractivity ()) ) logger . debug ( weather_model ) if makePlots : weather_model . plot ( 'wh' , True ) weather_model . plot ( 'pqt' , True ) plt . close ( 'all' ) try : f = weather_model . write () return f except Exception as e : logger . exception ( \"Unable to save weathermodel to file\" ) logger . exception ( e ) raise RuntimeError ( \"Unable to save weathermodel to file\" ) finally : del weather_model Potential download failure ERA-5/ERA-I products require access to the ESA Copernicus servers. GMAO and MERRA-2 products require access to the NASA Earthdata servers. If you are unable to download products, ensure that you have registered and have downloaded the public API key, and accepted/added the license/application for type of product you wish to download as detailed below. 2. NOAA weather models (HRRR) High-resolution rapid refresh (HRRR) weather model data products are generated by NOAA for the coninental US (CONUS) but not archived beyond three days. However, a public archive is available at the University of Utah. This archive does not require a license agreement. This model has the highest spatial resolution available in RAiDER, with a horizontal grid spacing of about 3 km, and is provided in a Lambert conformal conic projection. 3. ECMWF weather models (ERA5, ERA5T, ERAI, HRES) The Copernicus Climate Data Store (CDS) provides access to the European Centre for Medium-Range Weather Forecasts ( ECMWF ) provides a number of different weather models, including ERA5 and ERA5T reanalysis models. The ECMWF provides access to both reanalysis and real-time prediction models. You can read more information about their reanalysis models here and real-time model here . ECMWF models are global, with horizontal resolution of about 30 km for ERA-I, ERA-5, and ERA-5T, and 6 km for Hi-RES. All of these models come in a global projection (EPSG 4326, WGS-84). Accessing ERA5 and ERA5T weather reanalysis products from Copernicus CDS Create an account on the Copernicus servers here Confirm your email, etc. Install the public API key and client as instructed here : a. Copy the URL and API key from the webpage into a file in your home directory name ~/.cdsapirc url: https://cds.climate.copernicus.eu/api/v2 key: your_key_here Note : the key represents the API key obtained upon the registration of CDS API, and should be replaced with the user's own information. b. Install the CDS API using pip pip install cdsapi Note : this step has been included in the conda install of RAiDER, thus can be omitted if one uses the recommended conda install of RAiDER You must accept the license for each product you wish to download. Accessing ERAI, HRES ECMWF requires a license agreement to be able to access, download, and use their products. Instructions for completing this process is below. Create an account on the ECMWF servers here . The ERA-I model is open-access, while HRES requires a special licence agreement. Confirm your email, etc. Install the public API key and client as instructed here : a. Copy the URL and API key from the webpage into a file in your home directory name ~/.ecmwfapirc { \"url\" : \"https://api.ecmwf.int/v1\", \"key\" : your key here, \"email\" : your email here } Note : the email that is used to register the user account, and the key represents the API key obtained upon the registration of ECMWF API, and should be replaced with the user's own information. b. Install the ECMWF API using pip: pip install ecmwf-api-client` Note : this step has been included in the conda install of RAiDER, thus can be omitted if one uses the recommended conda install of RAiDER 4. NASA weather models (GMAO, MERRA2) The Global Modeling and Assimilation Office ( GMAO ) at NASA generates reanalysis weather models. GMAO products can also be accessed without a license agreement through the pyDAP interface implemented in RAiDER. GMAO has a horizontal grid spacing of approximately 33 km, and its projection is EPSG code 4326 (WGS-84). The Modern-Era Retrospective analysis for Research and Applications, Version 2 ( MERRA-2 ) provides data beginning in 1980. MERRA-2 is also produced by NASA and has a spatial resolution of about 50 km and a global projection (EPSG 4326, WGS-84). Reference: The Modern-Era Retrospective Analysis for Research and Applications, Version 2 (MERRA-2) , Ronald Gelaro, et al., 2017, J. Clim., doi: 10.1175/JCLI-D-16-0758.1 Accessing NASA weather model data Create an account on the NASA's Earthdata website here Confirm your email, etc. Copy the login username and password to a file in your home directory name ~/.netrc machine urs.earthdata.nasa.gov login <USERNAME> password <PASSWORD> Note : the username and password represent the user's username and password. Add the application NASA GESDISC DATA ARCHIVE by clicking on the Applications->Authorized Apps on the menu after logging into your Earthdata profile, and then scrolling down to the application NASA GESDISC DATA ARCHIVE to approve it. This seems not required for GMAO for now, but recommended to do so for all OpenDAP-based weather models. Install the OpenDAP using pip: pip install pydap==3.2.1 Note : this step has been included in the conda install of RAiDER, thus can be omitted if one uses the recommended conda install of RAiDER Note : PyDAP v3.2.1 is required for now (thus specified in the above pip install command) because the latest v3.2.2 (as of now) has a known bug in accessing and slicing the GMAO data. This bug is expected to be fixed in newer versions of PyDAP.","title":"Weather Model Access"},{"location":"WeatherModels/#accessing-weather-model-data","text":"RAiDER has built-in support for a number of different weather models. RAiDER provides all the interfacing to data servers required to access data for the different weather models, although some weather models require a license agreement and accounts to be set-up. Instructions for accessing data, including license-limited data, are provided below. It is the user's responsibility to accept license agreements for whatever model is desired. In addition, RAiDER provides functionality for adding additional weather models. See the RAiDER-docs repository page on how to do this. We would love to expand the suite of supported models, and welcome any contributions. Please see the contributing guidelines or reach out through an issue ticket for help.","title":"Accessing weather model data"},{"location":"WeatherModels/#1-usage","text":"","title":"1. Usage"},{"location":"WeatherModels/#RAiDER.processWM.prepareWeatherModel","text":"Parse inputs to download and prepare a weather model grid for interpolation Parameters: Name Type Description Default weather_model WeatherModel - instantiated weather model object required time datetime - Python datetime to request. Will be rounded to nearest available time None wmLoc str str - file path to which to write weather model file(s) None ll_bounds List [ float ] list of float - bounding box to download in [S, N, W, E] format None download_only bool bool - False if preprocessing weather model data False makePlots bool bool - whether to write debug plots False force_download bool bool - True if you want to download even when the weather model exists False Returns: Name Type Description str str filename of the netcdf file to which the weather model has been written Source code in RAiDER/processWM.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def prepareWeatherModel ( weather_model , time = None , wmLoc : str = None , ll_bounds : List [ float ] = None , download_only : bool = False , makePlots : bool = False , force_download : bool = False , ) -> str : \"\"\"Parse inputs to download and prepare a weather model grid for interpolation Args: weather_model: WeatherModel - instantiated weather model object time: datetime - Python datetime to request. Will be rounded to nearest available time wmLoc: str - file path to which to write weather model file(s) ll_bounds: list of float - bounding box to download in [S, N, W, E] format download_only: bool - False if preprocessing weather model data makePlots: bool - whether to write debug plots force_download: bool - True if you want to download even when the weather model exists Returns: str: filename of the netcdf file to which the weather model has been written \"\"\" # Ensure the file output location exists if wmLoc is None : wmLoc = os . path . join ( os . getcwd (), 'weather_files' ) os . makedirs ( wmLoc , exist_ok = True ) # check whether weather model files are supplied or should be downloaded download_flag = True if weather_model . files is None : if time is None : raise RuntimeError ( 'prepareWeatherModel: Either a file or a time must be specified' ) weather_model . filename ( time , wmLoc ) if os . path . exists ( weather_model . files [ 0 ]): if not force_download : logger . warning ( 'Weather model already exists, please remove it (\" %s \") if you want ' 'to download a new one.' , weather_model . files ) download_flag = False else : download_flag = False # if no weather model files supplied, check the standard location if download_flag : weather_model . fetch ( * weather_model . files , ll_bounds , time ) else : time = getTimeFromFile ( weather_model . files [ 0 ]) weather_model . setTime ( time ) containment = weather_model . checkContainment ( ll_bounds ) if not containment : logger . warning ( 'The weather model passed does not cover all of the input ' 'points; you may need to download a larger area.' ) # If only downloading, exit now if download_only : logger . warning ( 'download_only flag selected. No further processing will happen.' ) return None # Otherwise, load the weather model data f = weather_model . load ( wmLoc , ll_bounds = ll_bounds , ) if f is not None : logger . warning ( 'The processed weather model file already exists,' ' so I will use that.' ) return f # Logging some basic info logger . debug ( 'Number of weather model nodes: {} ' . format ( np . prod ( weather_model . getWetRefractivity () . shape ) ) ) shape = weather_model . getWetRefractivity () . shape logger . debug ( f 'Shape of weather model: { shape } ' ) logger . debug ( 'Bounds of the weather model: %.2f / %.2f / %.2f / %.2f (SNWE)' , np . nanmin ( weather_model . _ys ), np . nanmax ( weather_model . _ys ), np . nanmin ( weather_model . _xs ), np . nanmax ( weather_model . _xs ) ) logger . debug ( 'Weather model: %s ' , weather_model . Model ()) logger . debug ( 'Mean value of the wet refractivity: %f ' , np . nanmean ( weather_model . getWetRefractivity ()) ) logger . debug ( 'Mean value of the hydrostatic refractivity: %f ' , np . nanmean ( weather_model . getHydroRefractivity ()) ) logger . debug ( weather_model ) if makePlots : weather_model . plot ( 'wh' , True ) weather_model . plot ( 'pqt' , True ) plt . close ( 'all' ) try : f = weather_model . write () return f except Exception as e : logger . exception ( \"Unable to save weathermodel to file\" ) logger . exception ( e ) raise RuntimeError ( \"Unable to save weathermodel to file\" ) finally : del weather_model","title":"prepareWeatherModel()"},{"location":"WeatherModels/#potential-download-failure","text":"ERA-5/ERA-I products require access to the ESA Copernicus servers. GMAO and MERRA-2 products require access to the NASA Earthdata servers. If you are unable to download products, ensure that you have registered and have downloaded the public API key, and accepted/added the license/application for type of product you wish to download as detailed below.","title":"Potential download failure"},{"location":"WeatherModels/#2-noaa-weather-models-hrrr","text":"High-resolution rapid refresh (HRRR) weather model data products are generated by NOAA for the coninental US (CONUS) but not archived beyond three days. However, a public archive is available at the University of Utah. This archive does not require a license agreement. This model has the highest spatial resolution available in RAiDER, with a horizontal grid spacing of about 3 km, and is provided in a Lambert conformal conic projection.","title":"2. NOAA weather models (HRRR)"},{"location":"WeatherModels/#3-ecmwf-weather-models-era5-era5t-erai-hres","text":"The Copernicus Climate Data Store (CDS) provides access to the European Centre for Medium-Range Weather Forecasts ( ECMWF ) provides a number of different weather models, including ERA5 and ERA5T reanalysis models. The ECMWF provides access to both reanalysis and real-time prediction models. You can read more information about their reanalysis models here and real-time model here . ECMWF models are global, with horizontal resolution of about 30 km for ERA-I, ERA-5, and ERA-5T, and 6 km for Hi-RES. All of these models come in a global projection (EPSG 4326, WGS-84).","title":"3. ECMWF weather models (ERA5, ERA5T, ERAI, HRES)"},{"location":"WeatherModels/#accessing-era5-and-era5t-weather-reanalysis-products-from-copernicus-cds","text":"Create an account on the Copernicus servers here Confirm your email, etc. Install the public API key and client as instructed here : a. Copy the URL and API key from the webpage into a file in your home directory name ~/.cdsapirc url: https://cds.climate.copernicus.eu/api/v2 key: your_key_here Note : the key represents the API key obtained upon the registration of CDS API, and should be replaced with the user's own information. b. Install the CDS API using pip pip install cdsapi Note : this step has been included in the conda install of RAiDER, thus can be omitted if one uses the recommended conda install of RAiDER You must accept the license for each product you wish to download.","title":"Accessing ERA5 and ERA5T weather reanalysis products from Copernicus CDS"},{"location":"WeatherModels/#accessing-erai-hres","text":"ECMWF requires a license agreement to be able to access, download, and use their products. Instructions for completing this process is below. Create an account on the ECMWF servers here . The ERA-I model is open-access, while HRES requires a special licence agreement. Confirm your email, etc. Install the public API key and client as instructed here : a. Copy the URL and API key from the webpage into a file in your home directory name ~/.ecmwfapirc { \"url\" : \"https://api.ecmwf.int/v1\", \"key\" : your key here, \"email\" : your email here } Note : the email that is used to register the user account, and the key represents the API key obtained upon the registration of ECMWF API, and should be replaced with the user's own information. b. Install the ECMWF API using pip: pip install ecmwf-api-client` Note : this step has been included in the conda install of RAiDER, thus can be omitted if one uses the recommended conda install of RAiDER","title":"Accessing ERAI, HRES"},{"location":"WeatherModels/#4-nasa-weather-models-gmao-merra2","text":"The Global Modeling and Assimilation Office ( GMAO ) at NASA generates reanalysis weather models. GMAO products can also be accessed without a license agreement through the pyDAP interface implemented in RAiDER. GMAO has a horizontal grid spacing of approximately 33 km, and its projection is EPSG code 4326 (WGS-84). The Modern-Era Retrospective analysis for Research and Applications, Version 2 ( MERRA-2 ) provides data beginning in 1980. MERRA-2 is also produced by NASA and has a spatial resolution of about 50 km and a global projection (EPSG 4326, WGS-84). Reference: The Modern-Era Retrospective Analysis for Research and Applications, Version 2 (MERRA-2) , Ronald Gelaro, et al., 2017, J. Clim., doi: 10.1175/JCLI-D-16-0758.1","title":"4. NASA weather models (GMAO, MERRA2)"},{"location":"WeatherModels/#accessing-nasa-weather-model-data","text":"Create an account on the NASA's Earthdata website here Confirm your email, etc. Copy the login username and password to a file in your home directory name ~/.netrc machine urs.earthdata.nasa.gov login <USERNAME> password <PASSWORD> Note : the username and password represent the user's username and password. Add the application NASA GESDISC DATA ARCHIVE by clicking on the Applications->Authorized Apps on the menu after logging into your Earthdata profile, and then scrolling down to the application NASA GESDISC DATA ARCHIVE to approve it. This seems not required for GMAO for now, but recommended to do so for all OpenDAP-based weather models. Install the OpenDAP using pip: pip install pydap==3.2.1 Note : this step has been included in the conda install of RAiDER, thus can be omitted if one uses the recommended conda install of RAiDER Note : PyDAP v3.2.1 is required for now (thus specified in the above pip install command) because the latest v3.2.2 (as of now) has a known bug in accessing and slicing the GMAO data. This bug is expected to be fixed in newer versions of PyDAP.","title":"Accessing NASA weather model data"},{"location":"reference/","text":"RAiDER v0.3.0 API Reference Raytracing Atmospheric Delay Estimation for RADAR Copyright (c) 2019-2022, California Institute of Technology (\"Caltech\"). All rights reserved. checkArgs checkArgs ( args ) Helper fcn for checking argument compatibility and returns the correct variables Source code in RAiDER/checkArgs.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def checkArgs ( args ): ''' Helper fcn for checking argument compatibility and returns the correct variables ''' ######################################################################################################################### # Directories if not os . path . exists ( args . weather_model_directory ): os . mkdir ( args . weather_model_directory ) ######################################################################################################################### # Date and Time parsing args . date_list = [ datetime . combine ( d , args . time ) for d in args . date_list ] ######################################################################################################################### # filenames wetNames , hydroNames = [], [] for d in args . date_list : if ( args . aoi . type () != 'bounding_box' ): # Handle the GNSS station file if ( args . aoi . type () == 'station_file' ): wetFilename = os . path . join ( args . output_directory , ' {} _Delay_ {} .csv' . format ( args . weather_model . Model (), d . strftime ( '%Y%m %d T%H%M%S' ), ) ) hydroFilename = None # only the 'wetFilename' is used for the station_file # copy the input station file to the output location for editing indf = pd . read_csv ( args . aoi . _filename ) . drop_duplicates ( subset = [ \"Lat\" , \"Lon\" ]) indf . to_csv ( wetFilename , index = False ) else : # This implies rasters fmt = get_raster_ext ( args . file_format ) wetFilename , hydroFilename = makeDelayFileNames ( d , args . los , fmt , args . weather_model . _dataset . upper (), args . output_directory , ) else : # In this case a cube file format is needed if args . file_format not in '.nc .h5 h5 hdf5 .hdf5 nc' . split (): fmt = 'nc' logger . debug ( 'Invalid extension %s for cube. Defaulting to .nc' , args . file_format ) else : fmt = args . file_format . strip ( '.' ) . replace ( 'df' , '' ) wetFilename , hydroFilename = makeDelayFileNames ( d , args . los , fmt , args . weather_model . _dataset . upper (), args . output_directory , ) wetNames . append ( wetFilename ) hydroNames . append ( hydroFilename ) args . wetFilenames = wetNames args . hydroFilenames = hydroNames return args makeDelayFileNames ( time , los , outformat , weather_model_name , out ) return names for the wet and hydrostatic delays. Examples: makeDelayFileNames(datetime(2020, 1, 1, 0, 0, 0), None, \"h5\", \"model_name\", \"some_dir\") ('some_dir/model_name_wet_00_00_00_ztd.h5', 'some_dir/model_name_hydro_00_00_00_ztd.h5') makeDelayFileNames(None, None, \"h5\", \"model_name\", \"some_dir\") ('some_dir/model_name_wet_ztd.h5', 'some_dir/model_name_hydro_ztd.h5') Source code in RAiDER/checkArgs.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def makeDelayFileNames ( time , los , outformat , weather_model_name , out ): ''' return names for the wet and hydrostatic delays. # Examples: >>> makeDelayFileNames(datetime(2020, 1, 1, 0, 0, 0), None, \"h5\", \"model_name\", \"some_dir\") ('some_dir/model_name_wet_00_00_00_ztd.h5', 'some_dir/model_name_hydro_00_00_00_ztd.h5') >>> makeDelayFileNames(None, None, \"h5\", \"model_name\", \"some_dir\") ('some_dir/model_name_wet_ztd.h5', 'some_dir/model_name_hydro_ztd.h5') ''' format_string = \" {model_name} _{{}}_ {time}{los} . {ext} \" . format ( model_name = weather_model_name , time = time . strftime ( \"%Y%m %d T%H%M%S_\" ) if time is not None else \"\" , los = \"ztd\" if ( isinstance ( los , Zenith ) or los is None ) else \"std\" , ext = outformat ) hydroname , wetname = ( format_string . format ( dtyp ) for dtyp in ( 'hydro' , 'wet' ) ) hydro_file_name = os . path . join ( out , hydroname ) wet_file_name = os . path . join ( out , wetname ) return wet_file_name , hydro_file_name cli raider calcDelays ( iargs = None ) Parse command line arguments using argparse. Source code in RAiDER/cli/raider.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 def calcDelays ( iargs = None ): \"\"\" Parse command line arguments using argparse. \"\"\" import RAiDER from RAiDER.delay import tropo_delay from RAiDER.checkArgs import checkArgs from RAiDER.processWM import prepareWeatherModel from RAiDER.utilFcns import writeDelays examples = 'Examples of use:' \\ ' \\n\\t raider.py customTemplatefile.cfg' \\ ' \\n\\t raider.py -g' p = argparse . ArgumentParser ( description = 'Command line interface for RAiDER processing with a configure file.' 'Default options can be found by running: raider.py --generate_config' , epilog = examples , formatter_class = argparse . RawDescriptionHelpFormatter ) p . add_argument ( 'customTemplateFile' , nargs = '?' , help = 'custom template with option settings. \\n ' + \"ignored if the default smallbaselineApp.cfg is input.\" ) p . add_argument ( '-g' , '--generate_template' , action = 'store_true' , help = 'generate default template (if it does not exist) and exit.' ) p . add_argument ( '--download_only' , action = 'store_true' , help = 'only download a weather model.' ) ## if not None, will replace first argument (customTemplateFile) args = p . parse_args ( args = iargs ) # default input file template_file = os . path . join ( os . path . dirname ( RAiDER . __file__ ), 'cli' , 'raider.yaml' ) if args . generate_template : dst = os . path . join ( os . getcwd (), 'raider.yaml' ) shutil . copyfile ( template_file , dst ) logger . info ( 'Wrote %s ' , dst ) os . sys . exit () # check: existence of input template files if ( not args . customTemplateFile and not os . path . isfile ( os . path . basename ( template_file )) and not args . generate_template ): msg = \"No template file found! It requires that either:\" msg += \" \\n a custom template file, OR the default template \" msg += \" \\n file 'raider.yaml' exists in current directory.\" p . print_usage () print ( examples ) raise SystemExit ( f 'ERROR: { msg } ' ) if args . customTemplateFile : # check the existence if not os . path . isfile ( args . customTemplateFile ): raise FileNotFoundError ( args . customTemplateFile ) args . customTemplateFile = os . path . abspath ( args . customTemplateFile ) else : args . customTemplateFile = template_file # Read the template file params = read_template_file ( args . customTemplateFile ) # Argument checking params = checkArgs ( params ) if not params . verbose : logger . setLevel ( logging . INFO ) delay_dct = {} for t , w , f in zip ( params [ 'date_list' ], params [ 'wetFilenames' ], params [ 'hydroFilenames' ] ): los = params [ 'los' ] aoi = params [ 'aoi' ] model = params [ 'weather_model' ] if los . ray_trace (): ll_bounds = aoi . add_buffer ( buffer = 1 ) # add a buffer for raytracing else : ll_bounds = aoi . bounds () ########################################################### # weather model calculation logger . debug ( 'Starting to run the weather model calculation' ) logger . debug ( 'Time: {} ' . format ( t . strftime ( '%Y%m %d ' ))) logger . debug ( 'Beginning weather model pre-processing' ) try : weather_model_file = prepareWeatherModel ( model , t , ll_bounds = ll_bounds , # SNWE wmLoc = params [ 'weather_model_directory' ], makePlots = params [ 'verbose' ], ) except RuntimeError : logger . exception ( \"Date %s failed\" , t ) continue # Now process the delays try : wet_delay , hydro_delay = tropo_delay ( t , weather_model_file , aoi , los , height_levels = params [ 'height_levels' ], out_proj = params [ 'output_projection' ], look_dir = params [ 'look_dir' ], cube_spacing_m = params [ 'cube_spacing_in_m' ], ) except RuntimeError : logger . exception ( \"Date %s failed\" , t ) continue ########################################################### # Write the delays to file # Different options depending on the inputs if los . is_Projected (): out_filename = w . replace ( \"_ztd\" , \"_std\" ) f = f . replace ( \"_ztd\" , \"_std\" ) elif los . ray_trace (): out_filename = w . replace ( \"_std\" , \"_ray\" ) f = f . replace ( \"_std\" , \"_ray\" ) else : out_filename = w if hydro_delay is None : # means that a dataset was returned ds = wet_delay ext = os . path . splitext ( out_filename )[ 1 ] if ext not in [ '.nc' , '.h5' ]: out_filename = f ' { os . path . splitext ( out_filename )[ 0 ] } .nc' out_filename = out_filename . replace ( \"wet\" , \"tropo\" ) if out_filename . endswith ( \".nc\" ): ds . to_netcdf ( out_filename , mode = \"w\" ) elif out_filename . endswith ( \".h5\" ): ds . to_netcdf ( out_filename , engine = \"h5netcdf\" , invalid_netcdf = True ) else : if aoi . type () == 'station_file' : out_filename = f ' { os . path . splitext ( out_filename )[ 0 ] } .csv' if aoi . type () in [ 'station_file' , 'radar_rasters' , 'geocoded_file' ]: writeDelays ( aoi , wet_delay , hydro_delay , out_filename , f , outformat = params [ 'raster_format' ]) logger . info ( 'Wrote hydro delays to: %s ' , f ) logger . info ( 'Wrote wet delays to: %s ' , out_filename ) # delay_dct[t] = wet_delay, hydro_delay delay_dct [ t ] = out_filename , f return delay_dct create_parser () Parse command line arguments using argparse. Source code in RAiDER/cli/raider.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def create_parser (): \"\"\"Parse command line arguments using argparse.\"\"\" p = argparse . ArgumentParser ( formatter_class = argparse . RawDescriptionHelpFormatter , description = HELP_MESSAGE , epilog = EXAMPLES , ) p . add_argument ( 'customTemplateFile' , nargs = '?' , help = 'custom template with option settings. \\n ' + \"ignored if the default smallbaselineApp.cfg is input.\" ) p . add_argument ( '-g' , '--generate_template' , dest = 'generate_template' , action = 'store_true' , help = 'generate default template (if it does not exist) and exit.' ) p . add_argument ( '--download-only' , action = 'store_true' , help = 'only download a weather model.' ) return p downloadGNSS () Parse command line arguments using argparse. Source code in RAiDER/cli/raider.py 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 def downloadGNSS (): \"\"\"Parse command line arguments using argparse.\"\"\" from RAiDER.gnss.downloadGNSSDelays import main as dlGNSS p = argparse . ArgumentParser ( formatter_class = argparse . RawDescriptionHelpFormatter , description = \"\"\" \\ Check for and download tropospheric zenith delays for a set of GNSS stations from UNR Example call to virtually access and append zenith delay information to a CSV table in specified output directory, across specified range of time (in YYMMDD YYMMDD) and all available times of day, and confined to specified geographic bounding box : downloadGNSSdelay.py --out products -y 20100101 20141231 -b '39 40 -79 -78' Example call to virtually access and append zenith delay information to a CSV table in specified output directory, across specified range of time (in YYMMDD YYMMDD) and specified time of day, and distributed globally : downloadGNSSdelay.py --out products -y 20100101 20141231 --returntime '00:00:00' Example call to virtually access and append zenith delay information to a CSV table in specified output directory, across specified range of time in 12 day steps (in YYMMDD YYMMDD days) and specified time of day, and distributed globally : downloadGNSSdelay.py --out products -y 20100101 20141231 12 --returntime '00:00:00' Example call to virtually access and append zenith delay information to a CSV table in specified output directory, across specified range of time (in YYMMDD YYMMDD) and specified time of day, and distributed globally but restricted to list of stations specified in input textfile : downloadGNSSdelay.py --out products -y 20100101 20141231 --returntime '00:00:00' -f station_list.txt NOTE, following example call to physically download zenith delay information not recommended as it is not necessary for most applications. Example call to physically download and append zenith delay information to a CSV table in specified output directory, across specified range of time (in YYMMDD YYMMDD) and specified time of day, and confined to specified geographic bounding box : downloadGNSSdelay.py --download --out products -y 20100101 20141231 --returntime '00:00:00' -b '39 40 -79 -78' \"\"\" ) # Stations to check/download area = p . add_argument_group ( 'Stations to check/download. Can be a lat/lon bounding box or file, or will run the whole world if not specified' ) area . add_argument ( '--station_file' , '-f' , default = None , dest = 'station_file' , help = ( 'Text file containing a list of 4-char station IDs separated by newlines' )) area . add_argument ( '-b' , '--bounding_box' , dest = 'bounding_box' , type = str , default = None , help = \"Provide either valid shapefile or Lat/Lon Bounding SNWE. -- Example : '19 20 -99.5 -98.5'\" ) area . add_argument ( '--gpsrepo' , '-gr' , default = 'UNR' , dest = 'gps_repo' , help = ( 'Specify GPS repository you wish to query. Currently supported archives: UNR.' )) misc = p . add_argument_group ( \"Run parameters\" ) add_out ( misc ) misc . add_argument ( '--date' , dest = 'dateList' , help = dedent ( \"\"\" \\ Date to calculate delay. Can be a single date, a list of two dates (earlier, later) with 1-day interval, or a list of two dates and interval in days (earlier, later, interval). Example accepted formats: YYYYMMDD or YYYYMMDD YYYYMMDD YYYYMMDD YYYYMMDD N \"\"\" ), nargs = \"+\" , action = DateListAction , type = date_type , required = True ) misc . add_argument ( '--returntime' , dest = 'returnTime' , help = \"Return delays closest to this specified time. If not specified, the GPS delays for all times will be returned. Input in 'HH:MM:SS', e.g. '16:00:00'\" , default = None ) misc . add_argument ( '--download' , help = 'Physically download data. Note this option is not necessary to proceed with statistical analyses, as data can be handled virtually in the program.' , action = 'store_true' , dest = 'download' , default = False ) add_cpus ( misc ) add_verbose ( misc ) args = p . parse_args () dlGNSS ( args ) return parseCMD ( iargs = None ) Parse command-line arguments and pass to delay.py Source code in RAiDER/cli/raider.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def parseCMD ( iargs = None ): \"\"\" Parse command-line arguments and pass to delay.py \"\"\" p = create_parser () args = p . parse_args ( args = iargs ) args . argv = iargs if iargs else sys . argv [ 1 :] # default input file template_file = os . path . join ( os . path . dirname ( RAiDER . __file__ ), 'cli' , 'raider.yaml' ) if '-g' in args . argv : dst = os . path . join ( os . getcwd (), 'raider.yaml' ) shutil . copyfile ( template_file , dst , ) logger . info ( 'Wrote %s ' , dst ) sys . exit ( 0 ) # check: existence of input template files if ( not args . customTemplateFile and not os . path . isfile ( os . path . basename ( template_file )) and not args . generate_template ): p . print_usage () print ( EXAMPLES ) msg = \"No template file found! It requires that either:\" msg += \" \\n a custom template file, OR the default template \" msg += \" \\n file 'raider.yaml' exists in current directory.\" raise SystemExit ( f 'ERROR: { msg } ' ) if args . customTemplateFile : # check the existence if not os . path . isfile ( args . customTemplateFile ): raise FileNotFoundError ( args . customTemplateFile ) args . customTemplateFile = os . path . abspath ( args . customTemplateFile ) return args read_template_file ( fname ) Read the template file into a dictionary structure. Args: fname (str): full path to the template file Returns: Name Type Description dict arguments to pass to RAiDER functions Examples: template = read_template_file('raider.yaml') Source code in RAiDER/cli/raider.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def read_template_file ( fname ): \"\"\" Read the template file into a dictionary structure. Args: fname (str): full path to the template file Returns: dict: arguments to pass to RAiDER functions Examples: >>> template = read_template_file('raider.yaml') \"\"\" from RAiDER.cli.validators import ( enforce_time , enforce_bbox , parse_dates , get_query_region , get_heights , get_los , enforce_wm ) with open ( fname , 'r' ) as f : try : params = yaml . safe_load ( f ) except yaml . YAMLError as exc : print ( exc ) raise ValueError ( 'Something is wrong with the yaml file {} ' . format ( fname )) # Drop any values not specified params = drop_nans ( params ) # Need to ensure that all the groups exist, even if they are not specified by the user group_keys = [ 'date_group' , 'time_group' , 'aoi_group' , 'height_group' , 'los_group' , 'runtime_group' ] for key in group_keys : if not key in params . keys (): params [ key ] = {} # Parse the user-provided arguments template = DEFAULT_DICT for key , value in params . items (): if key == 'runtime_group' : for k , v in value . items (): if v is not None : template [ k ] = v if key == 'weather_model' : template [ key ] = enforce_wm ( value ) if key == 'time_group' : template . update ( enforce_time ( AttributeDict ( value ))) if key == 'date_group' : template [ 'date_list' ] = parse_dates ( AttributeDict ( value )) if key == 'aoi_group' : ## in case a DEM is passed and should be used dct_temp = { ** AttributeDict ( value ), ** AttributeDict ( params [ 'height_group' ])} template [ 'aoi' ] = get_query_region ( AttributeDict ( dct_temp )) if key == 'los_group' : template [ 'los' ] = get_los ( AttributeDict ( value )) if key == 'look_dir' : if value . lower () not in [ 'right' , 'left' ]: raise ValueError ( f \"Unknown look direction { value } \" ) template [ 'look_dir' ] = value . lower () # Have to guarantee that certain variables exist prior to looking at heights for key , value in params . items (): if key == 'height_group' : template . update ( get_heights ( AttributeDict ( value ), template [ 'output_directory' ], template [ 'station_file' ], template [ 'bounding_box' ], ) ) return AttributeDict ( template ) statsPlot RaiderStats Bases: object Class which loads standard weather model/GPS delay files and generates a series of user-requested statistics and graphics. Source code in RAiDER/cli/statsPlot.py 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 class RaiderStats ( object ): ''' Class which loads standard weather model/GPS delay files and generates a series of user-requested statistics and graphics. ''' # import dependencies import glob def __init__ ( self , filearg , col_name , unit = 'm' , workdir = './' , bbox = None , spacing = 1 , timeinterval = None , seasonalinterval = None , obs_errlimit = 'inf' , time_lines = False , stationsongrids = False , station_seasonal_phase = False , cbounds = None , colorpercentile = [ 25 , 95 ], usr_colormap = 'hot_r' , grid_heatmap = False , grid_delay_mean = False , grid_delay_median = False , grid_delay_stdev = False , grid_seasonal_phase = False , grid_delay_absolute_mean = False , grid_delay_absolute_median = False , grid_delay_absolute_stdev = False , grid_seasonal_absolute_phase = False , grid_to_raster = False , min_span = [ 2 , 0.6 ], period_limit = 0.5 , numCPUs = 8 , phaseamp_per_station = False ): self . fname = filearg self . col_name = col_name self . unit = unit self . workdir = workdir self . bbox = bbox self . spacing = spacing self . timeinterval = timeinterval self . seasonalinterval = seasonalinterval self . obs_errlimit = float ( obs_errlimit ) self . time_lines = time_lines self . stationsongrids = stationsongrids self . station_seasonal_phase = station_seasonal_phase self . cbounds = cbounds self . colorpercentile = colorpercentile self . usr_colormap = usr_colormap self . grid_heatmap = grid_heatmap self . grid_delay_mean = grid_delay_mean self . grid_delay_median = grid_delay_median self . grid_delay_stdev = grid_delay_stdev self . grid_seasonal_phase = grid_seasonal_phase self . grid_seasonal_amplitude = False self . grid_seasonal_period = False self . grid_seasonal_phase_stdev = False self . grid_seasonal_amplitude_stdev = False self . grid_seasonal_period_stdev = False self . grid_seasonal_fit_rmse = False self . grid_delay_absolute_mean = grid_delay_absolute_mean self . grid_delay_absolute_median = grid_delay_absolute_median self . grid_delay_absolute_stdev = grid_delay_absolute_stdev self . grid_seasonal_absolute_phase = grid_seasonal_absolute_phase self . grid_seasonal_absolute_amplitude = False self . grid_seasonal_absolute_period = False self . grid_seasonal_absolute_phase_stdev = False self . grid_seasonal_absolute_amplitude_stdev = False self . grid_seasonal_absolute_period_stdev = False self . grid_seasonal_absolute_fit_rmse = False self . grid_to_raster = grid_to_raster self . min_span = min_span self . period_limit = period_limit self . numCPUs = numCPUs self . phaseamp_per_station = phaseamp_per_station self . grid_range = False self . grid_variance = False self . grid_variogram_rmse = False # create workdir if it doesn't exist if not os . path . exists ( self . workdir ): os . mkdir ( self . workdir ) # get colorbounds if self . cbounds : self . cbounds = [ float ( val ) for val in self . cbounds . split ()] # Pass color percentile and check for input error if self . colorpercentile is None : self . colorpercentile = [ 25 , 95 ] if self . colorpercentile [ 0 ] > self . colorpercentile [ 1 ]: raise Exception ( 'Input colorpercentile lower threshold {} higher than upper threshold {} ' . format ( self . colorpercentile [ 0 ], self . colorpercentile [ 1 ])) # load dataframe directly if previously generated TIF grid-file if self . fname . endswith ( '.tif' ): if 'grid_heatmap' in self . fname : self . grid_heatmap , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_heatmap' )[ 0 ] if 'grid_delay_mean' in self . fname : self . grid_delay_mean , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_delay_mean' )[ 0 ] if 'grid_delay_median' in self . fname : self . grid_delay_median , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_delay_median' )[ 0 ] if 'grid_delay_stdev' in self . fname : self . grid_delay_stdev , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_delay_stdev' )[ 0 ] if 'grid_seasonal_phase' in self . fname : self . grid_seasonal_phase , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_seasonal_phase' )[ 0 ] if 'grid_seasonal_period' in self . fname : self . grid_seasonal_period , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_seasonal_period' )[ 0 ] if 'grid_seasonal_amplitude' in self . fname : self . grid_seasonal_amplitude , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_seasonal_amplitude' )[ 0 ] if 'grid_seasonal_phase_stdev' in self . fname : self . grid_seasonal_phase_stdev , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_seasonal_phase_stdev' )[ 0 ] if 'grid_seasonal_amplitude_stdev' in self . fname : self . grid_seasonal_amplitude_stdev , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_seasonal_amplitude_stdev' )[ 0 ] if 'grid_seasonal_period_stdev' in self . fname : self . grid_seasonal_period_stdev , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_seasonal_period_stdev' )[ 0 ] if 'grid_seasonal_fit_rmse' in self . fname : self . grid_seasonal_fit_rmse , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_seasonal_fit_rmse' )[ 0 ] if 'grid_delay_absolute_mean' in self . fname : self . grid_delay_absolute_mean , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_delay_absolute_mean' )[ 0 ] if 'grid_delay_absolute_median' in self . fname : self . grid_delay_absolute_median , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_delay_absolute_median' )[ 0 ] if 'grid_delay_absolute_stdev' in self . fname : self . grid_delay_absolute_stdev , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_delay_absolute_stdev' )[ 0 ] if 'grid_seasonal_absolute_phase' in self . fname : self . grid_seasonal_absolute_phase , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_seasonal_absolute_phase' )[ 0 ] if 'grid_seasonal_absolute_period' in self . fname : self . grid_seasonal_absolute_period , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_seasonal_absolute_period' )[ 0 ] if 'grid_seasonal_absolute_amplitude' in self . fname : self . grid_seasonal_absolute_amplitude , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_seasonal_absolute_amplitude' )[ 0 ] if 'grid_seasonal_absolute_phase_stdev' in self . fname : self . grid_seasonal_absolute_phase_stdev , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_seasonal_absolute_phase_stdev' )[ 0 ] if 'grid_seasonal_absolute_amplitude_stdev' in self . fname : self . grid_seasonal_absolute_amplitude_stdev , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_seasonal_absolute_amplitude_stdev' )[ 0 ] if 'grid_seasonal_absolute_period_stdev' in self . fname : self . grid_seasonal_absolute_period_stdev , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_seasonal_absolute_period_stdev' )[ 0 ] if 'grid_seasonal_absolute_fit_rmse' in self . fname : self . grid_seasonal_absolute_fit_rmse , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_seasonal_absolute_fit_rmse' )[ 0 ] if 'grid_range' in self . fname : self . grid_range , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_range' )[ 0 ] if 'grid_variance' in self . fname : self . grid_variance , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_variance' )[ 0 ] if 'grid_variogram_rmse' in self . fname : self . grid_variogram_rmse , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_variogram_rmse' )[ 0 ] # setup dataframe for statistical analyses (if CSV) if self . fname . endswith ( '.csv' ): self . create_DF () def _get_extent ( self ): # dataset, spacing=1, userbbox=None \"\"\" Get the bbox, spacing in deg (by default 1deg), optionally pass user-specified bbox. Output array in WESN degrees \"\"\" extent = [ np . floor ( min ( self . df [ 'Lon' ])), np . ceil ( max ( self . df [ 'Lon' ])), np . floor ( min ( self . df [ 'Lat' ])), np . ceil ( max ( self . df [ 'Lat' ]))] if self . bbox is not None : dfextents_poly = Polygon ( np . column_stack (( np . array ([ extent [ 0 ], extent [ 0 ], extent [ 1 ], extent [ 1 ], extent [ 0 ]]), np . array ([ extent [ 2 ], extent [ 3 ], extent [ 3 ], extent [ 2 ], extent [ 2 ]])))) userbbox_poly = Polygon ( np . column_stack (( np . array ([ self . bbox [ 2 ], self . bbox [ 3 ], self . bbox [ 3 ], self . bbox [ 2 ], self . bbox [ 2 ]]), np . array ([ self . bbox [ 0 ], self . bbox [ 0 ], self . bbox [ 1 ], self . bbox [ 1 ], self . bbox [ 0 ]])))) if userbbox_poly . intersects ( dfextents_poly ): extent = [ np . floor ( self . bbox [ 2 ]), np . ceil ( self . bbox [ - 1 ]), np . floor ( self . bbox [ 0 ]), np . ceil ( self . bbox [ 1 ])] else : raise Exception ( \"User-specified bounds do not overlap with dataset bounds, adjust bounds and re-run program.\" ) if extent [ 0 ] < - 180. or extent [ 1 ] > 180. or extent [ 2 ] < - 90. or extent [ 3 ] > 90. : raise Exception ( \"Specified bounds exceed -180/180 lon and/or -90/90 lat, adjust bounds and re-run program.\" ) del dfextents_poly , userbbox_poly # ensure that extents do not exceed -180/180 lon and -90/90 lat if extent [ 0 ] < - 180. : extent [ 0 ] = - 180. if extent [ 1 ] > 180. : extent [ 1 ] = 180. if extent [ 2 ] < - 90. : extent [ 2 ] = - 90. if extent [ 3 ] > 90. : extent [ 3 ] = 90. # ensure even spacing, set spacing to 1 if specified spacing is not even multiple of bounds if ( extent [ 1 ] - extent [ 0 ]) % self . spacing != 0 or ( extent [ - 1 ] - extent [ - 2 ]) % self . spacing : logger . warning ( \"User-specified spacing %s is not even multiple of bounds, resetting spacing to 1 \\N{DEGREE SIGN} \" , self . spacing ) self . spacing = 1 # Create corners of rectangle to be transformed to a grid nw = [ extent [ 0 ] + ( self . spacing / 2 ), extent [ - 1 ] - ( self . spacing / 2 )] se = [ extent [ 1 ] - ( self . spacing / 2 ), extent [ 2 ] + ( self . spacing / 2 )] # Store grid dimension [y,x] grid_dim = [ int (( extent [ 1 ] - extent [ 0 ]) / self . spacing ), int (( extent [ - 1 ] - extent [ - 2 ]) / self . spacing )] # Iterate over 2D area gridpoints = [] y_shape = [] x_shape = [] x = se [ 0 ] while x >= nw [ 0 ]: y = se [ 1 ] while y <= nw [ 1 ]: y_shape . append ( y ) gridpoints . append ([ x , y ]) y += self . spacing x_shape . append ( x ) x -= self . spacing gridpoints . reverse () return extent , grid_dim , gridpoints def _check_stationgrid_intersection ( self , stat_ID ): ''' Return index of grid cell which intersects with station Note: Fast, but assumes station locations don't change ''' coord = Point (( self . unique_points [ 1 ][ self . unique_points [ 0 ] . index ( stat_ID )], self . unique_points [ 2 ][ self . unique_points [ 0 ] . index ( stat_ID )])) # Get grid cell polygon which intersect with station coordinate grid_int = self . polygon_tree . query ( coord ) # Pass corresponding grid cell index if grid_int : return self . polygon_dict [ id ( grid_int [ 0 ])] return 'NaN' def _reader ( self ): ''' Read a input file ''' try : data = pd . read_csv ( self . fname , parse_dates = [ 'Datetime' ]) data [ 'Date' ] = data [ 'Datetime' ] . apply ( lambda x : x . date ()) data [ 'Date' ] = data [ 'Date' ] . apply ( lambda x : dt . datetime . strptime ( x . strftime ( \"%Y-%m- %d \" ), \"%Y-%m- %d \" )) except BaseException : data = pd . read_csv ( self . fname , parse_dates = [ 'Date' ]) # check if user-specified key is valid if self . col_name not in data . keys (): raise Exception ( 'User-specified key {} not found in input file {} . Must specify valid key.' . format ( self . col_name , self . fname )) # if user-specified key is the same as the 'Date' field, rename if self . col_name == 'Date' : logger . warning ( 'Input key {} same as \"Date\" field name, rename the former' . format ( self . col_name )) self . col_name += '_plot' data [ self . col_name ] = data [ 'Date' ] # convert to specified output unit inputunit = 'm' data [ self . col_name ] = convert_SI ( data [ self . col_name ], inputunit , self . unit ) # filter out obs by error if 'sigZTD' in data . keys (): data [ 'sigZTD' ] = convert_SI ( data [ 'sigZTD' ], inputunit , self . unit ) self . obs_errlimit = convert_SI ( self . obs_errlimit , inputunit , self . unit ) data = data [ data [ 'sigZTD' ] <= self . obs_errlimit ] else : logger . warning ( 'Key \"sigZTD\" not found in dataset, cannot filter out obs by error' ) return data def create_DF ( self ): ''' Create dataframe. ''' # Open file self . df = self . _reader () # Filter dataframe # drop all nans self . df . dropna ( how = 'any' , inplace = True ) self . df . reset_index ( drop = True , inplace = True ) # convert to datetime object # time-interval filter if self . timeinterval : self . timeinterval = [ dt . datetime . strptime ( val , '%Y-%m- %d ' ) for val in self . timeinterval . split ()] self . df = self . df [( self . df [ 'Date' ] >= self . timeinterval [ 0 ]) & ( self . df [ 'Date' ] <= self . timeinterval [ - 1 ])] # seasonal filter if self . seasonalinterval : self . seasonalinterval = self . seasonalinterval . split () # get day of year self . seasonalinterval = [ dt . datetime . strptime ( '2001-' + self . seasonalinterval [ 0 ], '%Y-%m- %d ' ) . timetuple ( ) . tm_yday , dt . datetime . strptime ( '2001-' + self . seasonalinterval [ - 1 ], '%Y-%m- %d ' ) . timetuple () . tm_yday ] # track input order and wrap around year if necessary # e.g. month/day: 03/01 to 06/01 if self . seasonalinterval [ 0 ] < self . seasonalinterval [ 1 ]: # non leap-year filtered_self = self . df [( not self . df [ 'Date' ] . dt . is_leap_year ) & ( self . df [ 'Date' ] . dt . dayofyear >= self . seasonalinterval [ 0 ]) & ( self . df [ 'Date' ] . dt . dayofyear <= self . seasonalinterval [ - 1 ])] # leap-year self . seasonalinterval = [ i + 1 if i > 59 else i for i in self . seasonalinterval ] self . df = filtered_self . append ( self . df [( self . df [ 'Date' ] . dt . is_leap_year ) & ( self . df [ 'Date' ] . dt . dayofyear >= self . seasonalinterval [ 0 ]) & ( self . df [ 'Date' ] . dt . dayofyear <= self . seasonalinterval [ - 1 ])], ignore_index = True ) del filtered_self # e.g. month/day: 12/01 to 03/01 if self . seasonalinterval [ 0 ] > self . seasonalinterval [ 1 ]: # non leap-year filtered_self = self . df [( not self . df [ 'Date' ] . dt . is_leap_year ) & ( self . df [ 'Date' ] . dt . dayofyear >= self . seasonalinterval [ - 1 ]) & ( self . df [ 'Date' ] . dt . dayofyear <= self . seasonalinterval [ 0 ])] # leap-year self . seasonalinterval = [ i + 1 if i > 59 else i for i in self . seasonalinterval ] self . df = filtered_self . append ( self . df [( self . df [ 'Date' ] . dt . is_leap_year ) & ( self . df [ 'Date' ] . dt . dayofyear >= self . seasonalinterval [ - 1 ]) & ( self . df [ 'Date' ] . dt . dayofyear <= self . seasonalinterval [ 0 ])], ignore_index = True ) del filtered_self # estimate central longitude lines if '--time_lines' specified if self . time_lines and 'Datetime' in self . df . keys (): self . df [ 'Date_hr' ] = self . df [ 'Datetime' ] . dt . hour . astype ( float ) . astype ( \"Int32\" ) # get list of unique times all_hrs = sorted ( set ( self . df [ 'Date_hr' ])) # get central longitude bands associated with each time central_points = [] # if single time, avoid loop if len ( all_hrs ) == 1 : central_points . append (([ 0 , max ( self . df [ 'Lon' ])], [ 0 , min ( self . df [ 'Lon' ])])) else : for i in enumerate ( all_hrs ): # last entry if i [ 0 ] == len ( all_hrs ) - 1 : lons = self . df [ self . df [ 'Date_hr' ] > all_hrs [ i [ 0 ] - 1 ]] # first entry elif i [ 0 ] == 0 : lons = self . df [ self . df [ 'Date_hr' ] < all_hrs [ i [ 0 ] + 1 ]] else : lons = self . df [( self . df [ 'Date_hr' ] > all_hrs [ i [ 0 ] - 1 ]) & ( self . df [ 'Date_hr' ] < all_hrs [ i [ 0 ] + 1 ])] central_points . append (([ 0 , max ( lons [ 'Lon' ])], [ 0 , min ( lons [ 'Lon' ])])) # get central longitudes self . time_lines = [ midpoint ( i [ 0 ], i [ 1 ]) for i in central_points ] # Get bbox, buffered by grid spacing. # Check if bbox input is valid list. if self . bbox is not None : try : self . bbox = [ float ( val ) for val in self . bbox . split ()] except BaseException : raise Exception ( 'Cannot understand the --bounding_box argument. String input is incorrect or path does not exist.' ) self . plotbbox , self . grid_dim , self . gridpoints = self . _get_extent () # generate list of grid-polygons append_poly = [] for i in self . gridpoints : bbox = [ i [ 1 ] - ( self . spacing / 2 ), i [ 1 ] + ( self . spacing / 2 ), i [ 0 ] - ( self . spacing / 2 ), i [ 0 ] + ( self . spacing / 2 )] append_poly . append ( Polygon ( np . column_stack (( np . array ([ bbox [ 2 ], bbox [ 3 ], bbox [ 3 ], bbox [ 2 ], bbox [ 2 ]]), np . array ([ bbox [ 0 ], bbox [ 0 ], bbox [ 1 ], bbox [ 1 ], bbox [ 0 ]]))))) # Pass lons/lats to create polygon # Check for grid cell intersection with each station idtogrid_dict = {} self . unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' ]) . size () self . unique_points = [ self . unique_points . index . get_level_values ( 'ID' ) . tolist (), self . unique_points . index . get_level_values ( 'Lon' ) . tolist (), self . unique_points . index . get_level_values ( 'Lat' ) . tolist ()] # Initiate R-tree of gridded array domain self . polygon_dict = dict (( id ( pt ), i ) for i , pt in enumerate ( append_poly )) self . polygon_tree = STRtree ( append_poly ) for stat_ID in self . unique_points [ 0 ]: grd_index = self . _check_stationgrid_intersection ( stat_ID ) idtogrid_dict [ stat_ID ] = grd_index # map gridnode dictionary to dataframe self . df [ 'gridnode' ] = self . df [ 'ID' ] . map ( idtogrid_dict ) self . df = self . df [ self . df [ 'gridnode' ] . astype ( str ) != 'NaN' ] del self . unique_points , self . polygon_dict , self . polygon_tree , idtogrid_dict , append_poly # sort by grid and date self . df . sort_values ([ 'gridnode' , 'Date' ]) # If specified, pass station locations to superimpose on gridplots if self . stationsongrids : unique_points = self . df . groupby ([ 'Lon' , 'Lat' ]) . size () self . stationsongrids = [ unique_points . index . get_level_values ( 'Lon' ) . tolist (), unique_points . index . get_level_values ( 'Lat' ) . tolist ()] # If specified, setup gridded array(s) if self . grid_heatmap : self . grid_heatmap = np . array ([ np . nan if i [ 0 ] not in self . df [ 'gridnode' ] . values [:] else int ( len ( np . unique ( self . df [ 'ID' ][ self . df [ 'gridnode' ] == i [ 0 ]]))) for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_heatmap' + '.tif' ) save_gridfile ( self . grid_heatmap , 'grid_heatmap' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %1i ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'int16' , noData = 0 ) if self . grid_delay_mean : # Take mean of station-wise means per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ self . col_name ] . mean () unique_points = unique_points . groupby ([ 'gridnode' ])[ self . col_name ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_delay_mean = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_delay_mean' + '.tif' ) save_gridfile ( self . grid_delay_mean , 'grid_delay_mean' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.2f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) if self . grid_delay_median : # Take mean of station-wise medians per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ self . col_name ] . median () unique_points = unique_points . groupby ([ 'gridnode' ])[ self . col_name ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_delay_median = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_delay_median' + '.tif' ) save_gridfile ( self . grid_delay_median , 'grid_delay_median' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.2f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) if self . grid_delay_stdev : # Take mean of station-wise stdev per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ self . col_name ] . std () unique_points = unique_points . groupby ([ 'gridnode' ])[ self . col_name ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_delay_stdev = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_delay_stdev' + '.tif' ) save_gridfile ( self . grid_delay_stdev , 'grid_delay_stdev' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.2f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) if self . grid_delay_absolute_mean : # Take mean of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ self . col_name ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_delay_absolute_mean = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_delay_absolute_mean' + '.tif' ) save_gridfile ( self . grid_delay_absolute_mean , 'grid_delay_absolute_mean' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.2f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) if self . grid_delay_absolute_median : # Take median of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ self . col_name ] . median () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_delay_absolute_median = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_delay_absolute_median' + '.tif' ) save_gridfile ( self . grid_delay_absolute_median , 'grid_delay_absolute_median' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.2f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) if self . grid_delay_absolute_stdev : # Take stdev of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ self . col_name ] . std () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_delay_absolute_stdev = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_delay_absolute_stdev' + '.tif' ) save_gridfile ( self . grid_delay_absolute_stdev , 'grid_delay_absolute_stdev' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.2f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # If specified, compute phase/amplitude fits if self . station_seasonal_phase or self . grid_seasonal_phase or self . grid_seasonal_absolute_phase : # Sort by coordinates unique_points = self . df . sort_values ([ 'ID' , 'Date' ]) unique_points [ 'Date' ] = [ i . timestamp () for i in unique_points [ 'Date' ]] # Setup variables self . ampfit = [] self . phsfit = [] self . periodfit = [] self . ampfit_c = [] self . phsfit_c = [] self . periodfit_c = [] self . seasonalfit_rmse = [] args = [] for i in sorted ( list ( set ( unique_points [ 'ID' ]))): # pass all values corresponding to station (ID, data = y, time = x) args . append (( i , unique_points [ unique_points [ 'ID' ] == i ][ 'Date' ] . to_list (), unique_points [ unique_points [ 'ID' ] == i ][ self . col_name ] . to_list (), self . min_span [ 0 ], self . min_span [ 1 ], self . period_limit )) # Parallelize iteration through all grid-cells and time slices with multiprocessing . Pool ( self . numCPUs ) as multipool : for i , j , k , l , m , n , o in multipool . starmap ( self . _amplitude_and_phase , args ): self . ampfit . extend ( i ) self . phsfit . extend ( j ) self . periodfit . extend ( k ) self . ampfit_c . extend ( l ) self . phsfit_c . extend ( m ) self . periodfit_c . extend ( n ) self . seasonalfit_rmse . extend ( o ) # map phase/amplitude fits dictionary to dataframe self . phsfit = { k : v for d in self . phsfit for k , v in d . items ()} self . ampfit = { k : v for d in self . ampfit for k , v in d . items ()} self . periodfit = { k : v for d in self . periodfit for k , v in d . items ()} self . df [ 'phsfit' ] = self . df [ 'ID' ] . map ( self . phsfit ) # check if there are any valid data values if self . df [ 'phsfit' ] . isnull () . values . all ( axis = 0 ): raise Exception ( \"No valid data values, adjust --min_span inputs for time span in years {} and/or fractional obs. {} \" . format ( self . min_span [ 0 ], self . min_span [ 1 ])) self . df [ 'ampfit' ] = self . df [ 'ID' ] . map ( self . ampfit ) self . df [ 'periodfit' ] = self . df [ 'ID' ] . map ( self . periodfit ) self . phsfit_c = { k : v for d in self . phsfit_c for k , v in d . items ()} self . ampfit_c = { k : v for d in self . ampfit_c for k , v in d . items ()} self . periodfit_c = { k : v for d in self . periodfit_c for k , v in d . items ()} self . seasonalfit_rmse = { k : v for d in self . seasonalfit_rmse for k , v in d . items ()} self . df [ 'phsfit_c' ] = self . df [ 'ID' ] . map ( self . phsfit_c ) self . df [ 'ampfit_c' ] = self . df [ 'ID' ] . map ( self . ampfit_c ) self . df [ 'periodfit_c' ] = self . df [ 'ID' ] . map ( self . periodfit_c ) self . df [ 'seasonalfit_rmse' ] = self . df [ 'ID' ] . map ( self . seasonalfit_rmse ) # drop nan self . df . dropna ( how = 'any' , inplace = True ) # If grid plots specified if self . grid_seasonal_phase : # Pass mean phase of station-wise means per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ 'phsfit' ] . mean () unique_points = unique_points . groupby ([ 'gridnode' ])[ 'phsfit' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_phase = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_phase' + '.tif' ) save_gridfile ( self . grid_seasonal_phase , 'grid_seasonal_phase' , gridfile_name , self . plotbbox , self . spacing , 'days' , colorbarfmt = ' %.1i ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass mean amplitude of station-wise means per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ 'ampfit' ] . mean () unique_points = unique_points . groupby ([ 'gridnode' ])[ 'ampfit' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_amplitude = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_amplitude' + '.tif' ) save_gridfile ( self . grid_seasonal_amplitude , 'grid_seasonal_amplitude' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.3f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass mean period of station-wise means per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ 'periodfit' ] . mean () unique_points = unique_points . groupby ([ 'gridnode' ])[ 'periodfit' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_period = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_period' + '.tif' ) save_gridfile ( self . grid_seasonal_period , 'grid_seasonal_period' , gridfile_name , self . plotbbox , self . spacing , 'years' , colorbarfmt = ' %.2f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) ######################################################################################################################## # Pass mean phase stdev of station-wise means per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ 'phsfit_c' ] . mean () unique_points = unique_points . groupby ([ 'gridnode' ])[ 'phsfit_c' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_phase_stdev = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_phase_stdev' + '.tif' ) save_gridfile ( self . grid_seasonal_phase_stdev , 'grid_seasonal_phase_stdev' , gridfile_name , self . plotbbox , self . spacing , 'days' , colorbarfmt = ' %.1i ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass mean amplitude stdev of station-wise means per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ 'ampfit_c' ] . mean () unique_points = unique_points . groupby ([ 'gridnode' ])[ 'ampfit_c' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_amplitude_stdev = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_amplitude_stdev' + '.tif' ) save_gridfile ( self . grid_seasonal_amplitude_stdev , 'grid_seasonal_amplitude_stdev' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.3f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass mean period stdev of station-wise means per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ 'periodfit_c' ] . mean () unique_points = unique_points . groupby ([ 'gridnode' ])[ 'periodfit_c' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_period_stdev = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_period_stdev' + '.tif' ) save_gridfile ( self . grid_seasonal_period_stdev , 'grid_seasonal_period_stdev' , gridfile_name , self . plotbbox , self . spacing , 'years' , colorbarfmt = ' %.2e ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass mean seasonal fit RMSE of station-wise means per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ 'seasonalfit_rmse' ] . mean () unique_points = unique_points . groupby ([ 'gridnode' ])[ 'seasonalfit_rmse' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_fit_rmse = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_fit_rmse' + '.tif' ) save_gridfile ( self . grid_seasonal_fit_rmse , 'grid_seasonal_fit_rmse' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.3f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) ######################################################################################################################## if self . grid_seasonal_absolute_phase : # Pass absolute mean phase of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ 'phsfit' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_absolute_phase = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_absolute_phase' + '.tif' ) save_gridfile ( self . grid_seasonal_absolute_phase , 'grid_seasonal_absolute_phase' , gridfile_name , self . plotbbox , self . spacing , 'days' , colorbarfmt = ' %.1i ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass absolute mean amplitude of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ 'ampfit' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_absolute_amplitude = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_absolute_amplitude' + '.tif' ) save_gridfile ( self . grid_seasonal_absolute_amplitude , 'grid_seasonal_absolute_amplitude' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.3f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass absolute mean period of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ 'periodfit' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_absolute_period = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_absolute_period' + '.tif' ) save_gridfile ( self . grid_seasonal_absolute_period , 'grid_seasonal_absolute_period' , gridfile_name , self . plotbbox , self . spacing , 'years' , colorbarfmt = ' %.2f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) ######################################################################################################################## # Pass absolute mean phase stdev of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ 'phsfit_c' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_absolute_phase_stdev = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_absolute_phase_stdev' + '.tif' ) save_gridfile ( self . grid_seasonal_absolute_phase_stdev , 'grid_seasonal_absolute_phase_stdev' , gridfile_name , self . plotbbox , self . spacing , 'days' , colorbarfmt = ' %.1i ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass absolute mean amplitude stdev of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ 'ampfit_c' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_absolute_amplitude_stdev = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_absolute_amplitude_stdev' + '.tif' ) save_gridfile ( self . grid_seasonal_absolute_amplitude_stdev , 'grid_seasonal_absolute_amplitude_stdev' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.3f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass absolute mean period stdev of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ 'periodfit_c' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_absolute_period_stdev = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_absolute_period_stdev' + '.tif' ) save_gridfile ( self . grid_seasonal_absolute_period_stdev , 'grid_seasonal_absolute_period_stdev' , gridfile_name , self . plotbbox , self . spacing , 'years' , colorbarfmt = ' %.2e ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass absolute mean seasonal fit RMSE of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ 'seasonalfit_rmse' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_absolute_fit_rmse = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_absolute_fit_rmse' + '.tif' ) save_gridfile ( self . grid_seasonal_absolute_fit_rmse , 'grid_seasonal_absolute_fit_rmse' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.2e ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) def _amplitude_and_phase ( self , station , tt , yy , min_span = 2 , min_frac = 0.6 , period_limit = 0. ): ''' Fit sin to the input time sequence, and return fitting parameters: \"amp\", \"omega\", \"phase\", \"offset\", \"freq\", \"period\" and \"fitfunc\". Minimum time span in years (min_span), minimum fractional observations in span (min_frac), and period limit (period_limit) enforced for statistical analysis. Source: https://stackoverflow.com/questions/16716302/how-do-i-fit-a-sine-curve-to-my-data-with-pylab-and-numpy ''' ampfit = {} phsfit = {} periodfit = {} ampfit_c = {} phsfit_c = {} periodfit_c = {} seasonalfit_rmse = {} ampfit [ station ] = np . nan phsfit [ station ] = np . nan periodfit [ station ] = np . nan ampfit_c [ station ] = np . nan phsfit_c [ station ] = np . nan periodfit_c [ station ] = np . nan seasonalfit_rmse [ station ] = np . nan # Fit with custom fit function with fixed period, if specified if period_limit != 0 : # convert from years to radians/seconds w = ( 1 / period_limit ) * ( 1 / 31556952 ) * ( 2. * np . pi ) def custom_sine_function_base ( t , A , p , c ): return self . _sine_function_base ( t , A , w , p , c ) else : def custom_sine_function_base ( t , A , w , p , c ): return self . _sine_function_base ( t , A , w , p , c ) # If station TS does not span specified time period, pass NaNs time_span_yrs = ( max ( tt ) - min ( tt )) / 31556952 if time_span_yrs >= min_span and len ( list ( set ( tt ))) / ( time_span_yrs * 365.25 ) >= min_frac : tt = np . array ( tt ) yy = np . array ( yy ) ff = np . fft . fftfreq ( len ( tt ), ( tt [ 1 ] - tt [ 0 ])) # assume uniform spacing Fyy = abs ( np . fft . fft ( yy )) guess_freq = abs ( ff [ np . argmax ( Fyy [ 1 :]) + 1 ]) # excluding the zero period \"peak\", which is related to offset guess_amp = np . std ( yy ) * 2. ** 0.5 guess_offset = np . mean ( yy ) guess = np . array ([ guess_amp , 2. * np . pi * guess_freq , 0. , guess_offset ]) # Adjust frequency guess to reflect fixed period, if specified if period_limit != 0 : guess = np . array ([ guess_amp , 0. , guess_offset ]) # Catch warning where covariance cannot be estimated # I.e. OptimizeWarning: Covariance of the parameters could not be estimated with warnings . catch_warnings (): warnings . simplefilter ( \"error\" , OptimizeWarning ) try : optimize_warning = False try : # Note, may have to adjust max number of iterations (maxfev) higher to avoid crashes popt , pcov = optimize . curve_fit ( custom_sine_function_base , tt , yy , p0 = guess , maxfev = int ( 1e6 )) # If sparse input such that fittitng is not possible, pass NaNs except TypeError : self . ampfit . append ( np . nan ), self . phsfit . append ( np . nan ), self . periodfit . append ( np . nan ), \\ self . ampfit_c . append ( np . nan ), self . phsfit_c . append ( np . nan ), \\ self . periodfit_c . append ( np . nan ), self . seasonalfit_rmse . append ( np . nan ) return self . ampfit , self . phsfit , self . periodfit , self . ampfit_c , \\ self . phsfit_c , self . periodfit_c , self . seasonalfit_rmse except OptimizeWarning : optimize_warning = True warnings . simplefilter ( \"ignore\" , OptimizeWarning ) popt , pcov = optimize . curve_fit ( custom_sine_function_base , tt , yy , p0 = guess , maxfev = int ( 1e6 )) print ( 'OptimizeWarning: Covariance for station {} could not be estimated. Refer to debug figure here {} \\ ' . format ( station , os . path . join ( self . workdir , 'phaseamp_per_station' , 'station {} .png' . format ( station )))) pass # Adjust expected output to reflect fixed period, if specified if period_limit != 0 : A , p , c = popt else : A , w , p , c = popt # convert from radians/seconds to years f = ( w / ( 2. * np . pi )) * ( 31556952 ) f = 1 / f def fitfunc ( t ): return A * np . sin ( w * t + p ) + c # Outputs = \"amp\": A, \"angular frequency\": w, \"phase\": p, \"offset\": c, \"freq\": f, \"period\": 1./f, # \"fitfunc\": fitfunc, \"maxcov\": np.max(pcov), \"rawres\": (guess,popt,pcov) # Pass amplitude (specified units) and phase (days) and stdev ampfit [ station ] = abs ( A ) # Convert phase from rad to days, apply half wavelength shift if Amp is negative if A < 0 : p += 3.14159 phsfit [ station ] = ( 365.25 / 2 ) * np . sin ( p ) periodfit [ station ] = f # Catch warning where output is so small that it gets rounded to 0 # I.e. RuntimeWarning: invalid value encountered in double_scalars with np . errstate ( invalid = 'raise' ): try : # pass covariance for each parameter ampfit_c [ station ] = pcov [ 0 , 0 ] ** 0.5 periodfit_c [ station ] = pcov [ 1 , 1 ] ** 0.5 phsfit_c [ station ] = pcov [ 2 , 2 ] ** 0.5 # pass RMSE of fit seasonalfit_rmse [ station ] = yy - custom_sine_function_base ( tt , * popt ) seasonalfit_rmse [ station ] = ( scipy_sum ( seasonalfit_rmse [ station ] ** 2 ) / ( seasonalfit_rmse [ station ] . size - 2 )) ** 0.5 except FloatingPointError : pass if self . phaseamp_per_station or optimize_warning : # Debug plotting for each station # convert time (datetime seconds) to absolute years for plotting tt_plot = copy . deepcopy ( tt ) tt_plot -= min ( tt_plot ) tt_plot /= 31556952 plt . plot ( tt_plot , yy , \"ok\" , label = \"input\" ) plt . xlabel ( \"time (years)\" ) plt . ylabel ( \"data ( {} )\" . format ( self . unit )) num_testpoints = len ( tt ) * 10 if num_testpoints > 1000 : num_testpoints = 1000 tt2 = np . linspace ( min ( tt ), max ( tt ), num_testpoints ) # convert time to years for plotting tt2_plot = copy . deepcopy ( tt2 ) tt2_plot -= min ( tt2_plot ) tt2_plot /= 31556952 plt . plot ( tt2_plot , fitfunc ( tt2 ), \"r-\" , label = \"fit\" , linewidth = 2 ) plt . legend ( loc = \"best\" ) if not os . path . exists ( os . path . join ( self . workdir , 'phaseamp_per_station' )): os . mkdir ( os . path . join ( self . workdir , 'phaseamp_per_station' )) plt . savefig ( os . path . join ( self . workdir , 'phaseamp_per_station' , 'station {} .png' . format ( station )), format = 'png' , bbox_inches = 'tight' ) plt . close () optimize_warning = False self . ampfit . append ( ampfit ) self . phsfit . append ( phsfit ) self . periodfit . append ( periodfit ) self . ampfit_c . append ( ampfit_c ) self . phsfit_c . append ( phsfit_c ) self . periodfit_c . append ( periodfit_c ) self . seasonalfit_rmse . append ( seasonalfit_rmse ) return self . ampfit , self . phsfit , self . periodfit , self . ampfit_c , \\ self . phsfit_c , self . periodfit_c , self . seasonalfit_rmse def _sine_function_base ( self , t , A , w , p , c ): ''' Base function for modeling sinusoidal amplitude/phase fits. ''' return A * np . sin ( w * t + p ) + c def __call__ ( self , gridarr , plottype , workdir = './' , drawgridlines = False , colorbarfmt = ' %.2e ' , stationsongrids = None , resValue = 5 , plotFormat = 'pdf' , userTitle = None ): ''' Visualize a suite of statistics w.r.t. stations. Pass either a list of points or a gridded array as the first argument. Alternatively, you may superimpose your gridded array with a supplementary list of points by passing the latter through the stationsongrids argument. ''' from cartopy import crs as ccrs from cartopy import feature as cfeature from cartopy.mpl.ticker import LatitudeFormatter , LongitudeFormatter from matplotlib import ticker as mticker from mpl_toolkits.axes_grid1 import make_axes_locatable # If specified workdir doesn't exist, create it if not os . path . exists ( workdir ): os . mkdir ( workdir ) # Pass cbounds cbounds = self . cbounds # Initiate no-data array to mask data nodat_arr = [ 0 , np . nan , np . inf ] if self . unit in [ 'minute' , 'hour' , 'day' , 'year' ]: colorbarfmt = ' %.1i ' nodat_arr = [ np . nan , np . inf ] fig , axes = plt . subplots ( subplot_kw = { 'projection' : ccrs . PlateCarree ()}) # by default set background to white axes . add_feature ( cfeature . NaturalEarthFeature ( 'physical' , 'land' , '50m' , facecolor = 'white' ), zorder = 0 ) axes . set_extent ( self . plotbbox , ccrs . PlateCarree ()) # add coastlines axes . coastlines ( linewidth = 0.2 , color = \"gray\" , zorder = 4 ) cmap = copy . copy ( mpl . cm . get_cmap ( self . usr_colormap )) # cmap.set_bad('black', 0.) # extract all colors from the hot map cmaplist = [ cmap ( i ) for i in range ( cmap . N )] # create the new map cmap = mpl . colors . LinearSegmentedColormap . from_list ( 'Custom cmap' , cmaplist ) axes . set_xlabel ( 'Longitude' , weight = 'bold' , zorder = 2 ) axes . set_ylabel ( 'Latitude' , weight = 'bold' , zorder = 2 ) # set ticks axes . set_xticks ( np . linspace ( self . plotbbox [ 0 ], self . plotbbox [ 1 ], 5 ), crs = ccrs . PlateCarree ()) axes . set_yticks ( np . linspace ( self . plotbbox [ 2 ], self . plotbbox [ 3 ], 5 ), crs = ccrs . PlateCarree ()) lon_formatter = LongitudeFormatter ( number_format = '.0f' , degree_symbol = '' ) lat_formatter = LatitudeFormatter ( number_format = '.0f' , degree_symbol = '' ) axes . xaxis . set_major_formatter ( lon_formatter ) axes . yaxis . set_major_formatter ( lat_formatter ) # draw central longitude lines corresponding to respective datetimes if self . time_lines : tl = axes . grid ( axis = 'x' , linewidth = 1.5 , color = 'blue' , alpha = 0.5 , linestyle = '-' , zorder = 3 ) # If individual stations passed if isinstance ( gridarr , list ): # spatial distribution of stations if plottype == \"station_distribution\" : im = axes . scatter ( gridarr [ 0 ], gridarr [ 1 ], zorder = 1 , s = 0.5 , marker = '.' , color = 'b' , transform = ccrs . PlateCarree ()) # passing 3rd column as z-value if len ( gridarr ) > 2 : # set land/water background to light gray/blue respectively so station point data can be seen axes . add_feature ( cfeature . NaturalEarthFeature ( 'physical' , 'land' , '50m' , facecolor = '#A9A9A9' ), zorder = 0 ) axes . add_feature ( cfeature . NaturalEarthFeature ( 'physical' , 'ocean' , '50m' , facecolor = '#ADD8E6' ), zorder = 0 ) # set masked values as nans zvalues = gridarr [ 2 ] for i in nodat_arr : zvalues = np . ma . masked_where ( zvalues == i , zvalues ) zvalues = np . ma . filled ( zvalues , np . nan ) # define the bins and normalize if cbounds is None : # avoid \"ufunc 'isnan'\" error by casting array as float cbounds = [ np . nanpercentile ( zvalues . astype ( 'float' ), self . colorpercentile [ 0 ]), np . nanpercentile ( zvalues . astype ( 'float' ), self . colorpercentile [ 1 ])] # if upper/lower bounds identical, overwrite lower bound as 75% of upper bound to avoid plotting ValueError if cbounds [ 0 ] == cbounds [ 1 ]: cbounds [ 0 ] *= 0.75 cbounds . sort () # adjust precision for colorbar if necessary if ( abs ( np . nanmax ( zvalues ) - np . nanmin ( zvalues )) < 1 and ( np . nanmean ( zvalues )) < 1 ) \\ or abs ( np . nanmax ( zvalues ) - np . nanmin ( zvalues )) > 500 : colorbarfmt = ' %.2e ' colorbounds = np . linspace ( cbounds [ 0 ], cbounds [ 1 ], 256 ) colorbounds = np . unique ( colorbounds ) norm = mpl . colors . BoundaryNorm ( colorbounds , cmap . N ) colorbounds_ticks = np . linspace ( cbounds [ 0 ], cbounds [ 1 ], 10 ) # plot data and initiate colorbar im = axes . scatter ( gridarr [ 0 ], gridarr [ 1 ], c = zvalues , cmap = cmap , norm = norm , zorder = 1 , s = 0.5 , marker = '.' , transform = ccrs . PlateCarree ()) # initiate colorbar and control height of colorbar divider = make_axes_locatable ( axes ) cax = divider . append_axes ( \"right\" , size = \"5%\" , pad = 0.05 , axes_class = plt . Axes ) cbar_ax = fig . colorbar ( im , spacing = 'proportional' , ticks = colorbounds_ticks , boundaries = colorbounds , format = colorbarfmt , pad = 0.1 , cax = cax ) # If gridded area passed else : # set masked values as nans for i in nodat_arr : gridarr = np . ma . masked_where ( gridarr == i , gridarr ) gridarr = np . ma . filled ( gridarr , np . nan ) # set land/water background to light gray/blue respectively so grid cells can be seen axes . add_feature ( cfeature . NaturalEarthFeature ( 'physical' , 'land' , '50m' , facecolor = '#A9A9A9' ), zorder = 0 ) axes . add_feature ( cfeature . NaturalEarthFeature ( 'physical' , 'ocean' , '50m' , facecolor = '#ADD8E6' ), zorder = 0 ) # define the bins and normalize if cbounds is None : cbounds = [ np . nanpercentile ( gridarr , self . colorpercentile [ 0 ]), np . nanpercentile ( gridarr , self . colorpercentile [ 1 ])] # if upper/lower bounds identical, overwrite lower bound as 75% of upper bound to avoid plotting ValueError if cbounds [ 0 ] == cbounds [ 1 ]: cbounds [ 0 ] *= 0.75 cbounds . sort () # plot data and initiate colorbar if ( abs ( np . nanmax ( gridarr ) - np . nanmin ( gridarr )) < 1 and abs ( np . nanmean ( gridarr )) < 1 ) \\ or abs ( np . nanmax ( gridarr ) - np . nanmin ( gridarr )) > 500 : colorbarfmt = ' %.2e ' colorbounds = np . linspace ( cbounds [ 0 ], cbounds [ 1 ], 256 ) colorbounds = np . unique ( colorbounds ) norm = mpl . colors . BoundaryNorm ( colorbounds , cmap . N ) colorbounds_ticks = np . linspace ( cbounds [ 0 ], cbounds [ 1 ], 10 ) # plot data im = axes . imshow ( gridarr , cmap = cmap , norm = norm , extent = self . plotbbox , zorder = 1 , origin = 'upper' , transform = ccrs . PlateCarree ()) # initiate colorbar and control height of colorbar divider = make_axes_locatable ( axes ) cax = divider . append_axes ( \"right\" , size = \"5%\" , pad = 0.05 , axes_class = plt . Axes ) cbar_ax = fig . colorbar ( im , spacing = 'proportional' , ticks = colorbounds_ticks , boundaries = colorbounds , format = colorbarfmt , pad = 0.1 , cax = cax ) # superimpose your gridded array with a supplementary list of point, if specified if self . stationsongrids : axes . scatter ( self . stationsongrids [ 0 ], self . stationsongrids [ 1 ], zorder = 2 , s = 0.5 , marker = '.' , color = 'b' , transform = ccrs . PlateCarree ()) # draw gridlines, if specified if drawgridlines : gl = axes . gridlines ( crs = ccrs . PlateCarree ( ), linewidth = 0.5 , color = 'black' , alpha = 0.5 , linestyle = '-' , zorder = 3 ) gl . xlocator = mticker . FixedLocator ( np . arange ( self . plotbbox [ 0 ], self . plotbbox [ 1 ] + self . spacing , self . spacing ) . tolist ()) gl . ylocator = mticker . FixedLocator ( np . arange ( self . plotbbox [ 2 ], self . plotbbox [ 3 ] + self . spacing , self . spacing ) . tolist ()) # Add labels to colorbar, if necessary if 'cbar_ax' in locals (): # experimental variogram fit sill heatmap if plottype == \"grid_variance\" : cbar_ax . set_label ( \" \" . join ( plottype . replace ( 'grid_' , '' ) . split ( '_' )) . title () + ' ( {} \\u00b2 )' . format ( self . unit ), rotation =- 90 , labelpad = 10 ) # specify appropriate units for mean/median/std/amplitude/experimental variogram fit heatmap elif plottype == \"grid_delay_mean\" or plottype == \"grid_delay_median\" or plottype == \"grid_delay_stdev\" or \\ plottype == \"grid_seasonal_amplitude\" or plottype == \"grid_range\" or plottype == \"station_delay_mean\" or \\ plottype == \"station_delay_median\" or plottype == \"station_delay_stdev\" or \\ plottype == \"station_seasonal_amplitude\" or plottype == \"grid_delay_absolute_mean\" or \\ plottype == \"grid_delay_absolute_median\" or plottype == \"grid_delay_absolute_stdev\" or \\ plottype == \"grid_seasonal_absolute_amplitude\" or plottype == \"grid_seasonal_amplitude_stdev\" or \\ plottype == \"grid_seasonal_absolute_amplitude_stdev\" or plottype == \"grid_seasonal_fit_rmse\" or \\ plottype == \"grid_seasonal_absolute_fit_rmse\" or plottype == \"grid_variogram_rmse\" : # update label if sigZTD if 'sig' in self . col_name : cbar_ax . set_label ( \"sig ZTD \" + \" \" . join ( plottype . replace ( 'grid_' , '' ) . replace ( 'delay_' , '' ) . split ( '_' )) . title () + ' ( {} )' . format ( self . unit ), rotation =- 90 , labelpad = 10 ) else : cbar_ax . set_label ( \" \" . join ( plottype . replace ( 'grid_' , '' ) . split ( '_' )) . title () + ' ( {} )' . format ( self . unit ), rotation =- 90 , labelpad = 10 ) # specify appropriate units for phase heatmap (days) elif plottype == \"station_seasonal_phase\" or plottype == \"grid_seasonal_phase\" or plottype == \"grid_seasonal_absolute_phase\" or \\ plottype == \"grid_seasonal_absolute_phase_stdev\" or plottype == \"grid_seasonal_phase_stdev\" : cbar_ax . set_label ( \" \" . join ( plottype . replace ( 'grid_' , '' ) . split ( '_' )) . title () + ' ( {} )' . format ( 'days' ), rotation =- 90 , labelpad = 10 ) # specify appropriate units for period heatmap (years) elif plottype == \"station_delay_period\" or plottype == \"grid_seasonal_period\" or plottype == \"grid_seasonal_absolute_period\" or \\ plottype == \"grid_seasonal_absolute_period_stdev\" or plottype == \"grid_seasonal_period_stdev\" : cbar_ax . set_label ( \" \" . join ( plottype . replace ( 'grid_' , '' ) . split ( '_' )) . title () + ' ( {} )' . format ( 'years' ), rotation =- 90 , labelpad = 10 ) # gridmap of station density has no units else : cbar_ax . set_label ( \" \" . join ( plottype . replace ( 'grid_' , '' ) . split ( '_' )) . title (), rotation =- 90 , labelpad = 10 ) # Add title to plots, if specified if userTitle : axes . set_title ( userTitle , zorder = 2 ) # save/close figure # cbar_ax.ax.locator_params(nbins=10) # for label in cbar_ax.ax.xaxis.get_ticklabels()[::25]: # label.set_visible(False) plt . savefig ( os . path . join ( workdir , self . col_name + '_' + plottype + '.' + plotFormat ), format = plotFormat , bbox_inches = 'tight' ) plt . close () return __call__ ( gridarr , plottype , workdir = './' , drawgridlines = False , colorbarfmt = ' %.2e ' , stationsongrids = None , resValue = 5 , plotFormat = 'pdf' , userTitle = None ) Visualize a suite of statistics w.r.t. stations. Pass either a list of points or a gridded array as the first argument. Alternatively, you may superimpose your gridded array with a supplementary list of points by passing the latter through the stationsongrids argument. Source code in RAiDER/cli/statsPlot.py 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 def __call__ ( self , gridarr , plottype , workdir = './' , drawgridlines = False , colorbarfmt = ' %.2e ' , stationsongrids = None , resValue = 5 , plotFormat = 'pdf' , userTitle = None ): ''' Visualize a suite of statistics w.r.t. stations. Pass either a list of points or a gridded array as the first argument. Alternatively, you may superimpose your gridded array with a supplementary list of points by passing the latter through the stationsongrids argument. ''' from cartopy import crs as ccrs from cartopy import feature as cfeature from cartopy.mpl.ticker import LatitudeFormatter , LongitudeFormatter from matplotlib import ticker as mticker from mpl_toolkits.axes_grid1 import make_axes_locatable # If specified workdir doesn't exist, create it if not os . path . exists ( workdir ): os . mkdir ( workdir ) # Pass cbounds cbounds = self . cbounds # Initiate no-data array to mask data nodat_arr = [ 0 , np . nan , np . inf ] if self . unit in [ 'minute' , 'hour' , 'day' , 'year' ]: colorbarfmt = ' %.1i ' nodat_arr = [ np . nan , np . inf ] fig , axes = plt . subplots ( subplot_kw = { 'projection' : ccrs . PlateCarree ()}) # by default set background to white axes . add_feature ( cfeature . NaturalEarthFeature ( 'physical' , 'land' , '50m' , facecolor = 'white' ), zorder = 0 ) axes . set_extent ( self . plotbbox , ccrs . PlateCarree ()) # add coastlines axes . coastlines ( linewidth = 0.2 , color = \"gray\" , zorder = 4 ) cmap = copy . copy ( mpl . cm . get_cmap ( self . usr_colormap )) # cmap.set_bad('black', 0.) # extract all colors from the hot map cmaplist = [ cmap ( i ) for i in range ( cmap . N )] # create the new map cmap = mpl . colors . LinearSegmentedColormap . from_list ( 'Custom cmap' , cmaplist ) axes . set_xlabel ( 'Longitude' , weight = 'bold' , zorder = 2 ) axes . set_ylabel ( 'Latitude' , weight = 'bold' , zorder = 2 ) # set ticks axes . set_xticks ( np . linspace ( self . plotbbox [ 0 ], self . plotbbox [ 1 ], 5 ), crs = ccrs . PlateCarree ()) axes . set_yticks ( np . linspace ( self . plotbbox [ 2 ], self . plotbbox [ 3 ], 5 ), crs = ccrs . PlateCarree ()) lon_formatter = LongitudeFormatter ( number_format = '.0f' , degree_symbol = '' ) lat_formatter = LatitudeFormatter ( number_format = '.0f' , degree_symbol = '' ) axes . xaxis . set_major_formatter ( lon_formatter ) axes . yaxis . set_major_formatter ( lat_formatter ) # draw central longitude lines corresponding to respective datetimes if self . time_lines : tl = axes . grid ( axis = 'x' , linewidth = 1.5 , color = 'blue' , alpha = 0.5 , linestyle = '-' , zorder = 3 ) # If individual stations passed if isinstance ( gridarr , list ): # spatial distribution of stations if plottype == \"station_distribution\" : im = axes . scatter ( gridarr [ 0 ], gridarr [ 1 ], zorder = 1 , s = 0.5 , marker = '.' , color = 'b' , transform = ccrs . PlateCarree ()) # passing 3rd column as z-value if len ( gridarr ) > 2 : # set land/water background to light gray/blue respectively so station point data can be seen axes . add_feature ( cfeature . NaturalEarthFeature ( 'physical' , 'land' , '50m' , facecolor = '#A9A9A9' ), zorder = 0 ) axes . add_feature ( cfeature . NaturalEarthFeature ( 'physical' , 'ocean' , '50m' , facecolor = '#ADD8E6' ), zorder = 0 ) # set masked values as nans zvalues = gridarr [ 2 ] for i in nodat_arr : zvalues = np . ma . masked_where ( zvalues == i , zvalues ) zvalues = np . ma . filled ( zvalues , np . nan ) # define the bins and normalize if cbounds is None : # avoid \"ufunc 'isnan'\" error by casting array as float cbounds = [ np . nanpercentile ( zvalues . astype ( 'float' ), self . colorpercentile [ 0 ]), np . nanpercentile ( zvalues . astype ( 'float' ), self . colorpercentile [ 1 ])] # if upper/lower bounds identical, overwrite lower bound as 75% of upper bound to avoid plotting ValueError if cbounds [ 0 ] == cbounds [ 1 ]: cbounds [ 0 ] *= 0.75 cbounds . sort () # adjust precision for colorbar if necessary if ( abs ( np . nanmax ( zvalues ) - np . nanmin ( zvalues )) < 1 and ( np . nanmean ( zvalues )) < 1 ) \\ or abs ( np . nanmax ( zvalues ) - np . nanmin ( zvalues )) > 500 : colorbarfmt = ' %.2e ' colorbounds = np . linspace ( cbounds [ 0 ], cbounds [ 1 ], 256 ) colorbounds = np . unique ( colorbounds ) norm = mpl . colors . BoundaryNorm ( colorbounds , cmap . N ) colorbounds_ticks = np . linspace ( cbounds [ 0 ], cbounds [ 1 ], 10 ) # plot data and initiate colorbar im = axes . scatter ( gridarr [ 0 ], gridarr [ 1 ], c = zvalues , cmap = cmap , norm = norm , zorder = 1 , s = 0.5 , marker = '.' , transform = ccrs . PlateCarree ()) # initiate colorbar and control height of colorbar divider = make_axes_locatable ( axes ) cax = divider . append_axes ( \"right\" , size = \"5%\" , pad = 0.05 , axes_class = plt . Axes ) cbar_ax = fig . colorbar ( im , spacing = 'proportional' , ticks = colorbounds_ticks , boundaries = colorbounds , format = colorbarfmt , pad = 0.1 , cax = cax ) # If gridded area passed else : # set masked values as nans for i in nodat_arr : gridarr = np . ma . masked_where ( gridarr == i , gridarr ) gridarr = np . ma . filled ( gridarr , np . nan ) # set land/water background to light gray/blue respectively so grid cells can be seen axes . add_feature ( cfeature . NaturalEarthFeature ( 'physical' , 'land' , '50m' , facecolor = '#A9A9A9' ), zorder = 0 ) axes . add_feature ( cfeature . NaturalEarthFeature ( 'physical' , 'ocean' , '50m' , facecolor = '#ADD8E6' ), zorder = 0 ) # define the bins and normalize if cbounds is None : cbounds = [ np . nanpercentile ( gridarr , self . colorpercentile [ 0 ]), np . nanpercentile ( gridarr , self . colorpercentile [ 1 ])] # if upper/lower bounds identical, overwrite lower bound as 75% of upper bound to avoid plotting ValueError if cbounds [ 0 ] == cbounds [ 1 ]: cbounds [ 0 ] *= 0.75 cbounds . sort () # plot data and initiate colorbar if ( abs ( np . nanmax ( gridarr ) - np . nanmin ( gridarr )) < 1 and abs ( np . nanmean ( gridarr )) < 1 ) \\ or abs ( np . nanmax ( gridarr ) - np . nanmin ( gridarr )) > 500 : colorbarfmt = ' %.2e ' colorbounds = np . linspace ( cbounds [ 0 ], cbounds [ 1 ], 256 ) colorbounds = np . unique ( colorbounds ) norm = mpl . colors . BoundaryNorm ( colorbounds , cmap . N ) colorbounds_ticks = np . linspace ( cbounds [ 0 ], cbounds [ 1 ], 10 ) # plot data im = axes . imshow ( gridarr , cmap = cmap , norm = norm , extent = self . plotbbox , zorder = 1 , origin = 'upper' , transform = ccrs . PlateCarree ()) # initiate colorbar and control height of colorbar divider = make_axes_locatable ( axes ) cax = divider . append_axes ( \"right\" , size = \"5%\" , pad = 0.05 , axes_class = plt . Axes ) cbar_ax = fig . colorbar ( im , spacing = 'proportional' , ticks = colorbounds_ticks , boundaries = colorbounds , format = colorbarfmt , pad = 0.1 , cax = cax ) # superimpose your gridded array with a supplementary list of point, if specified if self . stationsongrids : axes . scatter ( self . stationsongrids [ 0 ], self . stationsongrids [ 1 ], zorder = 2 , s = 0.5 , marker = '.' , color = 'b' , transform = ccrs . PlateCarree ()) # draw gridlines, if specified if drawgridlines : gl = axes . gridlines ( crs = ccrs . PlateCarree ( ), linewidth = 0.5 , color = 'black' , alpha = 0.5 , linestyle = '-' , zorder = 3 ) gl . xlocator = mticker . FixedLocator ( np . arange ( self . plotbbox [ 0 ], self . plotbbox [ 1 ] + self . spacing , self . spacing ) . tolist ()) gl . ylocator = mticker . FixedLocator ( np . arange ( self . plotbbox [ 2 ], self . plotbbox [ 3 ] + self . spacing , self . spacing ) . tolist ()) # Add labels to colorbar, if necessary if 'cbar_ax' in locals (): # experimental variogram fit sill heatmap if plottype == \"grid_variance\" : cbar_ax . set_label ( \" \" . join ( plottype . replace ( 'grid_' , '' ) . split ( '_' )) . title () + ' ( {} \\u00b2 )' . format ( self . unit ), rotation =- 90 , labelpad = 10 ) # specify appropriate units for mean/median/std/amplitude/experimental variogram fit heatmap elif plottype == \"grid_delay_mean\" or plottype == \"grid_delay_median\" or plottype == \"grid_delay_stdev\" or \\ plottype == \"grid_seasonal_amplitude\" or plottype == \"grid_range\" or plottype == \"station_delay_mean\" or \\ plottype == \"station_delay_median\" or plottype == \"station_delay_stdev\" or \\ plottype == \"station_seasonal_amplitude\" or plottype == \"grid_delay_absolute_mean\" or \\ plottype == \"grid_delay_absolute_median\" or plottype == \"grid_delay_absolute_stdev\" or \\ plottype == \"grid_seasonal_absolute_amplitude\" or plottype == \"grid_seasonal_amplitude_stdev\" or \\ plottype == \"grid_seasonal_absolute_amplitude_stdev\" or plottype == \"grid_seasonal_fit_rmse\" or \\ plottype == \"grid_seasonal_absolute_fit_rmse\" or plottype == \"grid_variogram_rmse\" : # update label if sigZTD if 'sig' in self . col_name : cbar_ax . set_label ( \"sig ZTD \" + \" \" . join ( plottype . replace ( 'grid_' , '' ) . replace ( 'delay_' , '' ) . split ( '_' )) . title () + ' ( {} )' . format ( self . unit ), rotation =- 90 , labelpad = 10 ) else : cbar_ax . set_label ( \" \" . join ( plottype . replace ( 'grid_' , '' ) . split ( '_' )) . title () + ' ( {} )' . format ( self . unit ), rotation =- 90 , labelpad = 10 ) # specify appropriate units for phase heatmap (days) elif plottype == \"station_seasonal_phase\" or plottype == \"grid_seasonal_phase\" or plottype == \"grid_seasonal_absolute_phase\" or \\ plottype == \"grid_seasonal_absolute_phase_stdev\" or plottype == \"grid_seasonal_phase_stdev\" : cbar_ax . set_label ( \" \" . join ( plottype . replace ( 'grid_' , '' ) . split ( '_' )) . title () + ' ( {} )' . format ( 'days' ), rotation =- 90 , labelpad = 10 ) # specify appropriate units for period heatmap (years) elif plottype == \"station_delay_period\" or plottype == \"grid_seasonal_period\" or plottype == \"grid_seasonal_absolute_period\" or \\ plottype == \"grid_seasonal_absolute_period_stdev\" or plottype == \"grid_seasonal_period_stdev\" : cbar_ax . set_label ( \" \" . join ( plottype . replace ( 'grid_' , '' ) . split ( '_' )) . title () + ' ( {} )' . format ( 'years' ), rotation =- 90 , labelpad = 10 ) # gridmap of station density has no units else : cbar_ax . set_label ( \" \" . join ( plottype . replace ( 'grid_' , '' ) . split ( '_' )) . title (), rotation =- 90 , labelpad = 10 ) # Add title to plots, if specified if userTitle : axes . set_title ( userTitle , zorder = 2 ) # save/close figure # cbar_ax.ax.locator_params(nbins=10) # for label in cbar_ax.ax.xaxis.get_ticklabels()[::25]: # label.set_visible(False) plt . savefig ( os . path . join ( workdir , self . col_name + '_' + plottype + '.' + plotFormat ), format = plotFormat , bbox_inches = 'tight' ) plt . close () return create_DF () Create dataframe. Source code in RAiDER/cli/statsPlot.py 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 def create_DF ( self ): ''' Create dataframe. ''' # Open file self . df = self . _reader () # Filter dataframe # drop all nans self . df . dropna ( how = 'any' , inplace = True ) self . df . reset_index ( drop = True , inplace = True ) # convert to datetime object # time-interval filter if self . timeinterval : self . timeinterval = [ dt . datetime . strptime ( val , '%Y-%m- %d ' ) for val in self . timeinterval . split ()] self . df = self . df [( self . df [ 'Date' ] >= self . timeinterval [ 0 ]) & ( self . df [ 'Date' ] <= self . timeinterval [ - 1 ])] # seasonal filter if self . seasonalinterval : self . seasonalinterval = self . seasonalinterval . split () # get day of year self . seasonalinterval = [ dt . datetime . strptime ( '2001-' + self . seasonalinterval [ 0 ], '%Y-%m- %d ' ) . timetuple ( ) . tm_yday , dt . datetime . strptime ( '2001-' + self . seasonalinterval [ - 1 ], '%Y-%m- %d ' ) . timetuple () . tm_yday ] # track input order and wrap around year if necessary # e.g. month/day: 03/01 to 06/01 if self . seasonalinterval [ 0 ] < self . seasonalinterval [ 1 ]: # non leap-year filtered_self = self . df [( not self . df [ 'Date' ] . dt . is_leap_year ) & ( self . df [ 'Date' ] . dt . dayofyear >= self . seasonalinterval [ 0 ]) & ( self . df [ 'Date' ] . dt . dayofyear <= self . seasonalinterval [ - 1 ])] # leap-year self . seasonalinterval = [ i + 1 if i > 59 else i for i in self . seasonalinterval ] self . df = filtered_self . append ( self . df [( self . df [ 'Date' ] . dt . is_leap_year ) & ( self . df [ 'Date' ] . dt . dayofyear >= self . seasonalinterval [ 0 ]) & ( self . df [ 'Date' ] . dt . dayofyear <= self . seasonalinterval [ - 1 ])], ignore_index = True ) del filtered_self # e.g. month/day: 12/01 to 03/01 if self . seasonalinterval [ 0 ] > self . seasonalinterval [ 1 ]: # non leap-year filtered_self = self . df [( not self . df [ 'Date' ] . dt . is_leap_year ) & ( self . df [ 'Date' ] . dt . dayofyear >= self . seasonalinterval [ - 1 ]) & ( self . df [ 'Date' ] . dt . dayofyear <= self . seasonalinterval [ 0 ])] # leap-year self . seasonalinterval = [ i + 1 if i > 59 else i for i in self . seasonalinterval ] self . df = filtered_self . append ( self . df [( self . df [ 'Date' ] . dt . is_leap_year ) & ( self . df [ 'Date' ] . dt . dayofyear >= self . seasonalinterval [ - 1 ]) & ( self . df [ 'Date' ] . dt . dayofyear <= self . seasonalinterval [ 0 ])], ignore_index = True ) del filtered_self # estimate central longitude lines if '--time_lines' specified if self . time_lines and 'Datetime' in self . df . keys (): self . df [ 'Date_hr' ] = self . df [ 'Datetime' ] . dt . hour . astype ( float ) . astype ( \"Int32\" ) # get list of unique times all_hrs = sorted ( set ( self . df [ 'Date_hr' ])) # get central longitude bands associated with each time central_points = [] # if single time, avoid loop if len ( all_hrs ) == 1 : central_points . append (([ 0 , max ( self . df [ 'Lon' ])], [ 0 , min ( self . df [ 'Lon' ])])) else : for i in enumerate ( all_hrs ): # last entry if i [ 0 ] == len ( all_hrs ) - 1 : lons = self . df [ self . df [ 'Date_hr' ] > all_hrs [ i [ 0 ] - 1 ]] # first entry elif i [ 0 ] == 0 : lons = self . df [ self . df [ 'Date_hr' ] < all_hrs [ i [ 0 ] + 1 ]] else : lons = self . df [( self . df [ 'Date_hr' ] > all_hrs [ i [ 0 ] - 1 ]) & ( self . df [ 'Date_hr' ] < all_hrs [ i [ 0 ] + 1 ])] central_points . append (([ 0 , max ( lons [ 'Lon' ])], [ 0 , min ( lons [ 'Lon' ])])) # get central longitudes self . time_lines = [ midpoint ( i [ 0 ], i [ 1 ]) for i in central_points ] # Get bbox, buffered by grid spacing. # Check if bbox input is valid list. if self . bbox is not None : try : self . bbox = [ float ( val ) for val in self . bbox . split ()] except BaseException : raise Exception ( 'Cannot understand the --bounding_box argument. String input is incorrect or path does not exist.' ) self . plotbbox , self . grid_dim , self . gridpoints = self . _get_extent () # generate list of grid-polygons append_poly = [] for i in self . gridpoints : bbox = [ i [ 1 ] - ( self . spacing / 2 ), i [ 1 ] + ( self . spacing / 2 ), i [ 0 ] - ( self . spacing / 2 ), i [ 0 ] + ( self . spacing / 2 )] append_poly . append ( Polygon ( np . column_stack (( np . array ([ bbox [ 2 ], bbox [ 3 ], bbox [ 3 ], bbox [ 2 ], bbox [ 2 ]]), np . array ([ bbox [ 0 ], bbox [ 0 ], bbox [ 1 ], bbox [ 1 ], bbox [ 0 ]]))))) # Pass lons/lats to create polygon # Check for grid cell intersection with each station idtogrid_dict = {} self . unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' ]) . size () self . unique_points = [ self . unique_points . index . get_level_values ( 'ID' ) . tolist (), self . unique_points . index . get_level_values ( 'Lon' ) . tolist (), self . unique_points . index . get_level_values ( 'Lat' ) . tolist ()] # Initiate R-tree of gridded array domain self . polygon_dict = dict (( id ( pt ), i ) for i , pt in enumerate ( append_poly )) self . polygon_tree = STRtree ( append_poly ) for stat_ID in self . unique_points [ 0 ]: grd_index = self . _check_stationgrid_intersection ( stat_ID ) idtogrid_dict [ stat_ID ] = grd_index # map gridnode dictionary to dataframe self . df [ 'gridnode' ] = self . df [ 'ID' ] . map ( idtogrid_dict ) self . df = self . df [ self . df [ 'gridnode' ] . astype ( str ) != 'NaN' ] del self . unique_points , self . polygon_dict , self . polygon_tree , idtogrid_dict , append_poly # sort by grid and date self . df . sort_values ([ 'gridnode' , 'Date' ]) # If specified, pass station locations to superimpose on gridplots if self . stationsongrids : unique_points = self . df . groupby ([ 'Lon' , 'Lat' ]) . size () self . stationsongrids = [ unique_points . index . get_level_values ( 'Lon' ) . tolist (), unique_points . index . get_level_values ( 'Lat' ) . tolist ()] # If specified, setup gridded array(s) if self . grid_heatmap : self . grid_heatmap = np . array ([ np . nan if i [ 0 ] not in self . df [ 'gridnode' ] . values [:] else int ( len ( np . unique ( self . df [ 'ID' ][ self . df [ 'gridnode' ] == i [ 0 ]]))) for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_heatmap' + '.tif' ) save_gridfile ( self . grid_heatmap , 'grid_heatmap' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %1i ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'int16' , noData = 0 ) if self . grid_delay_mean : # Take mean of station-wise means per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ self . col_name ] . mean () unique_points = unique_points . groupby ([ 'gridnode' ])[ self . col_name ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_delay_mean = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_delay_mean' + '.tif' ) save_gridfile ( self . grid_delay_mean , 'grid_delay_mean' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.2f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) if self . grid_delay_median : # Take mean of station-wise medians per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ self . col_name ] . median () unique_points = unique_points . groupby ([ 'gridnode' ])[ self . col_name ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_delay_median = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_delay_median' + '.tif' ) save_gridfile ( self . grid_delay_median , 'grid_delay_median' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.2f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) if self . grid_delay_stdev : # Take mean of station-wise stdev per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ self . col_name ] . std () unique_points = unique_points . groupby ([ 'gridnode' ])[ self . col_name ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_delay_stdev = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_delay_stdev' + '.tif' ) save_gridfile ( self . grid_delay_stdev , 'grid_delay_stdev' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.2f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) if self . grid_delay_absolute_mean : # Take mean of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ self . col_name ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_delay_absolute_mean = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_delay_absolute_mean' + '.tif' ) save_gridfile ( self . grid_delay_absolute_mean , 'grid_delay_absolute_mean' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.2f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) if self . grid_delay_absolute_median : # Take median of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ self . col_name ] . median () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_delay_absolute_median = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_delay_absolute_median' + '.tif' ) save_gridfile ( self . grid_delay_absolute_median , 'grid_delay_absolute_median' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.2f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) if self . grid_delay_absolute_stdev : # Take stdev of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ self . col_name ] . std () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_delay_absolute_stdev = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_delay_absolute_stdev' + '.tif' ) save_gridfile ( self . grid_delay_absolute_stdev , 'grid_delay_absolute_stdev' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.2f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # If specified, compute phase/amplitude fits if self . station_seasonal_phase or self . grid_seasonal_phase or self . grid_seasonal_absolute_phase : # Sort by coordinates unique_points = self . df . sort_values ([ 'ID' , 'Date' ]) unique_points [ 'Date' ] = [ i . timestamp () for i in unique_points [ 'Date' ]] # Setup variables self . ampfit = [] self . phsfit = [] self . periodfit = [] self . ampfit_c = [] self . phsfit_c = [] self . periodfit_c = [] self . seasonalfit_rmse = [] args = [] for i in sorted ( list ( set ( unique_points [ 'ID' ]))): # pass all values corresponding to station (ID, data = y, time = x) args . append (( i , unique_points [ unique_points [ 'ID' ] == i ][ 'Date' ] . to_list (), unique_points [ unique_points [ 'ID' ] == i ][ self . col_name ] . to_list (), self . min_span [ 0 ], self . min_span [ 1 ], self . period_limit )) # Parallelize iteration through all grid-cells and time slices with multiprocessing . Pool ( self . numCPUs ) as multipool : for i , j , k , l , m , n , o in multipool . starmap ( self . _amplitude_and_phase , args ): self . ampfit . extend ( i ) self . phsfit . extend ( j ) self . periodfit . extend ( k ) self . ampfit_c . extend ( l ) self . phsfit_c . extend ( m ) self . periodfit_c . extend ( n ) self . seasonalfit_rmse . extend ( o ) # map phase/amplitude fits dictionary to dataframe self . phsfit = { k : v for d in self . phsfit for k , v in d . items ()} self . ampfit = { k : v for d in self . ampfit for k , v in d . items ()} self . periodfit = { k : v for d in self . periodfit for k , v in d . items ()} self . df [ 'phsfit' ] = self . df [ 'ID' ] . map ( self . phsfit ) # check if there are any valid data values if self . df [ 'phsfit' ] . isnull () . values . all ( axis = 0 ): raise Exception ( \"No valid data values, adjust --min_span inputs for time span in years {} and/or fractional obs. {} \" . format ( self . min_span [ 0 ], self . min_span [ 1 ])) self . df [ 'ampfit' ] = self . df [ 'ID' ] . map ( self . ampfit ) self . df [ 'periodfit' ] = self . df [ 'ID' ] . map ( self . periodfit ) self . phsfit_c = { k : v for d in self . phsfit_c for k , v in d . items ()} self . ampfit_c = { k : v for d in self . ampfit_c for k , v in d . items ()} self . periodfit_c = { k : v for d in self . periodfit_c for k , v in d . items ()} self . seasonalfit_rmse = { k : v for d in self . seasonalfit_rmse for k , v in d . items ()} self . df [ 'phsfit_c' ] = self . df [ 'ID' ] . map ( self . phsfit_c ) self . df [ 'ampfit_c' ] = self . df [ 'ID' ] . map ( self . ampfit_c ) self . df [ 'periodfit_c' ] = self . df [ 'ID' ] . map ( self . periodfit_c ) self . df [ 'seasonalfit_rmse' ] = self . df [ 'ID' ] . map ( self . seasonalfit_rmse ) # drop nan self . df . dropna ( how = 'any' , inplace = True ) # If grid plots specified if self . grid_seasonal_phase : # Pass mean phase of station-wise means per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ 'phsfit' ] . mean () unique_points = unique_points . groupby ([ 'gridnode' ])[ 'phsfit' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_phase = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_phase' + '.tif' ) save_gridfile ( self . grid_seasonal_phase , 'grid_seasonal_phase' , gridfile_name , self . plotbbox , self . spacing , 'days' , colorbarfmt = ' %.1i ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass mean amplitude of station-wise means per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ 'ampfit' ] . mean () unique_points = unique_points . groupby ([ 'gridnode' ])[ 'ampfit' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_amplitude = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_amplitude' + '.tif' ) save_gridfile ( self . grid_seasonal_amplitude , 'grid_seasonal_amplitude' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.3f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass mean period of station-wise means per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ 'periodfit' ] . mean () unique_points = unique_points . groupby ([ 'gridnode' ])[ 'periodfit' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_period = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_period' + '.tif' ) save_gridfile ( self . grid_seasonal_period , 'grid_seasonal_period' , gridfile_name , self . plotbbox , self . spacing , 'years' , colorbarfmt = ' %.2f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) ######################################################################################################################## # Pass mean phase stdev of station-wise means per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ 'phsfit_c' ] . mean () unique_points = unique_points . groupby ([ 'gridnode' ])[ 'phsfit_c' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_phase_stdev = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_phase_stdev' + '.tif' ) save_gridfile ( self . grid_seasonal_phase_stdev , 'grid_seasonal_phase_stdev' , gridfile_name , self . plotbbox , self . spacing , 'days' , colorbarfmt = ' %.1i ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass mean amplitude stdev of station-wise means per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ 'ampfit_c' ] . mean () unique_points = unique_points . groupby ([ 'gridnode' ])[ 'ampfit_c' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_amplitude_stdev = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_amplitude_stdev' + '.tif' ) save_gridfile ( self . grid_seasonal_amplitude_stdev , 'grid_seasonal_amplitude_stdev' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.3f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass mean period stdev of station-wise means per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ 'periodfit_c' ] . mean () unique_points = unique_points . groupby ([ 'gridnode' ])[ 'periodfit_c' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_period_stdev = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_period_stdev' + '.tif' ) save_gridfile ( self . grid_seasonal_period_stdev , 'grid_seasonal_period_stdev' , gridfile_name , self . plotbbox , self . spacing , 'years' , colorbarfmt = ' %.2e ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass mean seasonal fit RMSE of station-wise means per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ 'seasonalfit_rmse' ] . mean () unique_points = unique_points . groupby ([ 'gridnode' ])[ 'seasonalfit_rmse' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_fit_rmse = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_fit_rmse' + '.tif' ) save_gridfile ( self . grid_seasonal_fit_rmse , 'grid_seasonal_fit_rmse' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.3f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) ######################################################################################################################## if self . grid_seasonal_absolute_phase : # Pass absolute mean phase of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ 'phsfit' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_absolute_phase = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_absolute_phase' + '.tif' ) save_gridfile ( self . grid_seasonal_absolute_phase , 'grid_seasonal_absolute_phase' , gridfile_name , self . plotbbox , self . spacing , 'days' , colorbarfmt = ' %.1i ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass absolute mean amplitude of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ 'ampfit' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_absolute_amplitude = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_absolute_amplitude' + '.tif' ) save_gridfile ( self . grid_seasonal_absolute_amplitude , 'grid_seasonal_absolute_amplitude' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.3f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass absolute mean period of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ 'periodfit' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_absolute_period = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_absolute_period' + '.tif' ) save_gridfile ( self . grid_seasonal_absolute_period , 'grid_seasonal_absolute_period' , gridfile_name , self . plotbbox , self . spacing , 'years' , colorbarfmt = ' %.2f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) ######################################################################################################################## # Pass absolute mean phase stdev of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ 'phsfit_c' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_absolute_phase_stdev = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_absolute_phase_stdev' + '.tif' ) save_gridfile ( self . grid_seasonal_absolute_phase_stdev , 'grid_seasonal_absolute_phase_stdev' , gridfile_name , self . plotbbox , self . spacing , 'days' , colorbarfmt = ' %.1i ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass absolute mean amplitude stdev of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ 'ampfit_c' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_absolute_amplitude_stdev = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_absolute_amplitude_stdev' + '.tif' ) save_gridfile ( self . grid_seasonal_absolute_amplitude_stdev , 'grid_seasonal_absolute_amplitude_stdev' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.3f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass absolute mean period stdev of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ 'periodfit_c' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_absolute_period_stdev = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_absolute_period_stdev' + '.tif' ) save_gridfile ( self . grid_seasonal_absolute_period_stdev , 'grid_seasonal_absolute_period_stdev' , gridfile_name , self . plotbbox , self . spacing , 'years' , colorbarfmt = ' %.2e ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass absolute mean seasonal fit RMSE of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ 'seasonalfit_rmse' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_absolute_fit_rmse = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_absolute_fit_rmse' + '.tif' ) save_gridfile ( self . grid_seasonal_absolute_fit_rmse , 'grid_seasonal_absolute_fit_rmse' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.2e ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) VariogramAnalysis Class which ingests dataframe output from 'RaiderStats' class and performs variogram analysis. Source code in RAiDER/cli/statsPlot.py 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 class VariogramAnalysis (): ''' Class which ingests dataframe output from 'RaiderStats' class and performs variogram analysis. ''' def __init__ ( self , filearg , gridpoints , col_name , unit = 'm' , workdir = './' , seasonalinterval = None , densitythreshold = 10 , binnedvariogram = False , numCPUs = 8 , variogram_per_timeslice = False , variogram_errlimit = 'inf' ): self . df = filearg self . col_name = col_name self . unit = unit self . gridpoints = gridpoints self . workdir = workdir self . seasonalinterval = seasonalinterval self . densitythreshold = densitythreshold self . binnedvariogram = binnedvariogram self . numCPUs = numCPUs self . variogram_per_timeslice = variogram_per_timeslice self . variogram_errlimit = float ( variogram_errlimit ) def _get_samples ( self , data , Nsamp = 1000 ): ''' pull samples from a 2D image for variogram analysis ''' import random if len ( data ) < self . densitythreshold : logger . warning ( 'Less than {} points for this gridcell' , self . densitythreshold ) logger . info ( 'Will pass empty list' ) d = [] indpars = [] else : indpars = list ( itertools . combinations ( range ( len ( data )), 2 )) random . shuffle ( indpars ) # subsample Nvalidsamp = int ( len ( data ) * ( len ( data ) - 1 ) / 2 ) # Only downsample if Nsamps>specified value if Nvalidsamp > Nsamp : indpars = indpars [: Nsamp ] d = np . array ([[ data [ r [ 0 ]], data [ r [ 1 ]]] for r in indpars ]) return d , indpars def _get_XY ( self , x2d , y2d , indpars ): ''' Given a list of indices, return the x,y locations from two matrices ''' x = np . array ([[ x2d [ r [ 0 ]], x2d [ r [ 1 ]]] for r in indpars ]) y = np . array ([[ y2d [ r [ 0 ]], y2d [ r [ 1 ]]] for r in indpars ]) return x , y def _get_distances ( self , XY ): ''' Return the distances between each point in a list of points ''' from scipy.spatial.distance import cdist return np . diag ( cdist ( XY [:, :, 0 ], XY [:, :, 1 ], metric = 'euclidean' )) def _get_variogram ( self , XY , xy = None ): ''' Return variograms ''' return 0.5 * np . square ( XY - xy ) # XY = 1st col xy= 2nd col def _emp_vario ( self , x , y , data , Nsamp = 1000 ): ''' Compute empirical semivariance ''' # remove NaNs if possible mask = ~ np . isnan ( data ) if False in mask : data = data [ mask ] x = x [ mask ] y = y [ mask ] # deramp temp1 , temp2 , x , y = WGS84_to_UTM ( x , y , common_center = True ) A = np . array ([ x , y , np . ones ( len ( x ))]) . T ramp = np . linalg . lstsq ( A , data . T , rcond = None )[ 0 ] data = data - ( np . matmul ( A , ramp )) samples , indpars = self . _get_samples ( data , Nsamp ) x , y = self . _get_XY ( x , y , indpars ) dists = self . _get_distances ( np . array ([[ x [:, 0 ], y [:, 0 ]], [ x [:, 1 ], y [:, 1 ]]]) . T ) vario = self . _get_variogram ( samples [:, 0 ], samples [:, 1 ]) return dists , vario def _binned_vario ( self , hEff , rawVario , xBin = None ): ''' return a binned empirical variogram ''' if xBin is None : with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , message = \"All-NaN slice encountered\" ) xBin = np . linspace ( 0 , np . nanmax ( hEff ) * .67 , 20 ) nBins = len ( xBin ) - 1 hExp , expVario = [], [] for iBin in range ( nBins ): iBinMask = np . logical_and ( xBin [ iBin ] < hEff , hEff <= xBin [ iBin + 1 ]) # circumvent indexing try : with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , message = \"Mean of empty slice\" ) hExp . append ( np . nanmean ( hEff [ iBinMask ])) expVario . append ( np . nanmean ( rawVario [ iBinMask ])) except BaseException : # TODO: Which error(s)? pass if False in ~ np . isnan ( hExp ): # NaNs present in binned histogram hExp = [ x for x in hExp if str ( x ) != 'nan' ] expVario = [ x for x in expVario if str ( x ) != 'nan' ] return np . array ( hExp ), np . array ( expVario ) def _fit_vario ( self , dists , vario , model = None , x0 = None , Nparm = None , ub = None ): ''' Fit a variogram model to data ''' from scipy.optimize import least_squares def resid ( x , d , v , m ): return ( m ( x , d ) - v ) if ub is None : with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , message = \"All-NaN slice encountered\" ) ub = np . array ([ np . nanmax ( dists ) * 0.8 , np . nanmax ( vario ) * 0.8 , np . nanmax ( vario ) * 0.8 ]) if x0 is None and Nparm is None : raise RuntimeError ( 'Must specify either x0 or the number of model parameters' ) if x0 is not None : lb = np . zeros ( len ( x0 )) if Nparm is not None : lb = np . zeros ( Nparm ) x0 = ( ub - lb ) / 2 bounds = ( lb , ub ) mask = np . isnan ( dists ) | np . isnan ( vario ) d = dists [ ~ mask ] . copy () v = vario [ ~ mask ] . copy () res_robust = least_squares ( resid , x0 , bounds = bounds , loss = 'soft_l1' , f_scale = 0.1 , args = ( d , v , model )) with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , message = \"All-NaN slice encountered\" ) d_test = np . linspace ( 0 , np . nanmax ( dists ), 100 ) # v_test is my y., # res_robust.x =a, b, c, where a = range, b = sill, and c = nugget model, d_test=x v_test = model ( res_robust . x , d_test ) return res_robust , d_test , v_test # this would be expontential plus nugget def __exponential__ ( self , parms , h , nugget = False ): ''' returns a variogram model given a set of arguments and key-word arguments ''' # a = range, b = sill, c = nugget model a , b , c = parms with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , message = \"overflow encountered in true_divide\" ) if nugget : return b * ( 1 - np . exp ( - h / a )) + c else : return b * ( 1 - np . exp ( - h / a )) # this would be gaussian plus nugget def __gaussian__ ( self , parms , h ): ''' returns a Gaussian variogram model ''' a , b , c = parms return b * ( 1 - np . exp ( - np . square ( h ) / ( a ** 2 ))) + c def _append_variogram ( self , grid_ind , grid_subset ): ''' For a given grid-cell, iterate through time slices to generate/append empirical variogram(s) ''' # Comprehensive arrays recording data across all time epochs for given station dists_arr = [] vario_arr = [] dists_binned_arr = [] vario_binned_arr = [] res_robust_arr = [] d_test_arr = [] v_test_arr = [] for j in sorted ( list ( set ( grid_subset [ 'Date' ]))): # If insufficient sample size, skip slice and record occurence if len ( np . array ( grid_subset [ grid_subset [ 'Date' ] == j ][ self . col_name ])) < self . densitythreshold : # Record skipped [gridnode, timeslice] self . skipped_slices . append ([ grid_ind , j . strftime ( \"%Y-%m- %d \" )]) else : self . gridcenterlist . append ([ 'grid {} ' . format ( grid_ind ) + 'Lat: {} Lon: {} ' . format ( str ( self . gridpoints [ grid_ind ][ 1 ]), str ( self . gridpoints [ grid_ind ][ 0 ]))]) lonarr = np . array ( grid_subset [ grid_subset [ 'Date' ] == j ][ 'Lon' ]) latarr = np . array ( grid_subset [ grid_subset [ 'Date' ] == j ][ 'Lat' ]) delayarray = np . array ( grid_subset [ grid_subset [ 'Date' ] == j ][ self . col_name ]) # fit empirical variogram for each time AND grid dists , vario = self . _emp_vario ( lonarr , latarr , delayarray ) dists_binned , vario_binned = self . _binned_vario ( dists , vario ) # fit experimental variogram for each time AND grid, model default is exponential res_robust , d_test , v_test = self . _fit_vario ( dists_binned , vario_binned , model = self . __exponential__ , x0 = None , Nparm = 3 ) # Plot empirical + experimental variogram for this gridnode and timeslice if not os . path . exists ( os . path . join ( self . workdir , 'variograms/grid {} ' . format ( grid_ind ))): os . makedirs ( os . path . join ( self . workdir , 'variograms/grid {} ' . format ( grid_ind ))) # Make variogram plots for each time-slice if self . variogram_per_timeslice : # Plot empirical variogram for this gridnode and timeslice self . plot_variogram ( grid_ind , j . strftime ( \"%Y%m %d \" ), [ self . gridpoints [ grid_ind ][ 1 ], self . gridpoints [ grid_ind ][ 0 ]], workdir = os . path . join ( self . workdir , 'variograms/grid {} ' . format ( grid_ind )), dists = dists , vario = vario , dists_binned = dists_binned , vario_binned = vario_binned ) # Plot experimental variogram for this gridnode and timeslice self . plot_variogram ( grid_ind , j . strftime ( \"%Y%m %d \" ), [ self . gridpoints [ grid_ind ][ 1 ], self . gridpoints [ grid_ind ][ 0 ]], workdir = os . path . join ( self . workdir , 'variograms/grid {} ' . format ( grid_ind )), d_test = d_test , v_test = v_test , res_robust = res_robust . x , dists_binned = dists_binned , vario_binned = vario_binned ) # append for plotting self . good_slices . append ([ grid_ind , j . strftime ( \"%Y%m %d \" )]) dists_arr . append ( dists ) vario_arr . append ( vario ) dists_binned_arr . append ( dists_binned ) vario_binned_arr . append ( vario_binned ) res_robust_arr . append ( res_robust . x ) d_test_arr . append ( d_test ) v_test_arr . append ( v_test ) # fit experimental variogram for each grid if dists_binned_arr != []: # TODO: need to change this from accumulating binned data to raw data dists_arr = np . concatenate ( dists_arr ) . ravel () vario_arr = np . concatenate ( vario_arr ) . ravel () # if specified, passed binned empirical variograms if self . binnedvariogram : dists_binned_arr = np . concatenate ( dists_binned_arr ) . ravel () vario_binned_arr = np . concatenate ( vario_binned_arr ) . ravel () else : # dists_binned_arr = dists_arr ; vario_binned_arr = vario_arr dists_binned_arr , vario_binned_arr = self . _binned_vario ( dists_arr , vario_arr ) TOT_res_robust , TOT_d_test , TOT_v_test = self . _fit_vario ( dists_binned_arr , vario_binned_arr , model = self . __exponential__ , x0 = None , Nparm = 3 ) tot_timetag = self . good_slices [ 0 ][ 1 ] + '\u2013' + self . good_slices [ - 1 ][ 1 ] # Append TOT arrays self . TOT_good_slices . append ([ grid_ind , tot_timetag ]) self . TOT_res_robust_arr . append ( TOT_res_robust . x ) self . TOT_tot_timetag . append ( tot_timetag ) var_rmse = np . sqrt ( np . nanmean (( TOT_res_robust . fun ) ** 2 )) if var_rmse <= self . variogram_errlimit : self . TOT_res_robust_rmse . append ( var_rmse ) else : self . TOT_res_robust_rmse . append ( np . array ( np . nan )) # Plot empirical variogram for this gridnode self . plot_variogram ( grid_ind , tot_timetag , [ self . gridpoints [ grid_ind ][ 1 ], self . gridpoints [ grid_ind ][ 0 ]], workdir = os . path . join ( self . workdir , 'variograms/grid {} ' . format ( grid_ind )), dists = dists_arr , vario = vario_arr , dists_binned = dists_binned_arr , vario_binned = vario_binned_arr , seasonalinterval = self . seasonalinterval ) # Plot experimental variogram for this gridnode self . plot_variogram ( grid_ind , tot_timetag , [ self . gridpoints [ grid_ind ][ 1 ], self . gridpoints [ grid_ind ][ 0 ]], workdir = os . path . join ( self . workdir , 'variograms/grid {} ' . format ( grid_ind )), d_test = TOT_d_test , v_test = TOT_v_test , res_robust = TOT_res_robust . x , seasonalinterval = self . seasonalinterval , dists_binned = dists_binned_arr , vario_binned = vario_binned_arr ) # Record sparse grids which didn't have sufficient sample size of data through any of the timeslices else : self . sparse_grids . append ( grid_ind ) return self . TOT_good_slices , self . TOT_res_robust_arr , self . TOT_res_robust_rmse , self . gridcenterlist def create_variograms ( self ): ''' Iterate through grid-cells and time slices to generate empirical variogram(s) ''' # track data for plotting self . TOT_good_slices = [] self . TOT_res_robust_arr = [] self . TOT_res_robust_rmse = [] self . TOT_tot_timetag = [] # track pass/rejected grids self . sparse_grids = [] self . good_slices = [] self . skipped_slices = [] # record grid-centers for lookup-table self . gridcenterlist = [] args = [] for i in sorted ( list ( set ( self . df [ 'gridnode' ]))): # pass subset of all stations corresponding to given grid-cell grid_subset = self . df [ self . df [ 'gridnode' ] == i ] args . append (( i , grid_subset )) # Parallelize iteration through all grid-cells and time slices with multiprocessing . Pool ( self . numCPUs ) as multipool : for i , j , k , l in multipool . starmap ( self . _append_variogram , args ): self . TOT_good_slices . extend ( i ) self . TOT_res_robust_arr . extend ( j ) self . TOT_res_robust_rmse . extend ( k ) self . gridcenterlist . extend ( l ) # save grid-center lookup table self . gridcenterlist = [ list ( i ) for i in set ( tuple ( j ) for j in self . gridcenterlist )] self . gridcenterlist . sort ( key = lambda x : int ( x [ 0 ][ 4 : 6 ])) gridcenter = open ( ( os . path . join ( self . workdir , 'variograms/gridlocation_lookup.txt' )), \"w\" ) for element in self . gridcenterlist : gridcenter . writelines ( \" \\n \" . join ( element )) gridcenter . write ( \" \\n \" ) gridcenter . close () TOT_grids = [ i [ 0 ] for i in self . TOT_good_slices ] return TOT_grids , self . TOT_res_robust_arr , self . TOT_res_robust_rmse def plot_variogram ( self , gridID , timeslice , coords , workdir = './' , d_test = None , v_test = None , res_robust = None , dists = None , vario = None , dists_binned = None , vario_binned = None , seasonalinterval = None ): ''' Make empirical and/or experimental variogram fit plots ''' # If specified workdir doesn't exist, create it if not os . path . exists ( workdir ): os . mkdir ( workdir ) # make plot title title_str = ' \\n Lat: {:.2f} Lon: {:.2f} \\n Time: {} ' . format ( coords [ 1 ], coords [ 0 ], str ( timeslice )) if seasonalinterval : title_str += ' Season(mm/dd): {} / {} \u2013 {} / {} ' . format ( int ( timeslice [ 4 : 6 ]), int ( timeslice [ 6 : 8 ]), int ( timeslice [ - 4 : - 2 ]), int ( timeslice [ - 2 :])) if dists is not None and vario is not None : # scale from m to user-defined units dists = [ convert_SI ( i , 'm' , self . unit ) for i in dists ] plt . scatter ( dists , vario , s = 1 , facecolor = '0.5' , label = 'raw' ) if dists_binned is not None and vario_binned is not None : # scale from m to user-defined units dists_binned = [ convert_SI ( i , 'm' , self . unit ) for i in dists_binned ] plt . plot ( dists_binned , vario_binned , 'bo' , label = 'binned' ) if res_robust is not None : plt . axhline ( y = res_robust [ 1 ], color = 'g' , linestyle = '--' , label = '\u0263 \\u0332\\u00b2 ( {} \\u00b2 )' . format ( self . unit )) # scale from m to user-defined units res_robust [ 0 ] = convert_SI ( res_robust [ 0 ], 'm' , self . unit ) plt . axvline ( x = res_robust [ 0 ], color = 'c' , linestyle = '--' , label = 'h ( {} )' . format ( self . unit )) if d_test is not None and v_test is not None : # scale from m to user-defined units d_test = [ convert_SI ( i , 'm' , self . unit ) for i in d_test ] plt . plot ( d_test , v_test , 'r-' , label = 'experimental fit' ) plt . xlabel ( 'Distance ( {} )' . format ( self . unit )) plt . ylabel ( 'Dissimilarity ( {} \\u00b2 )' . format ( self . unit )) plt . legend ( bbox_to_anchor = ( 1.02 , 1 ), loc = 'upper left' , borderaxespad = 0. , framealpha = 1. ) # Plot empirical variogram if d_test is None and v_test is None : plt . title ( 'Empirical variogram' + title_str ) plt . tight_layout () plt . savefig ( os . path . join ( workdir , 'grid {} _timeslice {} _justEMPvariogram.eps' . format ( gridID , timeslice ))) # Plot just experimental variogram else : plt . title ( 'Experimental variogram' + title_str ) plt . tight_layout () plt . savefig ( os . path . join ( workdir , 'grid {} _timeslice {} _justEXPvariogram.eps' . format ( gridID , timeslice ))) plt . close () return __exponential__ ( parms , h , nugget = False ) returns a variogram model given a set of arguments and key-word arguments Source code in RAiDER/cli/statsPlot.py 463 464 465 466 467 468 469 470 471 472 473 474 475 def __exponential__ ( self , parms , h , nugget = False ): ''' returns a variogram model given a set of arguments and key-word arguments ''' # a = range, b = sill, c = nugget model a , b , c = parms with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , message = \"overflow encountered in true_divide\" ) if nugget : return b * ( 1 - np . exp ( - h / a )) + c else : return b * ( 1 - np . exp ( - h / a )) __gaussian__ ( parms , h ) returns a Gaussian variogram model Source code in RAiDER/cli/statsPlot.py 478 479 480 481 482 483 def __gaussian__ ( self , parms , h ): ''' returns a Gaussian variogram model ''' a , b , c = parms return b * ( 1 - np . exp ( - np . square ( h ) / ( a ** 2 ))) + c create_variograms () Iterate through grid-cells and time slices to generate empirical variogram(s) Source code in RAiDER/cli/statsPlot.py 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 def create_variograms ( self ): ''' Iterate through grid-cells and time slices to generate empirical variogram(s) ''' # track data for plotting self . TOT_good_slices = [] self . TOT_res_robust_arr = [] self . TOT_res_robust_rmse = [] self . TOT_tot_timetag = [] # track pass/rejected grids self . sparse_grids = [] self . good_slices = [] self . skipped_slices = [] # record grid-centers for lookup-table self . gridcenterlist = [] args = [] for i in sorted ( list ( set ( self . df [ 'gridnode' ]))): # pass subset of all stations corresponding to given grid-cell grid_subset = self . df [ self . df [ 'gridnode' ] == i ] args . append (( i , grid_subset )) # Parallelize iteration through all grid-cells and time slices with multiprocessing . Pool ( self . numCPUs ) as multipool : for i , j , k , l in multipool . starmap ( self . _append_variogram , args ): self . TOT_good_slices . extend ( i ) self . TOT_res_robust_arr . extend ( j ) self . TOT_res_robust_rmse . extend ( k ) self . gridcenterlist . extend ( l ) # save grid-center lookup table self . gridcenterlist = [ list ( i ) for i in set ( tuple ( j ) for j in self . gridcenterlist )] self . gridcenterlist . sort ( key = lambda x : int ( x [ 0 ][ 4 : 6 ])) gridcenter = open ( ( os . path . join ( self . workdir , 'variograms/gridlocation_lookup.txt' )), \"w\" ) for element in self . gridcenterlist : gridcenter . writelines ( \" \\n \" . join ( element )) gridcenter . write ( \" \\n \" ) gridcenter . close () TOT_grids = [ i [ 0 ] for i in self . TOT_good_slices ] return TOT_grids , self . TOT_res_robust_arr , self . TOT_res_robust_rmse plot_variogram ( gridID , timeslice , coords , workdir = './' , d_test = None , v_test = None , res_robust = None , dists = None , vario = None , dists_binned = None , vario_binned = None , seasonalinterval = None ) Make empirical and/or experimental variogram fit plots Source code in RAiDER/cli/statsPlot.py 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 def plot_variogram ( self , gridID , timeslice , coords , workdir = './' , d_test = None , v_test = None , res_robust = None , dists = None , vario = None , dists_binned = None , vario_binned = None , seasonalinterval = None ): ''' Make empirical and/or experimental variogram fit plots ''' # If specified workdir doesn't exist, create it if not os . path . exists ( workdir ): os . mkdir ( workdir ) # make plot title title_str = ' \\n Lat: {:.2f} Lon: {:.2f} \\n Time: {} ' . format ( coords [ 1 ], coords [ 0 ], str ( timeslice )) if seasonalinterval : title_str += ' Season(mm/dd): {} / {} \u2013 {} / {} ' . format ( int ( timeslice [ 4 : 6 ]), int ( timeslice [ 6 : 8 ]), int ( timeslice [ - 4 : - 2 ]), int ( timeslice [ - 2 :])) if dists is not None and vario is not None : # scale from m to user-defined units dists = [ convert_SI ( i , 'm' , self . unit ) for i in dists ] plt . scatter ( dists , vario , s = 1 , facecolor = '0.5' , label = 'raw' ) if dists_binned is not None and vario_binned is not None : # scale from m to user-defined units dists_binned = [ convert_SI ( i , 'm' , self . unit ) for i in dists_binned ] plt . plot ( dists_binned , vario_binned , 'bo' , label = 'binned' ) if res_robust is not None : plt . axhline ( y = res_robust [ 1 ], color = 'g' , linestyle = '--' , label = '\u0263 \\u0332\\u00b2 ( {} \\u00b2 )' . format ( self . unit )) # scale from m to user-defined units res_robust [ 0 ] = convert_SI ( res_robust [ 0 ], 'm' , self . unit ) plt . axvline ( x = res_robust [ 0 ], color = 'c' , linestyle = '--' , label = 'h ( {} )' . format ( self . unit )) if d_test is not None and v_test is not None : # scale from m to user-defined units d_test = [ convert_SI ( i , 'm' , self . unit ) for i in d_test ] plt . plot ( d_test , v_test , 'r-' , label = 'experimental fit' ) plt . xlabel ( 'Distance ( {} )' . format ( self . unit )) plt . ylabel ( 'Dissimilarity ( {} \\u00b2 )' . format ( self . unit )) plt . legend ( bbox_to_anchor = ( 1.02 , 1 ), loc = 'upper left' , borderaxespad = 0. , framealpha = 1. ) # Plot empirical variogram if d_test is None and v_test is None : plt . title ( 'Empirical variogram' + title_str ) plt . tight_layout () plt . savefig ( os . path . join ( workdir , 'grid {} _timeslice {} _justEMPvariogram.eps' . format ( gridID , timeslice ))) # Plot just experimental variogram else : plt . title ( 'Experimental variogram' + title_str ) plt . tight_layout () plt . savefig ( os . path . join ( workdir , 'grid {} _timeslice {} _justEXPvariogram.eps' . format ( gridID , timeslice ))) plt . close () return convert_SI ( val , unit_in , unit_out ) Convert input to desired units Source code in RAiDER/cli/statsPlot.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def convert_SI ( val , unit_in , unit_out ): ''' Convert input to desired units ''' SI = { 'mm' : 0.001 , 'cm' : 0.01 , 'm' : 1.0 , 'km' : 1000. , 'mm^2' : 1e-6 , 'cm^2' : 1e-4 , 'm^2' : 1.0 , 'km^2' : 1e+6 } # avoid conversion if output unit in time if unit_out in [ 'minute' , 'hour' , 'day' , 'year' ]: # adjust if input isn't datetime, and assume it to be part of workflow # e.g. sigZTD filter, already extracted datetime object try : return eval ( 'val.apply(pd.to_datetime).dt. {} .astype(float).astype(\"Int32\")' . format ( unit_out )) except BaseException : # TODO: Which error(s)? return val # check if output spatial unit is supported if unit_out not in SI : raise Exception ( \"User-specified output unit {} not recognized.\" . format ( unit_out )) return val * SI [ unit_in ] / SI [ unit_out ] create_parser () Parse command line arguments using argparse. Source code in RAiDER/cli/statsPlot.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 def create_parser (): \"\"\"Parse command line arguments using argparse.\"\"\" parser = argparse . ArgumentParser ( formatter_class = argparse . RawDescriptionHelpFormatter , description = \"\"\" Perform basic statistical analyses concerning the spatiotemporal distribution of zenith delays. Specifically, make any of the following specified plot(s): scatterplot of station locations, total empirical and experimental variogram fits for data in each grid cell (and for each valid time-slice if -variogram_per_timeslice specified), and gridded heatmaps of data, station distribution, range and sill values associated with experimental variogram fits. The default is to generate all of these. Example call to plot gridded station mean delay in a specific time interval : raiderStats.py -f <filename> -grid_delay_mean -ti '2016-01-01 2018-01-01' Example call to plot gridded station mean delay in a specific time interval with superimposed gridlines and station scatterplots : raiderStats.py -f <filename> -grid_delay_mean -ti '2016-01-01 2018-01-01' --drawgridlines --stationsongrids Example call to plot gridded station variogram in a specific time interval and through explicitly the summer seasons: raiderStats.py -f <filename> -grid_delay_mean -ti '2016-01-01 2018-01-01' --seasonalinterval '06-21 09-21' -variogramplot \"\"\" ) # User inputs userinps = parser . add_argument_group ( 'User inputs/options for which especially careful review is recommended' ) userinps . add_argument ( '-f' , '--file' , dest = 'fname' , type = str , required = True , help = 'Final output file generated from downloadGNSSDelays.py which contains GPS zenith delays for a specified time period and spatial footprint. ' ) userinps . add_argument ( '-c' , '--column_name' , dest = 'col_name' , type = str , default = 'ZTD' , help = 'Name of the input column to plot. Input assumed to be in units of meters' ) userinps . add_argument ( '-u' , '--unit' , dest = 'unit' , type = str , default = 'm' , help = 'Specified output unit (as distance or time), by default m. Input unit assumed to be m following convention in downloadGNSSDelays.py. Refer to \"convert_SI\" for supported units. Note if you specify time unit here, you must specify input for \"--obs_errlimit\" to be in units of m' ) userinps . add_argument ( '-w' , '--workdir' , dest = 'workdir' , default = './' , help = 'Specify directory to deposit all outputs. Default is local directory where script is launched.' ) add_cpus ( userinps ) userinps . add_argument ( '-verbose' , '--verbose' , action = 'store_true' , dest = 'verbose' , help = \"Run in verbose (debug) mode. Default False\" ) # Spatiotemporal subset options dtsubsets = parser . add_argument_group ( 'Controls for spatiotemporal subsetting.' ) dtsubsets . add_argument ( '-b' , '--bounding_box' , dest = 'bounding_box' , type = str , default = None , help = \"Provide either valid shapefile or Lat/Lon Bounding SNWE. -- Example : '19 20 -99.5 -98.5'\" ) dtsubsets . add_argument ( '-sp' , '--spacing' , dest = 'spacing' , type = float , default = '1' , help = 'Specify spacing of grid-cells for statistical analyses. By default 1 deg.' ) dtsubsets . add_argument ( '-ti' , '--timeinterval' , dest = 'timeinterval' , type = str , default = None , help = \"Subset in time by specifying earliest YYYY-MM-DD date followed by latest date YYYY-MM-DD. -- Example : '2016-01-01 2019-01-01'.\" ) dtsubsets . add_argument ( '-si' , '--seasonalinterval' , dest = 'seasonalinterval' , type = str , default = None , help = \"Subset in by an specific interval for each year by specifying earliest MM-DD time followed by latest MM-DD time. -- Example : '03-21 06-21'.\" ) dtsubsets . add_argument ( '-oe' , '--obs_errlimit' , dest = 'obs_errlimit' , type = float , default = 'inf' , help = \"Observation error threshold to discard observations with large uncertainties.\" ) # Plot formatting/options pltformat = parser . add_argument_group ( 'Optional controls for plot formatting/options.' ) pltformat . add_argument ( '-figdpi' , '--figdpi' , dest = 'figdpi' , type = int , default = 100 , help = 'DPI to use for saving figures' ) pltformat . add_argument ( '-title' , '--user_title' , dest = 'user_title' , type = str , default = None , help = 'Specify custom title for plots.' ) pltformat . add_argument ( '-fmt' , '--plot_format' , dest = 'plot_fmt' , type = str , default = 'png' , help = 'Plot format to use for saving figures' ) pltformat . add_argument ( '-cb' , '--color_bounds' , dest = 'cbounds' , type = str , default = None , help = 'List of two floats to use as color axis bounds' ) pltformat . add_argument ( '-cp' , '--colorpercentile' , dest = 'colorpercentile' , type = float , default = None , nargs = 2 , help = 'Set low and upper percentile for plot colorbars. By default 25 %% and 95 %% , respectively.' ) pltformat . add_argument ( '-cm' , '--colormap' , dest = 'usr_colormap' , type = str , default = 'hot_r' , help = 'Specify matplotlib colorbar.' ) pltformat . add_argument ( '-dt' , '--densitythreshold' , dest = 'densitythreshold' , type = int , default = '10' , help = 'For variogram plots, given grid-cell is only valid if it contains this specified threshold of stations. By default 10 stations.' ) pltformat . add_argument ( '-sg' , '--stationsongrids' , dest = 'stationsongrids' , action = 'store_true' , help = 'In gridded plots, superimpose your gridded array with a scatterplot of station locations.' ) pltformat . add_argument ( '-dg' , '--drawgridlines' , dest = 'drawgridlines' , action = 'store_true' , help = 'Draw gridlines on gridded plots.' ) pltformat . add_argument ( '-tl' , '--time_lines' , dest = 'time_lines' , action = 'store_true' , help = 'Draw central longitudinal lines with respect to datetime. Most useful for local-time analyses.' ) pltformat . add_argument ( '-plotall' , '--plotall' , action = 'store_true' , dest = 'plotall' , help = \"Generate all supported plots, including variogram plots.\" ) pltformat . add_argument ( '-min_span' , '--min_span' , dest = 'min_span' , type = float , default = [ 2 , 0.6 ], nargs = 2 , help = \"Minimum TS span (years) and minimum fractional observations in span (fraction) imposed for seasonal amplitude/phase analyses to be performed for a given station.\" ) pltformat . add_argument ( '-period_limit' , '--period_limit' , dest = 'period_limit' , type = float , default = 0. , help = \"period limit (years) imposed for seasonal amplitude/phase analyses to be performed for a given station.\" ) # All plot types # Station scatter-plots pltscatter = parser . add_argument_group ( 'Supported types of individual station scatter-plots.' ) pltscatter . add_argument ( '-station_distribution' , '--station_distribution' , action = 'store_true' , dest = 'station_distribution' , help = \"Plot station distribution.\" ) pltscatter . add_argument ( '-station_delay_mean' , '--station_delay_mean' , action = 'store_true' , dest = 'station_delay_mean' , help = \"Plot station mean delay.\" ) pltscatter . add_argument ( '-station_delay_median' , '--station_delay_median' , action = 'store_true' , dest = 'station_delay_median' , help = \"Plot station median delay.\" ) pltscatter . add_argument ( '-station_delay_stdev' , '--station_delay_stdev' , action = 'store_true' , dest = 'station_delay_stdev' , help = \"Plot station delay stdev.\" ) pltscatter . add_argument ( '-station_seasonal_phase' , '--station_seasonal_phase' , action = 'store_true' , dest = 'station_seasonal_phase' , help = \"Plot station delay phase/amplitude.\" ) pltscatter . add_argument ( '-phaseamp_per_station' , '--phaseamp_per_station' , action = 'store_true' , dest = 'phaseamp_per_station' , help = \"Save debug figures of curve-fit vs data per station.\" ) # Gridded plots pltgrids = parser . add_argument_group ( 'Supported types of gridded plots.' ) pltgrids . add_argument ( '-grid_heatmap' , '--grid_heatmap' , action = 'store_true' , dest = 'grid_heatmap' , help = \"Plot gridded station heatmap.\" ) pltgrids . add_argument ( '-grid_delay_mean' , '--grid_delay_mean' , action = 'store_true' , dest = 'grid_delay_mean' , help = \"Plot gridded station-wise mean delay.\" ) pltgrids . add_argument ( '-grid_delay_median' , '--grid_delay_median' , action = 'store_true' , dest = 'grid_delay_median' , help = \"Plot gridded station-wise median delay.\" ) pltgrids . add_argument ( '-grid_delay_stdev' , '--grid_delay_stdev' , action = 'store_true' , dest = 'grid_delay_stdev' , help = \"Plot gridded station-wise delay stdev.\" ) pltgrids . add_argument ( '-grid_seasonal_phase' , '--grid_seasonal_phase' , action = 'store_true' , dest = 'grid_seasonal_phase' , help = \"Plot gridded station-wise delay phase/amplitude.\" ) pltgrids . add_argument ( '-grid_delay_absolute_mean' , '--grid_delay_absolute_mean' , action = 'store_true' , dest = 'grid_delay_absolute_mean' , help = \"Plot absolute gridded station mean delay.\" ) pltgrids . add_argument ( '-grid_delay_absolute_median' , '--grid_delay_absolute_median' , action = 'store_true' , dest = 'grid_delay_absolute_median' , help = \"Plot absolute gridded station median delay.\" ) pltgrids . add_argument ( '-grid_delay_absolute_stdev' , '--grid_delay_absolute_stdev' , action = 'store_true' , dest = 'grid_delay_absolute_stdev' , help = \"Plot absolute gridded station delay stdev.\" ) pltgrids . add_argument ( '-grid_seasonal_absolute_phase' , '--grid_seasonal_absolute_phase' , action = 'store_true' , dest = 'grid_seasonal_absolute_phase' , help = \"Plot absolute gridded station delay phase/amplitude.\" ) pltgrids . add_argument ( '-grid_to_raster' , '--grid_to_raster' , action = 'store_true' , dest = 'grid_to_raster' , help = \"Save gridded array as raster. May directly load/plot in successive script call.\" ) # Variogram plots pltvario = parser . add_argument_group ( 'Supported types of variogram plots.' ) pltvario . add_argument ( '-variogramplot' , '--variogramplot' , action = 'store_true' , dest = 'variogramplot' , help = \"Plot gridded station variogram.\" ) pltvario . add_argument ( '-binnedvariogram' , '--binnedvariogram' , action = 'store_true' , dest = 'binnedvariogram' , help = \"Apply experimental variogram fit to total binned empirical variograms for each time slice. Default is to pass total unbinned empiricial variogram.\" ) pltvario . add_argument ( '-variogram_per_timeslice' , '--variogram_per_timeslice' , action = 'store_true' , dest = 'variogram_per_timeslice' , help = \"Generate variogram plots per gridded station AND time-slice.\" ) pltvario . add_argument ( '-variogram_errlimit' , '--variogram_errlimit' , dest = 'variogram_errlimit' , type = float , default = 'inf' , help = \"Variogram RMSE threshold to discard grid-cells with large uncertainties.\" ) return parser load_gridfile ( fname , unit ) Function to load gridded-arrays saved from previous runs. Source code in RAiDER/cli/statsPlot.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 def load_gridfile ( fname , unit ): ''' Function to load gridded-arrays saved from previous runs. ''' with rasterio . open ( fname ) as src : grid_array = src . read () . astype ( float ) # Read metadata variables needed for plotting metadata_dict = src . tags () # Initiate no-data array to mask data nodat_arr = [ 0 , np . nan , np . inf ] if unit in [ 'minute' , 'hour' , 'day' , 'year' ]: nodat_arr = [ np . nan , np . inf ] # set masked values as nans for i in nodat_arr : grid_array = np . ma . masked_where ( grid_array == i , grid_array ) grid_array = np . ma . filled ( grid_array , np . nan ) # Make plotting command a global variable gridfile_type = metadata_dict [ 'gridfile_type' ] globals ()[ gridfile_type ] = True plotbbox = [ float ( i ) for i in metadata_dict [ 'plotbbox' ] . split ()] spacing = float ( metadata_dict [ 'spacing' ]) colorbarfmt = metadata_dict [ 'colorbarfmt' ] inputunit = metadata_dict [ 'unit' ] # adjust conversion if native units are squared if '^2' in inputunit : unit = unit . split ( '^2' )[ 0 ] + '^2' # convert to specified output unit grid_array = convert_SI ( grid_array , inputunit , unit ) # Backwards compatible for cases where this key doesn't exist try : time_lines = metadata_dict [ 'time_lines' ] except KeyError : time_lines = False if metadata_dict [ 'stationsongrids' ] == 'False' : stationsongrids = False else : stationsongrids = [ float ( i ) for i in metadata_dict [ 'stationsongrids' . split ()]] if metadata_dict [ 'time_lines' ] == 'False' : time_lines = False else : time_lines = [ float ( i ) for i in metadata_dict [ 'time_lines' ] . split ()] return grid_array , plotbbox , spacing , colorbarfmt , stationsongrids , time_lines midpoint ( p1 , p2 ) Calculate central longitude for '--time_lines' option Source code in RAiDER/cli/statsPlot.py 195 196 197 198 199 200 201 202 203 204 205 206 207 def midpoint ( p1 , p2 ): ''' Calculate central longitude for '--time_lines' option ''' import math lat1 , lon1 , lat2 , lon2 = map ( math . radians , ( p1 [ 0 ], p1 [ 1 ], p2 [ 0 ], p2 [ 1 ])) dlon = lon2 - lon1 dx = math . cos ( lat2 ) * math . cos ( dlon ) dy = math . cos ( lat2 ) * math . sin ( dlon ) lon3 = lon1 + math . atan2 ( dy , math . cos ( lat1 ) + dx ) return ( int ( math . degrees ( lon3 ))) save_gridfile ( df , gridfile_type , fname , plotbbox , spacing , unit , colorbarfmt = ' %.2f ' , stationsongrids = False , time_lines = False , dtype = 'float32' , noData = np . nan ) Function to save gridded-arrays as GDAL-readable file. Source code in RAiDER/cli/statsPlot.py 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def save_gridfile ( df , gridfile_type , fname , plotbbox , spacing , unit , colorbarfmt = ' %.2f ' , stationsongrids = False , time_lines = False , dtype = \"float32\" , noData = np . nan ): ''' Function to save gridded-arrays as GDAL-readable file. ''' # Pass metadata metadata_dict = {} metadata_dict [ 'gridfile_type' ] = gridfile_type metadata_dict [ 'plotbbox' ] = ' ' . join ([ str ( i ) for i in plotbbox ]) metadata_dict [ 'spacing' ] = str ( spacing ) metadata_dict [ 'unit' ] = unit if unit in [ 'minute' , 'hour' , 'day' , 'year' ]: colorbarfmt = ' %1i ' metadata_dict [ 'colorbarfmt' ] = colorbarfmt if stationsongrids : metadata_dict [ 'stationsongrids' ] = ' ' . join ([ str ( i ) for i in stationsongrids ]) else : metadata_dict [ 'stationsongrids' ] = 'False' if time_lines : metadata_dict [ 'time_lines' ] = ' ' . join ([ str ( i ) for i in time_lines ]) else : metadata_dict [ 'time_lines' ] = 'False' # Write data to file with rasterio . open ( fname , mode = \"w\" , count = 1 , width = df . shape [ 1 ], height = df . shape [ 0 ], dtype = dtype , nodata = noData ) as dst : dst . write ( df , 1 ) dst . update_tags ( 1 , ** metadata_dict ) # Finalize VRT vrtname = fname + \".vrt\" rasterio . shutil . copy ( fname , fname + \".vrt\" , driver = \"VRT\" ) return stats_analyses ( fname , col_name , unit , workdir , numCPUs , verbose , bbox , spacing , timeinterval , seasonalinterval , obs_errlimit , figdpi , user_title , plot_fmt , cbounds , colorpercentile , usr_colormap , densitythreshold , stationsongrids , drawgridlines , time_lines , plotall , station_distribution , station_delay_mean , station_delay_median , station_delay_stdev , station_seasonal_phase , phaseamp_per_station , grid_heatmap , grid_delay_mean , grid_delay_median , grid_delay_stdev , grid_seasonal_phase , grid_delay_absolute_mean , grid_delay_absolute_median , grid_delay_absolute_stdev , grid_seasonal_absolute_phase , grid_to_raster , min_span , period_limit , variogramplot , binnedvariogram , variogram_per_timeslice , variogram_errlimit ) Main workflow for generating a suite of plots to illustrate spatiotemporal distribution and/or character of zenith delays Source code in RAiDER/cli/statsPlot.py 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 def stats_analyses ( fname , col_name , unit , workdir , numCPUs , verbose , bbox , spacing , timeinterval , seasonalinterval , obs_errlimit , figdpi , user_title , plot_fmt , cbounds , colorpercentile , usr_colormap , densitythreshold , stationsongrids , drawgridlines , time_lines , plotall , station_distribution , station_delay_mean , station_delay_median , station_delay_stdev , station_seasonal_phase , phaseamp_per_station , grid_heatmap , grid_delay_mean , grid_delay_median , grid_delay_stdev , grid_seasonal_phase , grid_delay_absolute_mean , grid_delay_absolute_median , grid_delay_absolute_stdev , grid_seasonal_absolute_phase , grid_to_raster , min_span , period_limit , variogramplot , binnedvariogram , variogram_per_timeslice , variogram_errlimit ): ''' Main workflow for generating a suite of plots to illustrate spatiotemporal distribution and/or character of zenith delays ''' if verbose : logger . setLevel ( logging . DEBUG ) # Control DPI for output figures mpl . rcParams [ 'savefig.dpi' ] = figdpi # If user requests to generate all plots. if plotall : logger . info ( '\"-plotall\" == True. All plots will be made.' ) station_distribution = True station_delay_mean = True station_delay_median = True station_delay_stdev = True station_seasonal_phase = True grid_heatmap = True grid_delay_mean = True grid_delay_median = True grid_delay_stdev = True grid_seasonal_phase = True grid_delay_absolute_mean = True grid_delay_absolute_median = True grid_delay_absolute_stdev = True grid_seasonal_absolute_phase = True variogramplot = True logger . info ( \"***Stats Function:***\" ) # prep dataframe object for plotting/variogram analysis based off of user specifications df_stats = RaiderStats ( fname , col_name , unit , workdir , bbox , spacing , timeinterval , seasonalinterval , obs_errlimit , time_lines , stationsongrids , station_seasonal_phase , cbounds , colorpercentile , usr_colormap , grid_heatmap , grid_delay_mean , grid_delay_median , grid_delay_stdev , grid_seasonal_phase , grid_delay_absolute_mean , grid_delay_absolute_median , grid_delay_absolute_stdev , grid_seasonal_absolute_phase , grid_to_raster , min_span , period_limit , numCPUs , phaseamp_per_station ) # Station plots # Plot each individual station if station_distribution : logger . info ( \"- Plot spatial distribution of stations.\" ) unique_points = df_stats . df . groupby ([ 'Lon' , 'Lat' ]) . size () df_stats ([ unique_points . index . get_level_values ( 'Lon' ) . tolist (), unique_points . index . get_level_values ( 'Lat' ) . tolist ( )], 'station_distribution' , workdir = os . path . join ( workdir , 'figures' ), plotFormat = plot_fmt , userTitle = user_title ) # Plot mean delay per station if station_delay_mean : logger . info ( \"- Plot mean delay for each station.\" ) unique_points = df_stats . df . groupby ( [ 'Lon' , 'Lat' ])[ col_name ] . median () unique_points . dropna ( how = 'any' , inplace = True ) df_stats ([ unique_points . index . get_level_values ( 'Lon' ) . tolist (), unique_points . index . get_level_values ( 'Lat' ) . tolist ( ), unique_points . values ], 'station_delay_mean' , workdir = os . path . join ( workdir , 'figures' ), plotFormat = plot_fmt , userTitle = user_title ) # Plot median delay per station if station_delay_median : logger . info ( \"- Plot median delay for each station.\" ) unique_points = df_stats . df . groupby ( [ 'Lon' , 'Lat' ])[ col_name ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) df_stats ([ unique_points . index . get_level_values ( 'Lon' ) . tolist (), unique_points . index . get_level_values ( 'Lat' ) . tolist ( ), unique_points . values ], 'station_delay_median' , workdir = os . path . join ( workdir , 'figures' ), plotFormat = plot_fmt , userTitle = user_title ) # Plot delay stdev per station if station_delay_stdev : logger . info ( \"- Plot delay stdev for each station.\" ) unique_points = df_stats . df . groupby ( [ 'Lon' , 'Lat' ])[ col_name ] . std () unique_points . dropna ( how = 'any' , inplace = True ) df_stats ([ unique_points . index . get_level_values ( 'Lon' ) . tolist (), unique_points . index . get_level_values ( 'Lat' ) . tolist ( ), unique_points . values ], 'station_delay_stdev' , workdir = os . path . join ( workdir , 'figures' ), plotFormat = plot_fmt , userTitle = user_title ) # Plot delay phase/amplitude per station if station_seasonal_phase : logger . info ( \"- Plot delay phase/amplitude for each station.\" ) # phase unique_points_phase = df_stats . df . groupby ( [ 'Lon' , 'Lat' ])[ 'phsfit' ] . mean () unique_points_phase . dropna ( how = 'any' , inplace = True ) df_stats ([ unique_points_phase . index . get_level_values ( 'Lon' ) . tolist (), unique_points_phase . index . get_level_values ( 'Lat' ) . tolist ( ), unique_points_phase . values ], 'station_seasonal_phase' , workdir = os . path . join ( workdir , 'figures' ), colorbarfmt = ' %.1i ' , plotFormat = plot_fmt , userTitle = user_title ) # amplitude unique_points_amplitude = df_stats . df . groupby ( [ 'Lon' , 'Lat' ])[ 'ampfit' ] . mean () unique_points_amplitude . dropna ( how = 'any' , inplace = True ) df_stats ([ unique_points_amplitude . index . get_level_values ( 'Lon' ) . tolist (), unique_points_amplitude . index . get_level_values ( 'Lat' ) . tolist ( ), unique_points_amplitude . values ], 'station_seasonal_amplitude' , workdir = os . path . join ( workdir , 'figures' ), colorbarfmt = ' %.3f ' , plotFormat = plot_fmt , userTitle = user_title ) # period unique_points_period = df_stats . df . groupby ( [ 'Lon' , 'Lat' ])[ 'periodfit' ] . mean () df_stats ([ unique_points_period . index . get_level_values ( 'Lon' ) . tolist (), unique_points_period . index . get_level_values ( 'Lat' ) . tolist ( ), unique_points_period . values ], 'station_delay_period' , workdir = os . path . join ( workdir , 'figures' ), colorbarfmt = ' %.2f ' , plotFormat = plot_fmt , userTitle = user_title ) # Gridded station plots # Plot density of stations for each gridcell if isinstance ( df_stats . grid_heatmap , np . ndarray ): logger . info ( \"- Plot density of stations per gridcell.\" ) df_stats ( df_stats . grid_heatmap , 'grid_heatmap' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.1i ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot mean of station-wise mean delay across each gridcell if isinstance ( df_stats . grid_delay_mean , np . ndarray ): logger . info ( \"- Plot mean of station-wise mean delay across each gridcell.\" ) df_stats ( df_stats . grid_delay_mean , 'grid_delay_mean' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.2f ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot mean of station-wise median delay across each gridcell if isinstance ( df_stats . grid_delay_median , np . ndarray ): logger . info ( \"- Plot mean of station-wise median delay across each gridcell.\" ) df_stats ( df_stats . grid_delay_median , 'grid_delay_median' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.2f ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot mean of station-wise stdev delay across each gridcell if isinstance ( df_stats . grid_delay_stdev , np . ndarray ): logger . info ( \"- Plot mean of station-wise stdev delay across each gridcell.\" ) df_stats ( df_stats . grid_delay_stdev , 'grid_delay_stdev' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.2f ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot mean of station-wise delay phase across each gridcell if isinstance ( df_stats . grid_seasonal_phase , np . ndarray ): logger . info ( \"- Plot mean of station-wise delay phase across each gridcell.\" ) df_stats ( df_stats . grid_seasonal_phase , 'grid_seasonal_phase' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.1i ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot mean of station-wise delay amplitude across each gridcell if isinstance ( df_stats . grid_seasonal_amplitude , np . ndarray ): logger . info ( \"- Plot mean of station-wise delay amplitude across each gridcell.\" ) df_stats ( df_stats . grid_seasonal_amplitude , 'grid_seasonal_amplitude' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.3f ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot mean of station-wise delay period across each gridcell if isinstance ( df_stats . grid_seasonal_period , np . ndarray ): logger . info ( \"- Plot mean of station-wise delay period across each gridcell.\" ) df_stats ( df_stats . grid_seasonal_period , 'grid_seasonal_period' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.2f ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot mean stdev of station-wise delay phase across each gridcell if isinstance ( df_stats . grid_seasonal_phase_stdev , np . ndarray ): logger . info ( \"- Plot mean stdev of station-wise delay phase across each gridcell.\" ) df_stats ( df_stats . grid_seasonal_phase_stdev , 'grid_seasonal_phase_stdev' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.1i ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot mean stdev of station-wise delay amplitude across each gridcell if isinstance ( df_stats . grid_seasonal_amplitude_stdev , np . ndarray ): logger . info ( \"- Plot mean stdev of station-wise delay amplitude across each gridcell.\" ) df_stats ( df_stats . grid_seasonal_amplitude_stdev , 'grid_seasonal_amplitude_stdev' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.3f ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot mean stdev of station-wise delay period across each gridcell if isinstance ( df_stats . grid_seasonal_period_stdev , np . ndarray ): logger . info ( \"- Plot mean stdev of station-wise delay period across each gridcell.\" ) df_stats ( df_stats . grid_seasonal_period_stdev , 'grid_seasonal_period_stdev' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.2e ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot mean of seasonal fit RMSE across each gridcell if isinstance ( df_stats . grid_seasonal_fit_rmse , np . ndarray ): logger . info ( \"- Plot mean of seasonal fit RMSE across each gridcell.\" ) df_stats ( df_stats . grid_seasonal_fit_rmse , 'grid_seasonal_fit_rmse' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.3f ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot absolute mean delay for each gridcell if isinstance ( df_stats . grid_delay_absolute_mean , np . ndarray ): logger . info ( \"- Plot absolute mean delay per gridcell.\" ) df_stats ( df_stats . grid_delay_absolute_mean , 'grid_delay_absolute_mean' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.2f ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot absolute median delay for each gridcell if isinstance ( df_stats . grid_delay_absolute_median , np . ndarray ): logger . info ( \"- Plot absolute median delay per gridcell.\" ) df_stats ( df_stats . grid_delay_absolute_median , 'grid_delay_absolute_median' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.2f ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot absolute stdev delay for each gridcell if isinstance ( df_stats . grid_delay_absolute_stdev , np . ndarray ): logger . info ( \"- Plot absolute delay stdev per gridcell.\" ) df_stats ( df_stats . grid_delay_absolute_stdev , 'grid_delay_absolute_stdev' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.2f ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot absolute delay phase for each gridcell if isinstance ( df_stats . grid_seasonal_absolute_phase , np . ndarray ): logger . info ( \"- Plot absolute delay phase per gridcell.\" ) df_stats ( df_stats . grid_seasonal_absolute_phase , 'grid_seasonal_absolute_phase' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.1i ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot absolute delay amplitude for each gridcell if isinstance ( df_stats . grid_seasonal_absolute_amplitude , np . ndarray ): logger . info ( \"- Plot absolute delay amplitude per gridcell.\" ) df_stats ( df_stats . grid_seasonal_absolute_amplitude , 'grid_seasonal_absolute_amplitude' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.3f ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot absolute delay period for each gridcell if isinstance ( df_stats . grid_seasonal_absolute_period , np . ndarray ): logger . info ( \"- Plot absolute delay period per gridcell.\" ) df_stats ( df_stats . grid_seasonal_absolute_period , 'grid_seasonal_absolute_period' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.2f ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot absolute delay phase stdev for each gridcell if isinstance ( df_stats . grid_seasonal_absolute_phase_stdev , np . ndarray ): logger . info ( \"- Plot absolute delay phase stdev per gridcell.\" ) df_stats ( df_stats . grid_seasonal_absolute_phase_stdev , 'grid_seasonal_absolute_phase_stdev' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.1i ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot absolute delay amplitude stdev for each gridcell if isinstance ( df_stats . grid_seasonal_absolute_amplitude_stdev , np . ndarray ): logger . info ( \"- Plot absolute delay amplitude stdev per gridcell.\" ) df_stats ( df_stats . grid_seasonal_absolute_amplitude_stdev , 'grid_seasonal_absolute_amplitude_stdev' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.3f ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot absolute delay period stdev for each gridcell if isinstance ( df_stats . grid_seasonal_absolute_period_stdev , np . ndarray ): logger . info ( \"- Plot absolute delay period stdev per gridcell.\" ) df_stats ( df_stats . grid_seasonal_absolute_period_stdev , 'grid_seasonal_absolute_period_stdev' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.2e ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot absolute mean seasonal fit RMSE for each gridcell if isinstance ( df_stats . grid_seasonal_absolute_fit_rmse , np . ndarray ): logger . info ( \"- Plot absolute mean seasonal fit RMSE per gridcell.\" ) df_stats ( df_stats . grid_seasonal_absolute_fit_rmse , 'grid_seasonal_absolute_fit_rmse' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.2e ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Perform variogram analysis if variogramplot and not isinstance ( df_stats . grid_range , np . ndarray ) \\ and not isinstance ( df_stats . grid_variance , np . ndarray ) \\ and not isinstance ( df_stats . grid_variogram_rmse , np . ndarray ): logger . info ( \"***Variogram Analysis Function:***\" ) if unit in [ 'minute' , 'hour' , 'day' , 'year' ]: unit = 'm' df_stats . unit = 'm' logger . warning ( \"Output unit {} specified for Variogram analysis. Reverted to meters\" . format ( unit )) make_variograms = VariogramAnalysis ( df_stats . df , df_stats . gridpoints , col_name , unit , workdir , df_stats . seasonalinterval , densitythreshold , binnedvariogram , numCPUs , variogram_per_timeslice , variogram_errlimit ) TOT_grids , TOT_res_robust_arr , TOT_res_robust_rmse = make_variograms . create_variograms () # get range df_stats . grid_range = np . array ([ np . nan if i [ 0 ] not in TOT_grids else float ( TOT_res_robust_arr [ TOT_grids . index ( i [ 0 ])][ 0 ]) for i in enumerate ( df_stats . gridpoints )]) . reshape ( df_stats . grid_dim ) . T # convert range to specified output unit df_stats . grid_range = convert_SI ( df_stats . grid_range , 'm' , unit ) # get sill df_stats . grid_variance = np . array ([ np . nan if i [ 0 ] not in TOT_grids else float ( TOT_res_robust_arr [ TOT_grids . index ( i [ 0 ])][ 1 ]) for i in enumerate ( df_stats . gridpoints )]) . reshape ( df_stats . grid_dim ) . T # convert sill to specified output unit df_stats . grid_range = convert_SI ( df_stats . grid_range , 'm^2' , unit . split ( '^2' )[ 0 ] + '^2' ) # get variogram rmse df_stats . grid_variogram_rmse = np . array ([ np . nan if i [ 0 ] not in TOT_grids else float ( TOT_res_robust_rmse [ TOT_grids . index ( i [ 0 ])]) for i in enumerate ( df_stats . gridpoints )]) . reshape ( df_stats . grid_dim ) . T # convert range to specified output unit df_stats . grid_variogram_rmse = convert_SI ( df_stats . grid_variogram_rmse , 'm' , unit ) # If specified, save gridded array(s) if grid_to_raster : # write range gridfile_name = os . path . join ( workdir , col_name + '_' + 'grid_range' + '.tif' ) save_gridfile ( df_stats . grid_range , 'grid_range' , gridfile_name , df_stats . plotbbox , df_stats . spacing , df_stats . unit , colorbarfmt = ' %1i ' , stationsongrids = df_stats . stationsongrids , dtype = 'float32' ) # write sill gridfile_name = os . path . join ( workdir , col_name + '_' + 'grid_variance' + '.tif' ) save_gridfile ( df_stats . grid_variance , 'grid_variance' , gridfile_name , df_stats . plotbbox , df_stats . spacing , df_stats . unit + '^2' , colorbarfmt = ' %.3e ' , stationsongrids = df_stats . stationsongrids , dtype = 'float32' ) # write variogram rmse gridfile_name = os . path . join ( workdir , col_name + '_' + 'grid_variogram_rmse' + '.tif' ) save_gridfile ( df_stats . grid_variogram_rmse , 'grid_variogram_rmse' , gridfile_name , df_stats . plotbbox , df_stats . spacing , df_stats . unit , colorbarfmt = ' %.2e ' , stationsongrids = df_stats . stationsongrids , dtype = 'float32' ) if isinstance ( df_stats . grid_range , np . ndarray ): # plot range heatmap logger . info ( \"- Plot variogram range per gridcell.\" ) df_stats ( df_stats . grid_range , 'grid_range' , workdir = os . path . join ( workdir , 'figures' ), colorbarfmt = ' %1i ' , drawgridlines = drawgridlines , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) if isinstance ( df_stats . grid_variance , np . ndarray ): # plot sill heatmap logger . info ( \"- Plot variogram sill per gridcell.\" ) df_stats ( df_stats . grid_variance , 'grid_variance' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.3e ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) if isinstance ( df_stats . grid_variogram_rmse , np . ndarray ): # plot variogram rmse heatmap logger . info ( \"- Plot variogram RMSE per gridcell.\" ) df_stats ( df_stats . grid_variogram_rmse , 'grid_variogram_rmse' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.2e ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) validators BBoxAction Bases: Action An Action that parses and stores a valid bounding box Source code in RAiDER/cli/validators.py 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 class BBoxAction ( Action ): \"\"\"An Action that parses and stores a valid bounding box\"\"\" def __init__ ( self , option_strings , dest , nargs = None , const = None , default = None , type = None , choices = None , required = False , help = None , metavar = None ): if nargs != 4 : raise ValueError ( \"nargs must be 4!\" ) super () . __init__ ( option_strings = option_strings , dest = dest , nargs = nargs , const = const , default = default , type = type , choices = choices , required = required , help = help , metavar = metavar ) def __call__ ( self , parser , namespace , values , option_string = None ): S , N , W , E = values if N <= S or E <= W : raise ArgumentError ( self , 'Bounding box has no size; make sure you use \"S N W E\"' ) for sn in ( S , N ): if sn < - 90 or sn > 90 : raise ArgumentError ( self , 'Lats are out of S/N bounds (-90 to 90).' ) for we in ( W , E ): if we < - 180 or we > 180 : raise ArgumentError ( self , 'Lons are out of W/E bounds (-180 to 180); Lons in the format of (0 to 360) are not supported.' ) setattr ( namespace , self . dest , values ) DateListAction Bases: Action An Action that parses and stores a list of dates Source code in RAiDER/cli/validators.py 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 class DateListAction ( Action ): \"\"\"An Action that parses and stores a list of dates\"\"\" def __init__ ( self , option_strings , dest , nargs = None , const = None , default = None , type = None , choices = None , required = False , help = None , metavar = None ): if type is not date_type : raise ValueError ( \"type must be `date_type`!\" ) super () . __init__ ( option_strings = option_strings , dest = dest , nargs = nargs , const = const , default = default , type = type , choices = choices , required = required , help = help , metavar = metavar ) def __call__ ( self , parser , namespace , values , option_string = None ): if len ( values ) > 3 or not values : raise ArgumentError ( self , \"Only 1, 2 dates, or 2 dates and interval may be supplied\" ) if len ( values ) == 2 : start , end = values values = [ start + timedelta ( days = k ) for k in range ( 0 , ( end - start ) . days + 1 , 1 )] elif len ( values ) == 3 : start , end , stepsize = values if not isinstance ( stepsize . day , int ): raise ArgumentError ( self , \"The stepsize should be in integer days\" ) new_year = date ( year = stepsize . year , month = 1 , day = 1 ) stepsize = ( stepsize - new_year ) . days + 1 values = [ start + timedelta ( days = k ) for k in range ( 0 , ( end - start ) . days + 1 , stepsize )] setattr ( namespace , self . dest , values ) IntegerMappingType Bases: MappingType , IntegerType An integer type that converts non-integer types through a mapping. Example integer = IntegerMappingType(0, 100, random=42) assert integer(\"0\") == 0 assert integer(\"100\") == 100 assert integer(\"random\") == 42 Source code in RAiDER/cli/validators.py 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 class IntegerMappingType ( MappingType , IntegerType ): \"\"\" An integer type that converts non-integer types through a mapping. # Example ``` integer = IntegerMappingType(0, 100, random=42) assert integer(\"0\") == 0 assert integer(\"100\") == 100 assert integer(\"random\") == 42 ``` \"\"\" def __init__ ( self , lo = None , hi = None , mapping = {}, ** kwargs ): IntegerType . __init__ ( self , lo , hi ) kwargs . update ( mapping ) MappingType . __init__ ( self , ** kwargs ) def __call__ ( self , arg ): try : return IntegerType . __call__ ( self , arg ) except ValueError : return MappingType . __call__ ( self , arg ) IntegerType Bases: object A type that converts arguments to integers. Example integer = IntegerType(0, 100) assert integer(\"0\") == 0 assert integer(\"100\") == 100 integer(\"-10\") # Raises exception Source code in RAiDER/cli/validators.py 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 class IntegerType ( object ): \"\"\" A type that converts arguments to integers. # Example ``` integer = IntegerType(0, 100) assert integer(\"0\") == 0 assert integer(\"100\") == 100 integer(\"-10\") # Raises exception ``` \"\"\" def __init__ ( self , lo = None , hi = None ): self . lo = lo self . hi = hi def __call__ ( self , arg ): integer = int ( arg ) if self . lo is not None and integer < self . lo : raise ArgumentTypeError ( \"Must be greater than {} \" . format ( self . lo )) if self . hi is not None and integer > self . hi : raise ArgumentTypeError ( \"Must be less than {} \" . format ( self . hi )) return integer MappingType Bases: object A type that maps arguments to constants. Example mapping = MappingType(foo=42, bar=\"baz\").default(None) assert mapping(\"foo\") == 42 assert mapping(\"bar\") == \"baz\" assert mapping(\"hello\") is None Source code in RAiDER/cli/validators.py 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 class MappingType ( object ): \"\"\" A type that maps arguments to constants. # Example ``` mapping = MappingType(foo=42, bar=\"baz\").default(None) assert mapping(\"foo\") == 42 assert mapping(\"bar\") == \"baz\" assert mapping(\"hello\") is None ``` \"\"\" UNSET = object () def __init__ ( self , ** kwargs ): self . mapping = kwargs self . _default = self . UNSET def default ( self , default ): \"\"\"Set a default value if no mapping is found\"\"\" self . _default = default return self def __call__ ( self , arg ): if arg in self . mapping : return self . mapping [ arg ] if self . _default is self . UNSET : raise KeyError ( \"Invalid choice ' {} ', must be one of {} \" . format ( arg , list ( self . mapping . keys ()) ) ) return self . _default default ( default ) Set a default value if no mapping is found Source code in RAiDER/cli/validators.py 408 409 410 411 def default ( self , default ): \"\"\"Set a default value if no mapping is found\"\"\" self . _default = default return self date_type ( arg ) Parse a date from a string in pseudo-ISO 8601 format. Source code in RAiDER/cli/validators.py 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 def date_type ( arg ): \"\"\" Parse a date from a string in pseudo-ISO 8601 format. \"\"\" year_formats = ( '%Y-%m- %d ' , '%Y%m %d ' , ' %d ' , '%j' , ) for yf in year_formats : try : return date ( * strptime ( arg , yf )[ 0 : 3 ]) except ValueError : pass raise ArgumentTypeError ( 'Unable to coerce {} to a date. Try %Y-%m- %d ' . format ( arg ) ) enforce_bbox ( bbox ) Enforce a valid bounding box Source code in RAiDER/cli/validators.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 def enforce_bbox ( bbox ): \"\"\" Enforce a valid bounding box \"\"\" if isinstance ( bbox , str ): bbox = [ float ( d ) for d in bbox . strip () . split ()] else : bbox = [ float ( d ) for d in bbox ] # Check the bbox if len ( bbox ) != 4 : raise ValueError ( \"bounding box must have 4 elements!\" ) S , N , W , E = bbox if N <= S or E <= W : raise ValueError ( 'Bounding box has no size; make sure you use \"S N W E\"' ) for sn in ( S , N ): if sn < - 90 or sn > 90 : raise ValueError ( 'Lats are out of S/N bounds (-90 to 90).' ) for we in ( W , E ): if we < - 180 or we > 180 : raise ValueError ( 'Lons are out of W/E bounds (-180 to 180); Lons in the format of (0 to 360) are not supported.' ) return bbox enforce_time ( arg_dict ) Parse an input time (required to be ISO 8601) Source code in RAiDER/cli/validators.py 243 244 245 246 247 248 249 250 251 252 253 254 def enforce_time ( arg_dict ): ''' Parse an input time (required to be ISO 8601) ''' try : arg_dict [ 'time' ] = convert_time ( arg_dict [ 'time' ]) except KeyError : raise ValueError ( 'You must specify a \"time\" in the input config file' ) if 'end_time' in arg_dict . keys (): arg_dict [ 'end_time' ] = convert_time ( arg_dict [ 'end_time' ]) return arg_dict enforce_valid_dates ( arg ) Parse a date from a string in pseudo-ISO 8601 format. Source code in RAiDER/cli/validators.py 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 def enforce_valid_dates ( arg ): \"\"\" Parse a date from a string in pseudo-ISO 8601 format. \"\"\" year_formats = ( '%Y-%m- %d ' , '%Y%m %d ' , ' %d ' , '%j' , ) for yf in year_formats : try : return datetime . strptime ( str ( arg ), yf ) except ValueError : pass raise ValueError ( 'Unable to coerce {} to a date. Try %Y-%m- %d ' . format ( arg ) ) getBufferedExtent ( lats , lons = None , buf = 0.0 ) get the bounding box around a set of lats/lons Source code in RAiDER/cli/validators.py 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 def getBufferedExtent ( lats , lons = None , buf = 0. ): ''' get the bounding box around a set of lats/lons ''' if lons is None : lats , lons = lats [ ... , 0 ], lons [ ... , 1 ] try : if ( lats . size == 1 ) & ( lons . size == 1 ): out = [ lats - buf , lats + buf , lons - buf , lons + buf ] elif ( lats . size > 1 ) & ( lons . size > 1 ): out = [ np . nanmin ( lats ), np . nanmax ( lats ), np . nanmin ( lons ), np . nanmax ( lons )] elif lats . size == 1 : out = [ lats - buf , lats + buf , np . nanmin ( lons ), np . nanmax ( lons )] elif lons . size == 1 : out = [ np . nanmin ( lats ), np . nanmax ( lats ), lons - buf , lons + buf ] except AttributeError : if ( isinstance ( lats , tuple ) or isinstance ( lats , list )) and len ( lats ) == 2 : out = [ min ( lats ) - buf , max ( lats ) + buf , min ( lons ) - buf , max ( lons ) + buf ] except Exception as e : raise RuntimeError ( 'Not a valid lat/lon shape or variable' ) return np . array ( out ) get_heights ( args , out , station_file , bounding_box = None ) Parse the Height info and download a DEM if needed Source code in RAiDER/cli/validators.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def get_heights ( args , out , station_file , bounding_box = None ): ''' Parse the Height info and download a DEM if needed ''' dem_path = os . path . join ( out , 'geom' ) if not os . path . exists ( dem_path ): os . mkdir ( dem_path ) out = { 'dem' : None , 'height_file_rdr' : None , 'height_levels' : None , } if 'dem' in args . keys (): if ( station_file is not None ): if 'Hgt_m' not in pd . read_csv ( station_file ): out [ 'dem' ] = os . path . join ( dem_path , 'GLO30.dem' ) elif os . path . exists ( args . dem ): out [ 'dem' ] = args [ 'dem' ] if bounding_box is not None : dem_bounds = rio_extents ( rio_profile ( args . dem )) lats = dem_bounds [: 2 ] lons = dem_bounds [ 2 :] if isOutside ( bounding_box , getBufferedExtent ( lats , lons , buf = _BUFFER_SIZE , ) ): raise ValueError ( 'Existing DEM does not cover the area of the input lat/lon ' 'points; either move the DEM, delete it, or change the input ' 'points.' ) else : pass # will download the dem later elif 'height_file_rdr' in args . keys (): out [ 'height_file_rdr' ] = args . height_file_rdr elif 'height_levels' in args . keys (): l = re . findall ( '[0-9]+' , args . height_levels ) out [ 'height_levels' ] = [ float ( ll ) for ll in l ] else : # download the DEM if needed out [ 'dem' ] = os . path . join ( dem_path , 'GLO30.dem' ) return out get_query_region ( args ) Parse the query region from inputs Source code in RAiDER/cli/validators.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def get_query_region ( args ): ''' Parse the query region from inputs ''' # Get bounds from the inputs # make sure this is first if args . get ( 'use_dem_latlon' ): query = GeocodedFile ( args . dem , is_dem = True ) elif args . get ( 'lat_file' ): hgt_file = args . get ( 'height_file_rdr' ) # only get it if exists dem_file = args . get ( 'dem' ) query = RasterRDR ( args . lat_file , args . lon_file , hgt_file , dem_file ) elif args . get ( 'station_file' ): query = StationFile ( args . station_file ) elif args . get ( 'bounding_box' ): bbox = enforce_bbox ( args . bounding_box ) if ( np . min ( bbox [ 0 ]) < - 90 ) | ( np . max ( bbox [ 1 ]) > 90 ): raise ValueError ( 'Lats are out of N/S bounds; are your lat/lon coordinates switched? Should be SNWE' ) query = BoundingBox ( bbox ) elif args . get ( 'geocoded_file' ): gfile = os . path . basename ( args . geocoded_file ) . upper () if ( gfile . startswith ( 'SRTM' ) or gfile . startswith ( 'GLO' )): logger . debug ( 'Using user DEM: %s ' , gfile ) is_dem = True else : is_dem = False query = GeocodedFile ( args . geocoded_file , is_dem = is_dem ) ## untested elif 'los_cube' in args . keys (): query = Geocube ( args . los_cube ) else : # TODO: Need to incorporate the cube raise ValueError ( 'No valid query points or bounding box found in the configuration file' ) return query isInside ( extent1 , extent2 ) Determine whether all of extent1 lies inside extent2 extent1/2 should be a list containing [lower_lat, upper_lat, left_lon, right_lon]. Equal extents are considered \"inside\" Source code in RAiDER/cli/validators.py 352 353 354 355 356 357 358 359 360 361 362 363 364 def isInside ( extent1 , extent2 ): ''' Determine whether all of extent1 lies inside extent2 extent1/2 should be a list containing [lower_lat, upper_lat, left_lon, right_lon]. Equal extents are considered \"inside\" ''' t1 = extent1 [ 0 ] <= extent2 [ 0 ] t2 = extent1 [ 1 ] >= extent2 [ 1 ] t3 = extent1 [ 2 ] <= extent2 [ 2 ] t4 = extent1 [ 3 ] >= extent2 [ 3 ] if np . all ([ t1 , t2 , t3 , t4 ]): return True return False isOutside ( extent1 , extent2 ) Determine whether any of extent1 lies outside extent2 extent1/2 should be a list containing [lower_lat, upper_lat, left_lon, right_lon] Equal extents are considered \"inside\" Source code in RAiDER/cli/validators.py 337 338 339 340 341 342 343 344 345 346 347 348 349 def isOutside ( extent1 , extent2 ): ''' Determine whether any of extent1 lies outside extent2 extent1/2 should be a list containing [lower_lat, upper_lat, left_lon, right_lon] Equal extents are considered \"inside\" ''' t1 = extent1 [ 0 ] < extent2 [ 0 ] t2 = extent1 [ 1 ] > extent2 [ 1 ] t3 = extent1 [ 2 ] < extent2 [ 2 ] t4 = extent1 [ 3 ] > extent2 [ 3 ] if np . any ([ t1 , t2 , t3 , t4 ]): return True return False modelName2Module ( model_name ) Turn an arbitrary string into a module name. Takes as input a model name, which hopefully looks like ERA-I, and converts it to a module name, which will look like erai. I doesn't always produce a valid module name, but that's not the goal. The goal is just to handle common cases. Inputs model_name - Name of an allowed weather model (e.g., 'era-5') Outputs module_name - Name of the module wmObject - callable, weather model object Source code in RAiDER/cli/validators.py 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 def modelName2Module ( model_name ): \"\"\"Turn an arbitrary string into a module name. Takes as input a model name, which hopefully looks like ERA-I, and converts it to a module name, which will look like erai. I doesn't always produce a valid module name, but that's not the goal. The goal is just to handle common cases. Inputs: model_name - Name of an allowed weather model (e.g., 'era-5') Outputs: module_name - Name of the module wmObject - callable, weather model object \"\"\" module_name = 'RAiDER.models.' + model_name . lower () . replace ( '-' , '' ) model_module = importlib . import_module ( module_name ) wmObject = getattr ( model_module , model_name . upper () . replace ( '-' , '' )) return module_name , wmObject parse_dates ( arg_dict ) Determine the requested dates from the input parameters Source code in RAiDER/cli/validators.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 def parse_dates ( arg_dict ): ''' Determine the requested dates from the input parameters ''' if 'date_list' in arg_dict . keys (): l = arg_dict [ 'date_list' ] if isinstance ( l , str ): l = re . findall ( '[0-9]+' , l ) L = [ enforce_valid_dates ( d ) for d in l ] else : try : start = arg_dict [ 'date_start' ] except KeyError : raise ValueError ( 'Inputs must include either date_list or date_start' ) start = enforce_valid_dates ( start ) if 'date_end' in arg_dict . keys (): end = arg_dict [ 'date_end' ] end = enforce_valid_dates ( end ) else : end = start if 'date_step' in arg_dict . keys (): step = int ( arg_dict [ 'date_step' ]) else : step = 1 L = [ start + timedelta ( days = step ) for step in range ( 0 , ( end - start ) . days + 1 , step )] return L delay RAiDER tropospheric delay calculation This module provides the main RAiDER functionality for calculating tropospheric wet and hydrostatic delays from a weather model. Weather models are accessed as NETCDF files and should have \"wet\" \"hydro\" \"wet_total\" and \"hydro_total\" fields specified. transformPoints ( lats , lons , hgts , old_proj , new_proj ) Transform lat/lon/hgt data to an array of points in a new projection Parameters: Name Type Description Default lats np . ndarray ndarray - WGS-84 latitude (EPSG: 4326) required lons np . ndarray ndarray - ditto for longitude required hgts np . ndarray ndarray - Ellipsoidal height in meters required old_proj CRS CRS - the original projection of the points required new_proj CRS CRS - the new projection in which to return the points required Returns: Name Type Description ndarray np . ndarray the array of query points in the weather model coordinate system (YX) Source code in RAiDER/delay.py 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 def transformPoints ( lats : np . ndarray , lons : np . ndarray , hgts : np . ndarray , old_proj : CRS , new_proj : CRS ) -> np . ndarray : ''' Transform lat/lon/hgt data to an array of points in a new projection Args: lats: ndarray - WGS-84 latitude (EPSG: 4326) lons: ndarray - ditto for longitude hgts: ndarray - Ellipsoidal height in meters old_proj: CRS - the original projection of the points new_proj: CRS - the new projection in which to return the points Returns: ndarray: the array of query points in the weather model coordinate system (YX) ''' t = Transformer . from_crs ( old_proj , new_proj ) # Flags for flipping inputs or outputs if not isinstance ( new_proj , pyproj . CRS ): new_proj = CRS . from_epsg ( new_proj . lstrip ( 'EPSG:' )) if not isinstance ( old_proj , pyproj . CRS ): old_proj = CRS . from_epsg ( old_proj . lstrip ( 'EPSG:' )) in_flip = old_proj . axis_info [ 0 ] . direction out_flip = new_proj . axis_info [ 0 ] . direction if in_flip == 'east' : res = t . transform ( lons , lats , hgts ) else : res = t . transform ( lats , lons , hgts ) if out_flip == 'east' : return np . stack (( res [ 1 ], res [ 0 ], res [ 2 ]), axis =- 1 ) . T else : return np . stack ( res , axis =- 1 ) . T tropo_delay ( dt , weather_model_file , aoi , los , height_levels = None , out_proj = 4326 , cube_spacing_m = None , look_dir = 'right' ) Calculate integrated delays on query points. Parameters: Name Type Description Default dt Datetime - Datetime object for determining when to calculate delays required weather_model_File string - Name of the NETCDF file containing a pre-processed weather model required aoi AOI object - AOI object required los LOS object - LOS object required height_levels List [ float ] list - (optional) list of height levels on which to calculate delays. Only needed for cube generation. None out_proj int | str int,str - (optional) EPSG code for output projection 4326 look_dir str str - (optional) Satellite look direction. Only needed for slant delay calculation 'right' cube_spacing_m int int - (optional) Horizontal spacing in meters when generating cubes None Returns: Type Description xarray Dataset or ndarrays: - wet and hydrostatic delays at the grid nodes / query points. Source code in RAiDER/delay.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def tropo_delay ( dt , weather_model_file : str , aoi , los , height_levels : List [ float ] = None , out_proj : int | str = 4326 , cube_spacing_m : int = None , look_dir : str = 'right' , ): \"\"\" Calculate integrated delays on query points. Args: dt: Datetime - Datetime object for determining when to calculate delays weather_model_File: string - Name of the NETCDF file containing a pre-processed weather model aoi: AOI object - AOI object los: LOS object - LOS object height_levels: list - (optional) list of height levels on which to calculate delays. Only needed for cube generation. out_proj: int,str - (optional) EPSG code for output projection look_dir: str - (optional) Satellite look direction. Only needed for slant delay calculation cube_spacing_m: int - (optional) Horizontal spacing in meters when generating cubes Returns: xarray Dataset *or* ndarrays: - wet and hydrostatic delays at the grid nodes / query points. \"\"\" # get heights if height_levels is None : with xarray . load_dataset ( weather_model_file ) as ds : height_levels = ds . z . values #TODO: expose this as library function ds = _get_delays_on_cube ( dt , weather_model_file , aoi . bounds (), height_levels , los , out_proj = out_proj , cube_spacing_m = cube_spacing_m , look_dir = look_dir ) if ( aoi . type () == 'bounding_box' ) or ( aoi . type () == 'Geocube' ): return ds , None else : # CRS can be an int, str, or CRS object try : out_proj = CRS . from_epsg ( out_proj ) except pyproj . exceptions . CRSError : out_proj = out_proj pnt_proj = CRS . from_epsg ( 4326 ) lats , lons = aoi . readLL () hgts = aoi . readZ () pnts = transformPoints ( lats , lons , hgts , pnt_proj , out_proj ) if pnts . ndim == 3 : pnts = pnts . transpose ( 1 , 2 , 0 ) elif pnts . ndim == 2 : pnts = pnts . T ifWet , ifHydro = getInterpolators ( ds , 'ztd' ) # the cube from get_delays_on_cube calls the total delays 'wet' and 'hydro' wetDelay = ifWet ( pnts ) hydroDelay = ifHydro ( pnts ) # return the delays (ZTD or STD) if los . is_Projected (): los . setTime ( dt ) los . setPoints ( lats , lons , hgts ) wetDelay = los ( wetDelay ) hydroDelay = los ( hydroDelay ) return wetDelay , hydroDelay delayFcns calculate_start_points ( x , y , z , ds ) wm_file: str - A file containing a regularized weather model. ndarray - a * x 3 array containing the XYZ locations of the pixels in ECEF coordinates. Note the ordering of the array is [Y X Z] Source code in RAiDER/delayFcns.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def calculate_start_points ( x , y , z , ds ): ''' Args: ---------- wm_file: str - A file containing a regularized weather model. Returns: ------- SP: ndarray - a * x 3 array containing the XYZ locations of the pixels in ECEF coordinates. Note the ordering of the array is [Y X Z] ''' [ X , Y , Z ] = np . meshgrid ( x , y , z ) try : t = Transformer . from_crs ( ds [ 'CRS' ], 4978 , always_xy = True ) # converts to WGS84 geocentric except : print ( \"I can't find a CRS in the weather model file, so I will assume you are using WGS84\" ) t = Transformer . from_crs ( 4326 , 4978 , always_xy = True ) # converts to WGS84 geocentric return np . moveaxis ( np . array ( t . transform ( X , Y , Z )), 0 , - 1 ), np . stack ([ X , Y , Z ], axis =- 1 ) chunk ( chunkSize , in_shape ) Create a set of indices to use as chunks Source code in RAiDER/delayFcns.py 138 139 140 141 142 143 144 def chunk ( chunkSize , in_shape ): ''' Create a set of indices to use as chunks ''' startInds = makeChunkStartInds ( chunkSize , in_shape ) chunkInds = makeChunksFromInds ( startInds , chunkSize , in_shape ) return chunkInds getInterpolators ( wm_file , kind = 'pointwise' , shared = False ) Read 3D gridded data from a processed weather model file and wrap it with an interpolator Source code in RAiDER/delayFcns.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def getInterpolators ( wm_file , kind = 'pointwise' , shared = False ): ''' Read 3D gridded data from a processed weather model file and wrap it with an interpolator ''' # Get the weather model data try : ds = xarray . load_dataset ( wm_file ) except : ds = wm_file xs_wm = np . array ( ds . variables [ 'x' ][:]) ys_wm = np . array ( ds . variables [ 'y' ][:]) zs_wm = np . array ( ds . variables [ 'z' ][:]) wet = ds . variables [ 'wet_total' if kind == 'total' else 'wet' ][:] hydro = ds . variables [ 'hydro_total' if kind == 'total' else 'hydro' ][:] wet = np . array ( wet ) . transpose ( 1 , 2 , 0 ) hydro = np . array ( hydro ) . transpose ( 1 , 2 , 0 ) # If shared interpolators are requested # The arrays are not modified - so turning off lock for performance if shared : xs_wm = make_shared_raw ( xs_wm ) ys_wm = make_shared_raw ( ys_wm ) zs_wm = make_shared_raw ( zs_wm ) wet = make_shared_raw ( wet ) hydro = make_shared_raw ( hydro ) ifWet = Interpolator (( ys_wm , xs_wm , zs_wm ), wet , fill_value = np . nan , bounds_error = False ) ifHydro = Interpolator (( ys_wm , xs_wm , zs_wm ), hydro , fill_value = np . nan , bounds_error = False ) return ifWet , ifHydro get_delays ( stepSize , SP , LOS , wm_file , cpu_num = 0 ) Create the integration points for each ray path. Source code in RAiDER/delayFcns.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def get_delays ( stepSize , SP , LOS , wm_file , cpu_num = 0 ): ''' Create the integration points for each ray path. ''' ifWet , ifHydro = getInterpolators ( wm_file ) with xarray . load_dataset ( wm_file ) as f : try : wm_proj = f . attrs [ 'CRS' ] except : wm_proj = 4326 print ( \"I can't find a CRS in the weather model file, so I will assume you are using WGS84\" ) t = Transformer . from_crs ( 4326 , 4978 , always_xy = True ) # converts to WGS84 geocentric in_shape = SP . shape [: - 1 ] chunkSize = in_shape CHUNKS = chunk ( in_shape , in_shape ) Nchunks = len ( CHUNKS ) max_len = 15000 stepSize = 100 chunk_inputs = [( kk , CHUNKS [ kk ], wm_proj , SP , LOS , chunkSize , stepSize , ifWet , ifHydro , max_len , wm_file ) for kk in range ( Nchunks )] if Nchunks == 1 : delays = process_chunk ( * chunk_inputs [ 0 ]) else : with mp . Pool () as pool : individual_results = pool . starmap ( process_chunk , chunk_inputs ) try : delays = np . concatenate ( individual_results ) except ValueError : delays = np . concatenate ( individual_results , axis =- 1 ) wet_delay = delays [ 0 , ... ] . reshape ( in_shape ) hydro_delay = delays [ 1 , ... ] . reshape ( in_shape ) return wet_delay , hydro_delay interpolate2 ( fun , x , y , z ) helper function to make the interpolation step cleaner Source code in RAiDER/delayFcns.py 256 257 258 259 260 261 262 263 def interpolate2 ( fun , x , y , z ): ''' helper function to make the interpolation step cleaner ''' in_shape = x . shape out = fun (( y . ravel (), x . ravel (), z . ravel ())) # note that this re-ordering is on purpose to match the weather model outData = out . reshape ( in_shape ) return outData makeChunkStartInds ( chunkSize , in_shape ) Create a list of start indices for chunking a numpy D-dimensional array. Inputs chunkSize - length-D tuple containing chunk sizes in_shape - length-D tuple containing the shape of the array to be chunked Outputs chunkInds - a list of length-D tuples, where each tuple is the starting multi-index of each chunk Example makeChunkStartInds((2,2,16), (4,8,16)) [(0, 0, 0), (0, 2, 0), (0, 4, 0), (0, 6, 0), (2, 0, 0), (2, 2, 0), (2, 4, 0), (2, 6, 0)] Source code in RAiDER/delayFcns.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def makeChunkStartInds ( chunkSize , in_shape ): ''' Create a list of start indices for chunking a numpy D-dimensional array. Inputs: chunkSize - length-D tuple containing chunk sizes in_shape - length-D tuple containing the shape of the array to be chunked Outputs chunkInds - a list of length-D tuples, where each tuple is the starting multi-index of each chunk Example: makeChunkStartInds((2,2,16), (4,8,16)) Output: [(0, 0, 0), (0, 2, 0), (0, 4, 0), (0, 6, 0), (2, 0, 0), (2, 2, 0), (2, 4, 0), (2, 6, 0)] ''' if len ( in_shape ) == 1 : chunkInds = [( i ,) for i in range ( 0 , in_shape [ 0 ], chunkSize [ 0 ])] elif len ( in_shape ) == 2 : chunkInds = [( i , j ) for i , j in itertools . product ( range ( 0 , in_shape [ 0 ], chunkSize [ 0 ]), range ( 0 , in_shape [ 1 ], chunkSize [ 1 ]))] elif len ( in_shape ) == 3 : chunkInds = [( i , j , k ) for i , j , k in itertools . product ( range ( 0 , in_shape [ 0 ], chunkSize [ 0 ]), range ( 0 , in_shape [ 1 ], chunkSize [ 1 ]), range ( 0 , in_shape [ 2 ], chunkSize [ 2 ]))] else : raise NotImplementedError ( 'makeChunkStartInds: ndim > 3 not supported' ) return chunkInds makeChunksFromInds ( startInd , chunkSize , in_shape ) From a length-N list of tuples containing starting indices, create a list of indices into chunks of a numpy D-dimensional array. Inputs startInd - A length-N list of D-dimensional tuples containing the starting indices of a set of chunks chunkSize - A D-dimensional tuple containing chunk size in each dimension in_shape - A D-dimensional tuple containing the size of each dimension Outputs chunks - A length-N list of length-D lists, where each element of the length-D list is a numpy array of indices Example makeChunksFromInds([(0, 0), (0, 2), (2, 0), (2, 2)],(4,4),(2,2)) Output [[np.array([0, 0, 1, 1]), np.array([0, 1, 0, 1])], [np.array([0, 0, 1, 1]), np.array([2, 3, 2, 3])], [np.array([2, 2, 3, 3]), np.array([0, 1, 0, 1])], [np.array([2, 2, 3, 3]), np.array([2, 3, 2, 3])]] Source code in RAiDER/delayFcns.py 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 def makeChunksFromInds ( startInd , chunkSize , in_shape ): ''' From a length-N list of tuples containing starting indices, create a list of indices into chunks of a numpy D-dimensional array. Inputs: startInd - A length-N list of D-dimensional tuples containing the starting indices of a set of chunks chunkSize - A D-dimensional tuple containing chunk size in each dimension in_shape - A D-dimensional tuple containing the size of each dimension Outputs: chunks - A length-N list of length-D lists, where each element of the length-D list is a numpy array of indices Example: makeChunksFromInds([(0, 0), (0, 2), (2, 0), (2, 2)],(4,4),(2,2)) Output: [[np.array([0, 0, 1, 1]), np.array([0, 1, 0, 1])], [np.array([0, 0, 1, 1]), np.array([2, 3, 2, 3])], [np.array([2, 2, 3, 3]), np.array([0, 1, 0, 1])], [np.array([2, 2, 3, 3]), np.array([2, 3, 2, 3])]] ''' indices = [] for ci in startInd : index = [] for si , k , dim in zip ( ci , chunkSize , range ( len ( chunkSize ))): if si + k > in_shape [ dim ]: dend = in_shape [ dim ] else : dend = si + k index . append ( np . array ( range ( si , dend ))) indices . append ( index ) # Now create the index mesh (for Ndim > 1) chunks = [] if len ( in_shape ) > 1 : for index in indices : chunks . append ([ np . array ( g ) for g in zip ( * list ( itertools . product ( * index )))]) else : chunks = indices return chunks make_shared_raw ( inarr ) Make numpy view array of mp.Array Source code in RAiDER/delayFcns.py 123 124 125 126 127 128 129 130 131 132 133 134 135 def make_shared_raw ( inarr ): \"\"\" Make numpy view array of mp.Array \"\"\" # Create flat shared array shared_arr = mp . RawArray ( 'd' , inarr . size ) # Create a numpy view of it shared_arr_np = np . ndarray ( inarr . shape , dtype = np . float64 , buffer = shared_arr ) # Copy data to shared array np . copyto ( shared_arr_np , inarr ) return shared_arr_np process_chunk ( k , chunkInds , proj_wm , SP , SLV , chunkSize , stepSize , ifWet , ifHydro , max_len , wm_file ) Perform the interpolation and integration over a single chunk. Source code in RAiDER/delayFcns.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 def process_chunk ( k , chunkInds , proj_wm , SP , SLV , chunkSize , stepSize , ifWet , ifHydro , max_len , wm_file ): \"\"\" Perform the interpolation and integration over a single chunk. \"\"\" # Transformer from ECEF to weather model t = Transformer . from_proj ( 4978 , proj_wm , always_xy = True ) # datatype must be specific for the cython makePoints* function _DTYPE = np . float64 # H5PY does not support fancy indexing with tuples, hence this if/else check if len ( chunkSize ) == 1 : row = chunkInds [ 0 ] ray = makePoints1D ( max_len , SP [ row , :] . astype ( _DTYPE ), SLV [ row , :] . astype ( _DTYPE ), stepSize ) elif len ( chunkSize ) == 2 : row , col = chunkInds ray = makePoints1D ( max_len , SP [ row , col , :] . astype ( _DTYPE ), SLV [ row , col , :] . astype ( _DTYPE ), stepSize ) elif len ( chunkSize ) == 3 : row , col , zind = chunkInds ray = makePoints1D ( max_len , SP [ row , col , zind , :] . astype ( _DTYPE ), SLV [ row , col , zind , :] . astype ( _DTYPE ), stepSize ) else : raise RuntimeError ( 'Data in more than 4 dimensions is not supported' ) ray_x , ray_y , ray_z = t . transform ( ray [ ... , 0 , :], ray [ ... , 1 , :], ray [ ... , 2 , :]) delay_wet = interpolate2 ( ifWet , ray_x , ray_y , ray_z ) delay_hydro = interpolate2 ( ifHydro , ray_x , ray_y , ray_z ) int_delays = _integrateLOS ( stepSize , delay_wet , delay_hydro ) return int_delays dem download_dem ( ll_bounds , writeDEM = False , outName = 'warpedDEM' , buf = 0.02 , overwrite = False ) Download a DEM if one is not already present. Source code in RAiDER/dem.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def download_dem ( ll_bounds , writeDEM = False , outName = 'warpedDEM' , buf = 0.02 , overwrite = False , ): \"\"\" Download a DEM if one is not already present. \"\"\" if os . path . exists ( outName ) and not overwrite : logger . info ( 'Using existing DEM: %s ' , outName ) zvals , metadata = rio_open ( outName , returnProj = True ) else : # inExtent is SNWE # dem-stitcher wants WSEN bounds = [ np . floor ( ll_bounds [ 2 ]) - buf , np . floor ( ll_bounds [ 0 ]) - buf , np . ceil ( ll_bounds [ 3 ]) + buf , np . ceil ( ll_bounds [ 1 ]) + buf ] zvals , metadata = stitch_dem ( bounds , dem_name = 'glo_30' , dst_ellipsoidal_height = True , dst_area_or_point = 'Area' , ) if writeDEM : with rasterio . open ( outName , 'w' , ** metadata ) as ds : ds . write ( zvals , 1 ) ds . update_tags ( AREA_OR_POINT = 'Point' ) logger . info ( 'Wrote DEM: %s ' , outName ) return zvals , metadata getHeights ( ll_bounds , dem_type , dem_file , lats = None , lons = None ) Fcn to return heights from a DEM, either one that already exists or will download one if needed. Source code in RAiDER/dem.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def getHeights ( ll_bounds , dem_type , dem_file , lats = None , lons = None ): ''' Fcn to return heights from a DEM, either one that already exists or will download one if needed. ''' # height_type, height_data = heights if dem_type == 'hgt' : htinfo = get_file_and_band ( dem_file ) hts = rio_open ( htinfo [ 0 ], band = htinfo [ 1 ]) elif dem_type == 'csv' : # Heights are in the .csv file hts = pd . read_csv ( dem_file )[ 'Hgt_m' ] . values elif dem_type == 'interpolate' : # heights will be vertically interpolated to the heightlvs hts = None elif ( dem_type == 'download' ) or ( dem_type == 'dem' ): if ~ os . path . exists ( dem_file ): download_dem ( ll_bounds , writeDEM = True , outName = dem_file ) #TODO: interpolate heights to query lats/lons # Interpolate to the query points hts = interpolateDEM ( dem_file , lats , lons , ) return hts getStationDelays get_date ( stationFile ) extract the date from a station delay file Source code in RAiDER/getStationDelays.py 232 233 234 235 236 237 238 239 240 241 242 def get_date ( stationFile ): ''' extract the date from a station delay file ''' # find the date info year = int ( stationFile [ 1 ]) doy = int ( stationFile [ 2 ]) date = dt . datetime ( year , 1 , 1 ) + dt . timedelta ( doy - 1 ) return date , year , doy get_delays_UNR ( stationFile , filename , dateList , returnTime = None ) Parses and returns a dictionary containing either (1) all the GPS delays, if returnTime is None, or (2) only the delay at the closest times to to returnTime. Inputs stationFile - a .gz station delay file returnTime - specified time of GPS delay Outputs a dict and CSV file containing the times and delay information (delay in mm, delay uncertainty, delay gradients) *NOTE: Due to a formatting error in the tropo SINEX files, the two tropospheric gradient columns (TGNTOT and TGETOT) are interchanged, as are the formal error columns (_SIG). Source \u2014> http://geodesy.unr.edu/gps_timeseries/README_trop2.txt) Source code in RAiDER/getStationDelays.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def get_delays_UNR ( stationFile , filename , dateList , returnTime = None ): ''' Parses and returns a dictionary containing either (1) all the GPS delays, if returnTime is None, or (2) only the delay at the closest times to to returnTime. Inputs: stationFile - a .gz station delay file returnTime - specified time of GPS delay Outputs: a dict and CSV file containing the times and delay information (delay in mm, delay uncertainty, delay gradients) *NOTE: Due to a formatting error in the tropo SINEX files, the two tropospheric gradient columns (TGNTOT and TGETOT) are interchanged, as are the formal error columns (_SIG). Source \u2014> http://geodesy.unr.edu/gps_timeseries/README_trop2.txt) ''' # Refer to the following sites to interpret stationFile variable names: # ftp://igs.org/pub/data/format/sinex_tropo.txt # http://geodesy.unr.edu/gps_timeseries/README_trop2.txt # Wet and hydrostratic delays were derived as so: # Constants \u2014> k1 = 0.704, k2 = 0.776, k3 = 3739.0, m = 18.0152/28.9644, # k2' = k2-(k1*m) = 0.33812796398337275, Rv = 461.5 J/(kg\u00b7K), \u03c1l = 997 kg/m^3 # Note wet delays passed here may be computed as so # where PMV = precipitable water vapor, P = total atm pressure, Tm = mean temp of the column \u2014> # Wet zenith delay = 10^-6 \u03c1lRv(k2' + k3/Tm) PMV # Hydrostatic zenith delay = Total zenith delay - wet zenith delay = k1*(P/Tm) # Source \u2014> Hanssen, R. F. (2001) eqns. 6.2.7-10 # sort through station zip files allstationTarfiles = [] # if URL if stationFile . startswith ( 'http' ): r = requests . get ( stationFile ) ziprepo = zipfile . ZipFile ( io . BytesIO ( r . content )) # if downloaded file else : ziprepo = zipfile . ZipFile ( stationFile ) # iterate through tarfiles stationTarlist = sorted ( ziprepo . namelist ()) final_stationTarlist = [] for j in stationTarlist : # get the date of the file time , yearFromFile , doyFromFile = get_date ( os . path . basename ( j ) . split ( '.' )) # check if in list of specified input dates if time . strftime ( '%Y-%m- %d ' ) not in dateList : continue final_stationTarlist . append ( j ) f = gzip . open ( ziprepo . open ( j ), 'rb' ) # initialize variables d , Sig , dwet , dhydro , timesList = [], [], [], [], [] flag = False for line in f . readlines (): try : line = line . decode ( 'utf-8' ) except UnicodeDecodeError : line = line . decode ( 'latin-1' ) if flag : # Do not attempt to read header if 'SITE' in line : continue # Attempt to read data try : split_lines = line . split () # units: mm, mm, mm, deg, deg, deg, deg, mm, mm, K trotot , trototSD , trwet , tgetot , tgetotSD , tgntot , tgntotSD , wvapor , wvaporSD , mtemp = \\ [ float ( t ) for t in split_lines [ 2 :]] except BaseException : # TODO: What error(s)? continue site = split_lines [ 0 ] year , doy , seconds = [ int ( n ) for n in split_lines [ 1 ] . split ( ':' )] # Break iteration if time from line in file does not match date reported in filename if doy != doyFromFile : logger . warning ( 'time %s from line in conflict with time %s from file ' ' %s , will continue reading next tarfile(s)' , doy , doyFromFile , j ) continue # convert units from mm to m d . append ( trotot * 0.001 ) Sig . append ( trototSD * 0.001 ) dwet . append ( trwet * 0.001 ) dhydro . append (( trotot - trwet ) * 0.001 ) timesList . append ( seconds ) if 'TROP/SOLUTION' in line : flag = True del f # Break iteration if file contains no data. if d == []: logger . warning ( 'file %s for station %s is empty, will continue reading next ' 'tarfile(s)' , j , j . split ( '.' )[ 0 ] ) continue # check for missing times true_times = list ( range ( 0 , 86400 , 300 )) if len ( timesList ) != len ( true_times ): missing = [ True if t not in timesList else False for t in true_times ] mask = np . array ( missing ) delay , sig , wet_delay , hydro_delay = [ np . full (( 288 ,), np . nan )] * 4 delay [ ~ mask ] = d sig [ ~ mask ] = Sig wet_delay [ ~ mask ] = dwet hydro_delay [ ~ mask ] = dhydro times = true_times . copy () else : delay = np . array ( d ) times = np . array ( timesList ) sig = np . array ( Sig ) wet_delay = np . array ( dwet ) hydro_delay = np . array ( dhydro ) # if time not specified, pass all times if returnTime is None : filtoutput = { 'ID' : [ site ] * len ( wet_delay ), 'Date' : [ time ] * len ( wet_delay ), 'ZTD' : delay , 'wet_delay' : wet_delay , 'hydrostatic_delay' : hydro_delay , 'times' : times , 'sigZTD' : sig } filtoutput = [{ key : value [ k ] for key , value in filtoutput . items ()} for k in range ( len ( filtoutput [ 'ID' ]))] else : index = np . argmin ( np . abs ( np . array ( timesList ) - returnTime )) filtoutput = [{ 'ID' : site , 'Date' : time , 'ZTD' : delay [ index ], 'wet_delay' : wet_delay [ index ], 'hydrostatic_delay' : hydro_delay [ index ], 'times' : times [ index ], 'sigZTD' : sig [ index ]}] # setup pandas array and write output to CSV, making sure to update existing CSV. filtoutput = pd . DataFrame ( filtoutput ) if os . path . exists ( filename ): filtoutput . to_csv ( filename , index = False , mode = 'a' , header = False ) else : filtoutput . to_csv ( filename , index = False ) # record all used tar files allstationTarfiles . extend ([ os . path . join ( stationFile , k ) for k in stationTarlist ]) allstationTarfiles . sort () del ziprepo return get_station_data ( inFile , dateList , gps_repo = None , numCPUs = 8 , outDir = None , returnTime = None ) Pull tropospheric delay data for a given station name Source code in RAiDER/getStationDelays.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def get_station_data ( inFile , dateList , gps_repo = None , numCPUs = 8 , outDir = None , returnTime = None ): ''' Pull tropospheric delay data for a given station name ''' if outDir is None : outDir = os . getcwd () pathbase = os . path . join ( outDir , 'GPS_delays' ) if not os . path . exists ( pathbase ): os . mkdir ( pathbase ) returnTime = seconds_of_day ( returnTime ) # print warning if not divisible by 3 seconds if returnTime % 3 != 0 : index = np . argmin ( np . abs ( np . array ( list ( range ( 0 , 86400 , 300 ))) - returnTime )) updatedreturnTime = str ( dt . timedelta ( seconds = list ( range ( 0 , 86400 , 300 ))[ index ])) logger . warning ( 'input time %s not divisible by 3 seconds, so next closest time %s ' 'will be chosen' , returnTime , updatedreturnTime ) returnTime = updatedreturnTime # get list of station zip files inFile_df = pd . read_csv ( inFile ) stationFiles = inFile_df [ 'path' ] . to_list () del inFile_df if len ( stationFiles ) > 0 : outputfiles = [] args = [] # parse delays from UNR if gps_repo == 'UNR' : for sf in stationFiles : StationID = os . path . basename ( sf ) . split ( '.' )[ 0 ] name = os . path . join ( pathbase , StationID + '_ztd.csv' ) args . append (( sf , name , dateList , returnTime )) outputfiles . append ( name ) # Parallelize remote querying of zenith delays with multiprocessing . Pool ( numCPUs ) as multipool : multipool . starmap ( get_delays_UNR , args ) # confirm file exists (i.e. valid delays exists for specified time/region). outputfiles = [ i for i in outputfiles if os . path . exists ( i )] # Consolidate all CSV files into one object if outputfiles == []: raise Exception ( 'No valid delays found for specified time/region.' ) name = os . path . join ( outDir , ' {} combinedGPS_ztd.csv' . format ( gps_repo )) statsFile = pd . concat ([ pd . read_csv ( i ) for i in outputfiles ]) # drop all duplicate lines statsFile . drop_duplicates ( inplace = True ) # Convert the above object into a csv file and export statsFile . to_csv ( name , index = False , encoding = \"utf-8\" ) del statsFile # Add lat/lon/height info origstatsFile = pd . read_csv ( inFile ) statsFile = pd . read_csv ( name ) statsFile = pd . merge ( left = statsFile , right = origstatsFile [[ 'ID' , 'Lat' , 'Lon' , 'Hgt_m' ]], how = 'left' , left_on = 'ID' , right_on = 'ID' ) # drop all lines with nans and sort by station ID and year statsFile . dropna ( how = 'any' , inplace = True ) # drop all duplicate lines statsFile . drop_duplicates ( inplace = True ) statsFile . sort_values ([ 'ID' , 'Date' ]) statsFile . to_csv ( name , index = False ) del origstatsFile , statsFile seconds_of_day ( returnTime ) Convert HH:MM:SS format time-tag to seconds of day. Source code in RAiDER/getStationDelays.py 245 246 247 248 249 250 251 252 253 254 def seconds_of_day ( returnTime ): ''' Convert HH:MM:SS format time-tag to seconds of day. ''' if isinstance ( returnTime , dt . time ): h , m , s = returnTime . hour , returnTime . minute , returnTime . second else : h , m , s = map ( int , returnTime . split ( \":\" )) return h * 3600 + m * 60 + s gnss downloadGNSSDelays check_url ( url ) Check whether a file exists at a URL. Modified from https://stackoverflow.com/questions/9419162/download-returned-zip-file-from-url Source code in RAiDER/gnss/downloadGNSSDelays.py 153 154 155 156 157 158 159 160 161 162 def check_url ( url ): ''' Check whether a file exists at a URL. Modified from https://stackoverflow.com/questions/9419162/download-returned-zip-file-from-url ''' session = requests_retry_session () r = session . head ( url ) if r . status_code == 404 : url = '' return url download_UNR ( statID , year , writeDir = '.' , download = False , baseURL = _UNR_URL ) Download a zip file containing tropospheric delays for a given station and year The URL format is http://geodesy.unr.edu/gps_timeseries/trop/ / . .trop.zip Inputs statID - 4-character station identifier year - 4-numeral year Source code in RAiDER/gnss/downloadGNSSDelays.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def download_UNR ( statID , year , writeDir = '.' , download = False , baseURL = _UNR_URL ): ''' Download a zip file containing tropospheric delays for a given station and year The URL format is http://geodesy.unr.edu/gps_timeseries/trop/<ssss>/<ssss>.<yyyy>.trop.zip Inputs: statID - 4-character station identifier year - 4-numeral year ''' URL = \" {0} gps_timeseries/trop/ {1} / {1} . {2} .trop.zip\" . format ( baseURL , statID . upper (), year ) logger . debug ( 'Currently checking station %s in %s ' , statID , year ) if download : saveLoc = os . path . abspath ( os . path . join ( writeDir , ' {0} . {1} .trop.zip' . format ( statID . upper (), year ))) filepath = download_url ( URL , saveLoc ) else : filepath = check_url ( URL ) return { 'ID' : statID , 'year' : year , 'path' : filepath } download_tropo_delays ( stats , years , gps_repo = None , writeDir = '.' , numCPUs = 8 , download = False ) Check for and download GNSS tropospheric delays from an archive. If download is True then files will be physically downloaded, which again is not necessary as data can be virtually accessed. Source code in RAiDER/gnss/downloadGNSSDelays.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def download_tropo_delays ( stats , years , gps_repo = None , writeDir = '.' , numCPUs = 8 , download = False ): ''' Check for and download GNSS tropospheric delays from an archive. If download is True then files will be physically downloaded, which again is not necessary as data can be virtually accessed. ''' # argument checking if not isinstance ( stats , ( list , str )): raise TypeError ( 'stats should be a string or a list of strings' ) if not isinstance ( years , ( list , int )): raise TypeError ( 'years should be an int or a list of ints' ) # Iterate over stations and years and check or download data stat_year_tup = itertools . product ( stats , years ) stat_year_tup = (( * tup , writeDir , download ) for tup in stat_year_tup ) # Parallelize remote querying of station locations with multiprocessing . Pool ( numCPUs ) as multipool : # only record valid path if gps_repo == 'UNR' : results = [ fileurl for fileurl in multipool . starmap ( download_UNR , stat_year_tup ) if fileurl [ 'path' ] ] # Write results to file statDF = pd . DataFrame ( results ) . set_index ( 'ID' ) statDF . to_csv ( os . path . join ( writeDir , ' {} gnssStationList_overbbox_withpaths.csv' . format ( gps_repo ))) download_url ( url , save_path , chunk_size = 2048 ) Download a file from a URL. Modified from https://stackoverflow.com/questions/9419162/download-returned-zip-file-from-url Source code in RAiDER/gnss/downloadGNSSDelays.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def download_url ( url , save_path , chunk_size = 2048 ): ''' Download a file from a URL. Modified from https://stackoverflow.com/questions/9419162/download-returned-zip-file-from-url ''' session = requests_retry_session () r = session . get ( url , stream = True ) if r . status_code == 404 : return '' else : logger . debug ( 'Beginning download of %s to %s ' , url , save_path ) with open ( save_path , 'wb' ) as fd : for chunk in r . iter_content ( chunk_size = chunk_size ): fd . write ( chunk ) logger . debug ( 'Completed download of %s to %s ' , url , save_path ) return save_path fix_lons ( lon ) Fix the given longitudes into the range [-180, 180] . Source code in RAiDER/gnss/downloadGNSSDelays.py 180 181 182 183 184 185 186 187 188 def fix_lons ( lon ): \"\"\" Fix the given longitudes into the range ``[-180, 180]``. \"\"\" fixed_lon = (( lon + 180 ) % 360 ) - 180 # Make the positive 180s positive again. if fixed_lon == - 180 and lon > 0 : fixed_lon *= - 1 return fixed_lon get_ID ( line ) Pulls the station ID, lat, lon, and height for a given entry in the UNR text file Source code in RAiDER/gnss/downloadGNSSDelays.py 191 192 193 194 195 196 def get_ID ( line ): ''' Pulls the station ID, lat, lon, and height for a given entry in the UNR text file ''' stat_id , lat , lon , height = line . split ()[: 4 ] return stat_id , float ( lat ), float ( lon ), float ( height ) get_station_list ( bbox = None , writeLoc = None , userstatList = None , name_appendix = '' ) Creates a list of stations inside a lat/lon bounding box from a source Inputs bbox - length-4 list of floats that describes a bounding box. Format is S N W E Source code in RAiDER/gnss/downloadGNSSDelays.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def get_station_list ( bbox = None , writeLoc = None , userstatList = None , name_appendix = '' ): ''' Creates a list of stations inside a lat/lon bounding box from a source Inputs: bbox - length-4 list of floats that describes a bounding box. Format is S N W E ''' writeLoc = os . path . join ( writeLoc or os . getcwd (), 'gnssStationList_overbbox' + name_appendix + '.csv' ) if userstatList : userstatList = read_text_file ( userstatList ) statList = get_stats_by_llh ( llhBox = bbox , userstatList = userstatList ) # write to file and pass final stations list statList . to_csv ( writeLoc , index = False ) stations = list ( statList [ 'ID' ] . values ) return stations , writeLoc get_stats_by_llh ( llhBox = None , baseURL = _UNR_URL , userstatList = None ) Function to pull lat, lon, height, beginning date, end date, and number of solutions for stations inside the bounding box llhBox. llhBox should be a tuple with format (lat1, lat2, lon1, lon2), where lat1, lon1 define the lower left-hand corner and lat2, lon2 define the upper right corner. Source code in RAiDER/gnss/downloadGNSSDelays.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def get_stats_by_llh ( llhBox = None , baseURL = _UNR_URL , userstatList = None ): ''' Function to pull lat, lon, height, beginning date, end date, and number of solutions for stations inside the bounding box llhBox. llhBox should be a tuple with format (lat1, lat2, lon1, lon2), where lat1, lon1 define the lower left-hand corner and lat2, lon2 define the upper right corner. ''' if llhBox is None : llhBox = [ - 90 , 90 , 0 , 360 ] stationHoldings = ' {} NGLStationPages/DataHoldings.txt' . format ( baseURL ) # it's a file like object and works just like a file session = requests_retry_session () data = session . get ( stationHoldings ) stations = [] for ind , line in enumerate ( data . text . splitlines ()): # files are iterable if ind == 0 : continue statID , lat , lon , height = get_ID ( line ) # Only pass if in bbox # And if user list of stations specified, only pass info for stations within list if in_box ( lat , lon , llhBox ) and ( not userstatList or statID in userstatList ): # convert lon into range [-180,180] lon = fix_lons ( lon ) stations . append ({ 'ID' : statID , 'Lat' : lat , 'Lon' : lon , 'Hgt_m' : height }) logger . info ( ' %d stations were found in %s ' , len ( stations ), llhBox ) stations = pd . DataFrame ( stations ) # Report stations from user's list that do not cover bbox if userstatList : userstatList = [ i for i in userstatList if i not in stations [ 'ID' ] . to_list ()] if userstatList : logger . warning ( \"The following user-input stations are not covered by the input \" \"bounding box %s : %s \" , str ( llhBox ) . strip ( '[]' ), str ( userstatList ) . strip ( '[]' ) ) return stations in_box ( lat , lon , llhbox ) Checks whether the given lat, lon pair are inside the bounding box llhbox Source code in RAiDER/gnss/downloadGNSSDelays.py 173 174 175 176 177 def in_box ( lat , lon , llhbox ): ''' Checks whether the given lat, lon pair are inside the bounding box llhbox ''' return lat < llhbox [ 1 ] and lat > llhbox [ 0 ] and lon < llhbox [ 3 ] and lon > llhbox [ 2 ] main ( inps = None ) Main workflow for querying supported GPS repositories for zenith delay information. Source code in RAiDER/gnss/downloadGNSSDelays.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 def main ( inps = None ): \"\"\" Main workflow for querying supported GPS repositories for zenith delay information. \"\"\" try : dateList = inps . date_list returnTime = inps . time except : dateList = inps . dateList returnTime = inps . returnTime station_file = inps . station_file bounding_box = inps . bounding_box gps_repo = inps . gps_repo out = inps . out download = inps . download cpus = inps . cpus verbose = inps . verbose if verbose : logger . setLevel ( logging . DEBUG ) # Create specified output directory if it does not exist. if not os . path . exists ( out ): os . mkdir ( out ) # Setup bounding box if bounding_box : if isinstance ( bounding_box , str ) and not os . path . isfile ( bounding_box ): try : bbox = [ float ( val ) for val in bounding_box . split ()] except ValueError : raise Exception ( 'Cannot understand the --bbox argument. String input is incorrect or path does not exist.' ) elif isinstance ( bounding_box , list ): bbox = bounding_box else : raise Exception ( 'Passing a file with a bounding box not yet supported.' ) long_cross_zero = 1 if bbox [ 2 ] * bbox [ 3 ] < 0 else 0 # if necessary, convert negative longitudes to positive if bbox [ 2 ] < 0 : bbox [ 2 ] += 360 if bbox [ 3 ] < 0 : bbox [ 3 ] += 360 # If bbox not specified, query stations across the entire globe else : bbox = [ - 90 , 90 , 0 , 360 ] long_cross_zero = 1 # Handle station query if long_cross_zero == 1 : bbox1 = bbox . copy () bbox2 = bbox . copy () bbox1 [ 3 ] = 360.0 bbox2 [ 2 ] = 0.0 stats1 , origstatsFile1 = get_station_list ( bbox = bbox1 , writeLoc = out , userstatList = station_file , name_appendix = '_a' ) stats2 , origstatsFile2 = get_station_list ( bbox = bbox2 , writeLoc = out , userstatList = station_file , name_appendix = '_b' ) stats = stats1 + stats2 origstatsFile = origstatsFile1 [: - 6 ] + '.csv' file_a = pd . read_csv ( origstatsFile1 ) file_b = pd . read_csv ( origstatsFile2 ) frames = [ file_a , file_b ] result = pd . concat ( frames , ignore_index = True ) result . to_csv ( origstatsFile , index = False ) else : if bbox [ 3 ] < bbox [ 2 ]: bbox [ 3 ] = 360.0 stats , origstatsFile = get_station_list ( bbox = bbox , writeLoc = out , userstatList = station_file ) # iterate over years years = list ( set ([ i . year for i in dateList ])) download_tropo_delays ( stats , years , gps_repo = gps_repo , writeDir = out , download = download ) # Add lat/lon info origstatsFile = pd . read_csv ( origstatsFile ) statsFile = pd . read_csv ( os . path . join ( out , ' {} gnssStationList_overbbox_withpaths.csv' . format ( gps_repo ))) statsFile = pd . merge ( left = statsFile , right = origstatsFile , how = 'left' , left_on = 'ID' , right_on = 'ID' ) statsFile . to_csv ( os . path . join ( out , ' {} gnssStationList_overbbox_withpaths.csv' . format ( gps_repo )), index = False ) del origstatsFile , statsFile # Extract delays for each station dateList = [ k . strftime ( '%Y-%m- %d ' ) for k in dateList ] get_station_data ( os . path . join ( out , ' {} gnssStationList_overbbox_withpaths.csv' . format ( gps_repo )), dateList , gps_repo = gps_repo , numCPUs = cpus , outDir = out , returnTime = returnTime ) logger . debug ( 'Completed processing' ) read_text_file ( filename ) Read a list of GNSS station names from a plain text file Source code in RAiDER/gnss/downloadGNSSDelays.py 165 166 167 168 169 170 def read_text_file ( filename ): ''' Read a list of GNSS station names from a plain text file ''' with open ( filename , 'r' ) as f : return [ line . strip () for line in f ] processDelayFiles addDateTimeToFiles ( fileList , force = False , verbose = False ) Run through a list of files and add the datetime of each file as a column Source code in RAiDER/gnss/processDelayFiles.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def addDateTimeToFiles ( fileList , force = False , verbose = False ): ''' Run through a list of files and add the datetime of each file as a column ''' print ( 'Adding Datetime to delay files' ) for f in tqdm ( fileList ): data = pd . read_csv ( f ) if 'Datetime' in data . columns and not force : if verbose : print ( 'File {} already has a \"Datetime\" column, pass' '\"force = True\" if you want to override and ' 're-process' . format ( f ) ) else : try : dt = getDateTime ( f ) data [ 'Datetime' ] = dt # drop all lines with nans data . dropna ( how = 'any' , inplace = True ) # drop all duplicate lines data . drop_duplicates ( inplace = True ) data . to_csv ( f , index = False ) except ( AttributeError , ValueError ): print ( 'File {} does not contain datetime info, skipping' . format ( f ) ) del data concatDelayFiles ( fileList , sort_list = [ 'ID' , 'Datetime' ], return_df = False , outName = None , source = 'model' , ref = None , col_name = 'ZTD' ) Read a list of .csv files containing the same columns and append them together, sorting by specified columns Source code in RAiDER/gnss/processDelayFiles.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def concatDelayFiles ( fileList , sort_list = [ 'ID' , 'Datetime' ], return_df = False , outName = None , source = 'model' , ref = None , col_name = 'ZTD' ): ''' Read a list of .csv files containing the same columns and append them together, sorting by specified columns ''' dfList = [] print ( 'Concatenating delay files' ) for f in tqdm ( fileList ): if source == 'model' : dfList . append ( pd . read_csv ( f , parse_dates = [ 'Datetime' ])) else : dfList . append ( readZTDFile ( f , col_name = col_name )) # drop lines not found in reference file if ref : dfr = pd . read_csv ( ref , parse_dates = [ 'Datetime' ]) for i in enumerate ( dfList ): dfList [ i [ 0 ]] = pass_common_obs ( dfr , i [ 1 ]) del dfr df_c = pd . concat ( dfList , ignore_index = True ) . drop_duplicates () . reset_index ( drop = True ) df_c . sort_values ( by = sort_list , inplace = True ) print ( 'Total number of rows in the concatenated file: {} ' . format ( df_c . shape [ 0 ])) print ( 'Total number of rows containing NaNs: {} ' . format ( df_c [ df_c . isna () . any ( axis = 1 )] . shape [ 0 ] ) ) if return_df or outName is None : return df_c else : # drop all lines with nans df_c . dropna ( how = 'any' , inplace = True ) # drop all duplicate lines df_c . drop_duplicates ( inplace = True ) df_c . to_csv ( outName , index = False ) create_parser () Parse command line arguments using argparse. Source code in RAiDER/gnss/processDelayFiles.py 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 def create_parser (): \"\"\"Parse command line arguments using argparse.\"\"\" p = argparse . ArgumentParser ( formatter_class = argparse . RawDescriptionHelpFormatter , description = dedent ( \"\"\" \\ Combine delay files from a weather model and GPS Zenith delays Usage examples: raiderCombine.py --raiderDir './*' --raider 'combined_raider_delays.csv' raiderCombine.py --raiderDir ERA5/ --raider ERA5_combined_delays.csv --raider_column totalDelay --gnssDir GNSS/ --gnss UNRCombined_gnss.csv --column ZTD -o Combined_delays.csv raiderCombine.py --raiderDir ERA5_2019/ --raider ERA5_combined_delays_2019.csv --raider_column totalDelay --gnssDir GNSS_2019/ --gnss UNRCombined_gnss_2019.csv --column ZTD -o Combined_delays_2019_UTTC18.csv --localtime '18:00:00 1' \"\"\" ) ) p . add_argument ( '--raider' , dest = 'raider_file' , help = dedent ( \"\"\" \\ .csv file containing RAiDER-derived Zenith Delays. Should contain columns \"ID\" and \"Datetime\" in addition to the delay column If the file does not exist, I will attempt to create it from a directory of delay files. \"\"\" ), required = True ) p . add_argument ( '--raiderDir' , '-d' , dest = 'raider_folder' , help = dedent ( \"\"\" \\ Directory containing RAiDER-derived Zenith Delay files. Files should be named with a Datetime in the name and contain the column \"ID\" as the delay column names. \"\"\" ), default = os . getcwd () ) p . add_argument ( '--gnssDir' , '-gd' , dest = 'gnss_folder' , help = dedent ( \"\"\" \\ Directory containing GNSS-derived Zenith Delay files. Files should contain the column \"ID\" as the delay column names and times should be denoted by the \"Date\" key. \"\"\" ), default = os . getcwd () ) p . add_argument ( '--gnss' , dest = 'gnss_file' , help = dedent ( \"\"\" \\ Optional .csv file containing GPS Zenith Delays. Should contain columns \"ID\", \"ZTD\", and \"Datetime\" \"\"\" ), default = None ) p . add_argument ( '--raider_column' , '-r' , dest = 'raider_column_name' , help = dedent ( \"\"\" \\ Name of the column containing RAiDER delays. Only used with the \"--gnss\" option \"\"\" ), default = 'totalDelay' ) p . add_argument ( '--column' , '-c' , dest = 'column_name' , help = dedent ( \"\"\" \\ Name of the column containing GPS Zenith delays. Only used with the \"--gnss\" option \"\"\" ), default = 'ZTD' ) p . add_argument ( '--out' , '-o' , dest = 'out_name' , help = dedent ( \"\"\" \\ Name to use for the combined delay file. Only used with the \"--gnss\" option \"\"\" ), default = 'Combined_delays.csv' ) p . add_argument ( '--localtime' , '-lt' , dest = 'local_time' , help = dedent ( \"\"\" \\ \"Optional control to pass only data at local-time (in integer hours) WRT user-defined time at 0 longitude (1st argument), and within +/- specified hour threshold (2nd argument). By default UTC is passed as is without local-time conversions. Input in 'HH H', e.g. '16 1'\" \"\"\" ), default = None ) return p getDateTime ( filename ) Parse a datetime from a RAiDER delay filename Source code in RAiDER/gnss/processDelayFiles.py 87 88 89 90 91 92 93 94 95 def getDateTime ( filename ): ''' Parse a datetime from a RAiDER delay filename ''' filename = os . path . basename ( filename ) dtr = re . compile ( r '\\d {8} T\\d {6} ' ) dt = dtr . search ( filename ) return datetime . datetime . strptime ( dt . group (), '%Y%m %d T%H%M%S' ) local_time_filter ( raiderFile , ztdFile , dfr , dfz , localTime ) Convert to local-time reference frame WRT 0 longitude Source code in RAiDER/gnss/processDelayFiles.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 def local_time_filter ( raiderFile , ztdFile , dfr , dfz , localTime ): ''' Convert to local-time reference frame WRT 0 longitude ''' localTime_hrs = int ( localTime . split ( ' ' )[ 0 ]) localTime_hrthreshold = int ( localTime . split ( ' ' )[ 1 ]) # with rotation rate and distance to 0 lon, get localtime shift WRT 00 UTC at 0 lon # *rotation rate at given point = (360deg/23.9333333333hr) = 15.041782729825965 deg/hr dfr [ 'Localtime' ] = ( dfr [ 'Lon' ] / 15.041782729825965 ) dfz [ 'Localtime' ] = ( dfz [ 'Lon' ] / 15.041782729825965 ) # estimate local-times dfr [ 'Localtime' ] = dfr . apply ( lambda r : update_time ( r , localTime_hrs ), axis = 1 ) dfz [ 'Localtime' ] = dfz . apply ( lambda r : update_time ( r , localTime_hrs ), axis = 1 ) # filter out data outside of --localtime hour threshold dfr [ 'Localtime_u' ] = dfr [ 'Localtime' ] + \\ datetime . timedelta ( hours = localTime_hrthreshold ) dfr [ 'Localtime_l' ] = dfr [ 'Localtime' ] - \\ datetime . timedelta ( hours = localTime_hrthreshold ) OG_total = dfr . shape [ 0 ] dfr = dfr [( dfr [ 'Datetime' ] >= dfr [ 'Localtime_l' ]) & ( dfr [ 'Datetime' ] <= dfr [ 'Localtime_u' ])] # only keep observation closest to Localtime print ( 'Total number of datapoints dropped in {} for not being within ' ' {} hrs of specified local-time {} : {} out of {} ' . format ( raiderFile , localTime . split ( ' ' )[ 1 ], localTime . split ( ' ' )[ 0 ], dfr . shape [ 0 ], OG_total )) dfz [ 'Localtime_u' ] = dfz [ 'Localtime' ] + \\ datetime . timedelta ( hours = localTime_hrthreshold ) dfz [ 'Localtime_l' ] = dfz [ 'Localtime' ] - \\ datetime . timedelta ( hours = localTime_hrthreshold ) OG_total = dfz . shape [ 0 ] dfz = dfz [( dfz [ 'Datetime' ] >= dfz [ 'Localtime_l' ]) & ( dfz [ 'Datetime' ] <= dfz [ 'Localtime_u' ])] # only keep observation closest to Localtime print ( 'Total number of datapoints dropped in {} for not being within ' ' {} hrs of specified local-time {} : {} out of {} ' . format ( ztdFile , localTime . split ( ' ' )[ 1 ], localTime . split ( ' ' )[ 0 ], dfz . shape [ 0 ], OG_total )) # drop all lines with nans dfr . dropna ( how = 'any' , inplace = True ) dfz . dropna ( how = 'any' , inplace = True ) # drop all duplicate lines dfr . drop_duplicates ( inplace = True ) dfz . drop_duplicates ( inplace = True ) # drop and rename columns dfr . drop ( columns = [ 'Localtime_l' , 'Localtime_u' ], inplace = True ) dfz . drop ( columns = [ 'Localtime_l' , 'Localtime_u' ], inplace = True ) return dfr , dfz mergeDelayFiles ( raiderFile , ztdFile , col_name = 'ZTD' , raider_delay = 'totalDelay' , outName = None , localTime = None ) Merge a combined RAiDER delays file with a GPS ZTD delay file Source code in RAiDER/gnss/processDelayFiles.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 def mergeDelayFiles ( raiderFile , ztdFile , col_name = 'ZTD' , raider_delay = 'totalDelay' , outName = None , localTime = None ): ''' Merge a combined RAiDER delays file with a GPS ZTD delay file ''' print ( 'Merging delay files {} and {} ' . format ( raiderFile , ztdFile )) dfr = pd . read_csv ( raiderFile , parse_dates = [ 'Datetime' ]) # drop extra columns expected_data_columns = [ 'ID' , 'Lat' , 'Lon' , 'Hgt_m' , 'Datetime' , 'wetDelay' , 'hydroDelay' , raider_delay ] dfr = dfr . drop ( columns = [ col for col in dfr if col not in expected_data_columns ]) dfz = pd . read_csv ( ztdFile , parse_dates = [ 'Datetime' ]) # drop extra columns expected_data_columns = [ 'ID' , 'Date' , 'wet_delay' , 'hydrostatic_delay' , 'times' , 'sigZTD' , 'Lat' , 'Lon' , 'Hgt_m' , 'Datetime' , col_name ] dfz = dfz . drop ( columns = [ col for col in dfz if col not in expected_data_columns ]) # only pass common locations and times dfz = pass_common_obs ( dfr , dfz ) dfr = pass_common_obs ( dfz , dfr ) # If specified, convert to local-time reference frame WRT 0 longitude common_keys = [ 'Datetime' , 'ID' ] if localTime is not None : dfr , dfz = local_time_filter ( raiderFile , ztdFile , dfr , dfz , localTime ) common_keys . append ( 'Localtime' ) # only pass common locations and times dfz = pass_common_obs ( dfr , dfz , localtime = 'Localtime' ) dfr = pass_common_obs ( dfz , dfr , localtime = 'Localtime' ) # drop all lines with nans dfr . dropna ( how = 'any' , inplace = True ) dfz . dropna ( how = 'any' , inplace = True ) # drop all duplicate lines dfr . drop_duplicates ( inplace = True ) dfz . drop_duplicates ( inplace = True ) print ( 'Beginning merge' ) dfc = dfr . merge ( dfz [ common_keys + [ 'ZTD' , 'sigZTD' ]], how = 'left' , left_on = common_keys , right_on = common_keys , sort = True ) # only keep observation closest to Localtime if 'Localtime' in dfc . keys (): dfc [ 'Localtimediff' ] = abs (( dfc [ 'Datetime' ] - dfc [ 'Localtime' ]) . dt . total_seconds () / 3600 ) dfc = dfc . loc [ dfc . groupby ([ 'ID' , 'Localtime' ]) . Localtimediff . idxmin () ] . reset_index ( drop = True ) dfc . drop ( columns = [ 'Localtimediff' ], inplace = True ) # estimate residual dfc [ 'ZTD_minus_RAiDER' ] = dfc [ 'ZTD' ] - dfc [ raider_delay ] print ( 'Total number of rows in the concatenated file: ' ' {} ' . format ( dfc . shape [ 0 ])) print ( 'Total number of rows containing NaNs: {} ' . format ( dfc [ dfc . isna () . any ( axis = 1 )] . shape [ 0 ] ) ) print ( 'Merge finished' ) if outName is None : return dfc else : # drop all lines with nans dfc . dropna ( how = 'any' , inplace = True ) # drop all duplicate lines dfc . drop_duplicates ( inplace = True ) dfc . to_csv ( outName , index = False ) parseCMD () Parse command-line arguments and pass to delay.py Source code in RAiDER/gnss/processDelayFiles.py 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 def parseCMD (): \"\"\" Parse command-line arguments and pass to delay.py \"\"\" p = create_parser () args = p . parse_args () if not os . path . exists ( args . raider_file ): combineDelayFiles ( args . raider_file , loc = args . raider_folder ) if not os . path . exists ( args . gnss_file ): combineDelayFiles ( args . gnss_file , loc = args . gnss_folder , source = 'GNSS' , ref = args . raider_file , col_name = args . column_name ) if args . gnss_file is not None : mergeDelayFiles ( args . raider_file , args . gnss_file , col_name = args . column_name , raider_delay = args . raider_column_name , outName = args . out_name , localTime = args . local_time ) pass_common_obs ( reference , target , localtime = None ) Pass only observations in target spatiotemporally common to reference Source code in RAiDER/gnss/processDelayFiles.py 123 124 125 126 127 128 129 130 131 132 133 def pass_common_obs ( reference , target , localtime = None ): '''Pass only observations in target spatiotemporally common to reference''' if localtime : return target [ target [ 'Datetime' ] . dt . date . isin ( reference [ 'Datetime' ] . dt . date ) & target [ 'ID' ] . isin ( reference [ 'ID' ]) & target [ localtime ] . isin ( reference [ localtime ])] else : return target [ target [ 'Datetime' ] . dt . date . isin ( reference [ 'Datetime' ] . dt . date ) & target [ 'ID' ] . isin ( reference [ 'ID' ])] readZTDFile ( filename , col_name = 'ZTD' ) Read and parse a GPS zenith delay file Source code in RAiDER/gnss/processDelayFiles.py 328 329 330 331 332 333 334 335 336 337 338 339 340 def readZTDFile ( filename , col_name = 'ZTD' ): ''' Read and parse a GPS zenith delay file ''' try : data = pd . read_csv ( filename , parse_dates = [ 'Date' ]) times = data [ 'times' ] . apply ( lambda x : datetime . timedelta ( seconds = x )) data [ 'Datetime' ] = data [ 'Date' ] + times except ( KeyError , ValueError ): data = pd . read_csv ( filename , parse_dates = [ 'Datetime' ]) data . rename ( columns = { col_name : 'ZTD' }, inplace = True ) return data update_time ( row , localTime_hrs ) Update with local origin time Source code in RAiDER/gnss/processDelayFiles.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def update_time ( row , localTime_hrs ): '''Update with local origin time''' localTime_estimate = row [ 'Datetime' ] . replace ( hour = localTime_hrs , minute = 0 , second = 0 ) # determine if you need to shift days time_shift = datetime . timedelta ( days = 0 ) # round to nearest hour days_diff = ( row [ 'Datetime' ] - datetime . timedelta ( seconds = math . floor ( row [ 'Localtime' ]) * 3600 )) . day - \\ localTime_estimate . day # if lon <0, check if you need to add day if row [ 'Lon' ] < 0 : # add day if days_diff != 0 : time_shift = datetime . timedelta ( days = 1 ) # if lon >0, check if you need to subtract day if row [ 'Lon' ] > 0 : # subtract day if days_diff != 0 : time_shift = - datetime . timedelta ( days = 1 ) return localTime_estimate + datetime . timedelta ( seconds = row [ 'Localtime' ] * 3600 ) + time_shift interpolate Fast linear interpolator over a regular grid __doc__ = 'Fast linear interpolator over a regular grid' str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __file__ = '/home/jhkennedy/Documents/Code/contributing/RAiDER/tools/RAiDER/interpolate.cpython-310-x86_64-linux-gnu.so' str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __name__ = 'RAiDER.interpolate' str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __package__ = 'RAiDER' str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. interpolate () builtin interpolate(points: List[numpy.ndarray[numpy.float64]], values: numpy.ndarray[numpy.float64], interp_points: numpy.ndarray[numpy.float64], fill_value: Optional[float] = None, assume_sorted: bool = False, max_threads: int = 8) -> numpy.ndarray[numpy.float64] Linear interpolator in any dimension. Arguments are similar to scipy.interpolate.RegularGridInterpolator :param points: Tuple of N axis coordinates specifying the grid. :param values: Nd array containing the grid point values. :param interp_points: List of points to interpolate, should have dimension (x, N). If this list is guaranteed to be sorted make sure to use the assume_sorted option. :param fill_value: The value to return for interpolation points outside of the grid range. :param assume_sorted: Enable optimization when the list of interpolation points is sorted. :param max_threads: Limit the number of threads to a certain amount. Note: The number of threads will always be one of {1, 2, 4, 8} interpolate_along_axis () builtin interpolate_along_axis(points: numpy.ndarray[numpy.float64], values: numpy.ndarray[numpy.float64], interp_points: numpy.ndarray[numpy.float64], axis: int = -1, fill_value: Optional[float] = None, assume_sorted: bool = False, max_threads: int = 8) -> numpy.ndarray[numpy.float64] 1D linear interpolator along a specific axis. :param points: N-dimensional x coordinates. Axis specified by 'axis' must contain at least 2 points. :param values: N-dimensional y values. Must have the same shape as 'points'. :param interp_points: N-dimensional x coordinates to interpolate at. The shape may only differ from that of 'points' at the axis specified by 'axis'. For example if 'points' has shape (1, 2, 3) and 'axis' is 2, then and shape like (1, 2, X) is valid. :param axis: The axis to interpolate along. :param fill_value: The value to return for interpolation points outside of the grid range. :param assume_sorted: Enable optimization when the list of interpolation points is sorted along the axis of interpolation. :param max_threads: Limit the number of threads to a certain amount. interpolator RegularGridInterpolator Bases: object Provides a wrapper around RAiDER.interpolate.interpolate with a similar interface to scipy.interpolate.RegularGridInterpolator. Source code in RAiDER/interpolator.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 class RegularGridInterpolator ( object ): \"\"\" Provides a wrapper around RAiDER.interpolate.interpolate with a similar interface to scipy.interpolate.RegularGridInterpolator. \"\"\" def __init__ ( self , grid , values , fill_value = None , assume_sorted = False , max_threads = 8 ): self . grid = grid self . values = values self . fill_value = fill_value self . assume_sorted = assume_sorted self . max_threads = max_threads def __call__ ( self , points ): if isinstance ( points , tuple ): shape = points [ 0 ] . shape for arr in points : assert arr . shape == shape , \"All dimensions must contain the same number of points!\" interp_points = np . stack ( points , axis =- 1 ) in_shape = interp_points . shape elif points . ndim > 2 : in_shape = points . shape interp_points = points . reshape (( np . prod ( points . shape [: - 1 ]),) + ( points . shape [ - 1 ],)) else : interp_points = points in_shape = interp_points . shape out = interpolate ( self . grid , self . values , interp_points , fill_value = self . fill_value , assume_sorted = self . assume_sorted , max_threads = self . max_threads ) return out . reshape ( in_shape [: - 1 ]) interpV ( y , old_x , new_x , left = None , right = None , period = None ) Rearrange np.interp's arguments Source code in RAiDER/interpolator.py 80 81 82 83 84 def interpV ( y , old_x , new_x , left = None , right = None , period = None ): ''' Rearrange np.interp's arguments ''' return np . interp ( new_x , old_x , y , left = left , right = right , period = period ) interpVector ( vec , Nx ) Interpolate data from a single vector containing the original x, the original y, and the new x, in that order. Nx tells the number of original x-points. Source code in RAiDER/interpolator.py 87 88 89 90 91 92 93 94 95 96 97 def interpVector ( vec , Nx ): ''' Interpolate data from a single vector containing the original x, the original y, and the new x, in that order. Nx tells the number of original x-points. ''' x = vec [: Nx ] y = vec [ Nx : 2 * Nx ] xnew = vec [ 2 * Nx :] f = interp1d ( x , y , bounds_error = False , copy = False , assume_sorted = True ) return f ( xnew ) interp_along_axis ( oldCoord , newCoord , data , axis = 2 , pad = False ) DEPRECATED: Use RAiDER.interpolate.interpolate_along_axis instead (it is much faster). This function now primarily exists to verify the behavior of the new one. Interpolate an array of 3-D data along one axis. This function assumes that the x-coordinate increases monotonically. Source code in RAiDER/interpolator.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def interp_along_axis ( oldCoord , newCoord , data , axis = 2 , pad = False ): ''' DEPRECATED: Use RAiDER.interpolate.interpolate_along_axis instead (it is much faster). This function now primarily exists to verify the behavior of the new one. Interpolate an array of 3-D data along one axis. This function assumes that the x-coordinate increases monotonically. ''' if oldCoord . ndim > 1 : stackedData = np . concatenate ([ oldCoord , data , newCoord ], axis = axis ) out = np . apply_along_axis ( interpVector , axis = axis , arr = stackedData , Nx = oldCoord . shape [ axis ]) else : out = np . apply_along_axis ( interpV , axis = axis , arr = data , old_x = oldCoord , new_x = newCoord , left = np . nan , right = np . nan ) return out interpolateDEM ( demRaster , extent , outLL , method = 'linear' ) Interpolate a DEM raster to a set of lat/lon query points Source code in RAiDER/interpolator.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def interpolateDEM ( demRaster , extent , outLL , method = 'linear' ): ''' Interpolate a DEM raster to a set of lat/lon query points ''' minlat , maxlat , minlon , maxlon = extent nPixLat = demRaster . shape [ 0 ] nPixLon = demRaster . shape [ 1 ] xlats = np . linspace ( minlat , maxlat , nPixLat ) xlons = np . linspace ( minlon , maxlon , nPixLon ) interpolator = rgi ( points = ( xlats , xlons ), values = demRaster , method = method , bounds_error = False ) outInterp = interpolator ( outLL ) return outInterp llreader AOI Bases: object This instantiates a generic AOI class object Source code in RAiDER/llreader.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 class AOI ( object ): ''' This instantiates a generic AOI class object ''' def __init__ ( self ): self . _bounding_box = None self . _proj = CRS . from_epsg ( 4326 ) def type ( self ): return self . _type def bounds ( self ): return self . _bounding_box def projection ( self ): return self . _proj def add_buffer ( self , buffer ): ''' Check whether an extra lat/lon buffer is needed for raytracing ''' # if raytracing, add a 1-degree buffer all around try : ll_bounds = self . _bounding_box . copy () except AttributeError : ll_bounds = list ( self . _bounding_box ) ll_bounds [ 0 ] = np . max ([ ll_bounds [ 0 ] - buffer , - 90 ]) ll_bounds [ 1 ] = np . min ([ ll_bounds [ 1 ] + buffer , 90 ]) ll_bounds [ 2 ] = np . max ([ ll_bounds [ 2 ] - buffer , - 180 ]) ll_bounds [ 3 ] = np . min ([ ll_bounds [ 3 ] + buffer , 180 ]) return ll_bounds add_buffer ( buffer ) Check whether an extra lat/lon buffer is needed for raytracing Source code in RAiDER/llreader.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def add_buffer ( self , buffer ): ''' Check whether an extra lat/lon buffer is needed for raytracing ''' # if raytracing, add a 1-degree buffer all around try : ll_bounds = self . _bounding_box . copy () except AttributeError : ll_bounds = list ( self . _bounding_box ) ll_bounds [ 0 ] = np . max ([ ll_bounds [ 0 ] - buffer , - 90 ]) ll_bounds [ 1 ] = np . min ([ ll_bounds [ 1 ] + buffer , 90 ]) ll_bounds [ 2 ] = np . max ([ ll_bounds [ 2 ] - buffer , - 180 ]) ll_bounds [ 3 ] = np . min ([ ll_bounds [ 3 ] + buffer , 180 ]) return ll_bounds BoundingBox Bases: AOI Parse a bounding box AOI Source code in RAiDER/llreader.py 132 133 134 135 136 137 class BoundingBox ( AOI ): '''Parse a bounding box AOI''' def __init__ ( self , bbox ): AOI . __init__ ( self ) self . _bounding_box = bbox self . _type = 'bounding_box' GeocodedFile Bases: AOI Parse a Geocoded file for coordinates Source code in RAiDER/llreader.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 class GeocodedFile ( AOI ): '''Parse a Geocoded file for coordinates''' def __init__ ( self , filename , is_dem = False ): super () . __init__ () self . _filename = filename self . p = rio_profile ( filename ) self . _bounding_box = rio_extents ( self . p ) self . _is_dem = is_dem _ , self . _proj , self . _gt = rio_stats ( filename ) self . _type = 'geocoded_file' def readLL ( self ): # ll_bounds are SNWE S , N , W , E = self . _bounding_box w , h = self . p [ 'width' ], self . p [ 'height' ] px = ( E - W ) / w py = ( N - S ) / h x = np . array ([ W + ( t * px ) for t in range ( w )]) y = np . array ([ S + ( t * py ) for t in range ( h )]) X , Y = np . meshgrid ( x , y ) return Y , X # lats, lons def readZ ( self ): demFile = self . _filename if self . _is_dem else 'GLO30_fullres_dem.tif' bbox = self . _bounding_box zvals , metadata = download_dem ( bbox , writeDEM = True , outName = demFile ) z_bounds = get_bbox ( metadata ) z_out = interpolateDEM ( zvals , z_bounds , self . readLL (), method = 'nearest' ) return z_out Geocube Bases: AOI Parse a georeferenced data cube Source code in RAiDER/llreader.py 173 174 175 176 177 178 179 180 181 182 class Geocube ( AOI ): '''Parse a georeferenced data cube''' def __init__ ( self ): super () . __init__ () self . _type = 'geocube' raise NotImplementedError def readLL ( self ): return None , None StationFile Bases: AOI Use a .csv file containing at least Lat, Lon, and optionally Hgt_m columns Source code in RAiDER/llreader.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 class StationFile ( AOI ): '''Use a .csv file containing at least Lat, Lon, and optionally Hgt_m columns''' def __init__ ( self , station_file ): super () . __init__ () self . _filename = station_file self . _bounding_box = bounds_from_csv ( station_file ) self . _type = 'station_file' def readLL ( self ): df = pd . read_csv ( self . _filename ) . drop_duplicates ( subset = [ \"Lat\" , \"Lon\" ]) return df [ 'Lat' ] . values , df [ 'Lon' ] . values def readZ ( self ): df = pd . read_csv ( self . _filename ) if 'Hgt_m' in df . columns : return df [ 'Hgt_m' ] . values else : zvals , metadata = download_dem ( self . _bounding_box ) z_bounds = get_bbox ( metadata ) z_out = interpolateDEM ( zvals , z_bounds , self . readLL (), method = 'nearest' ) df [ 'Hgt_m' ] = z_out df . to_csv ( self . _filename , index = False ) self . __init__ ( self . _filename ) return z_out bounds_from_csv ( station_file ) station_file should be a comma-delimited file with at least \"Lat\" and \"Lon\" columns, which should be EPSG: 4326 projection (i.e WGS84) Source code in RAiDER/llreader.py 207 208 209 210 211 212 213 214 215 216 def bounds_from_csv ( station_file ): ''' station_file should be a comma-delimited file with at least \"Lat\" and \"Lon\" columns, which should be EPSG: 4326 projection (i.e WGS84) ''' stats = pd . read_csv ( station_file ) . drop_duplicates ( subset = [ \"Lat\" , \"Lon\" ]) if 'Hgt_m' in stats . columns : use_csv_heights = True snwe = [ stats [ 'Lat' ] . min (), stats [ 'Lat' ] . max (), stats [ 'Lon' ] . min (), stats [ 'Lon' ] . max ()] return snwe bounds_from_latlon_rasters ( latfile , lonfile ) Parse lat/lon/height inputs and return the appropriate outputs Source code in RAiDER/llreader.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 def bounds_from_latlon_rasters ( latfile , lonfile ): ''' Parse lat/lon/height inputs and return the appropriate outputs ''' latinfo = get_file_and_band ( latfile ) loninfo = get_file_and_band ( lonfile ) lat_stats , lat_proj , _ = rio_stats ( latinfo [ 0 ], band = latinfo [ 1 ]) lon_stats , lon_proj , _ = rio_stats ( loninfo [ 0 ], band = loninfo [ 1 ]) if lat_proj != lon_proj : raise ValueError ( 'Projection information for Latitude and Longitude files does not match' ) # TODO - handle dateline crossing here snwe = ( lat_stats . min , lat_stats . max , lon_stats . min , lon_stats . max ) fname = os . path . basename ( latfile ) . split ( '.' )[ 0 ] return lat_proj , snwe , fname logger Global logging configuration CustomFormatter Bases: UnixColorFormatter Adds levelname prefixes to the message on warning or above. Source code in RAiDER/logger.py 49 50 51 52 53 54 55 56 class CustomFormatter ( UnixColorFormatter ): \"\"\"Adds levelname prefixes to the message on warning or above.\"\"\" def formatMessage ( self , record ): message = super () . formatMessage ( record ) if record . levelno >= logging . WARNING : message = \": \" . join (( record . levelname , message )) return message losreader Conventional Bases: LOS Special value indicating that the zenith delay will be projected using the standard cos(inc) scaling. Source code in RAiDER/losreader.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 class Conventional ( LOS ): \"\"\" Special value indicating that the zenith delay will be projected using the standard cos(inc) scaling. \"\"\" def __init__ ( self , filename = None , los_convention = 'isce' , time = None , pad = 600 ): super () . __init__ () self . _file = filename self . _time = time self . _pad = pad self . _is_projected = True self . _convention = los_convention if self . _convention . lower () != 'isce' : raise NotImplementedError () def __call__ ( self , delays ): ''' Read the LOS file and convert it to look vectors ''' if self . _lats is None : raise ValueError ( 'Target points not set' ) if self . _file is None : raise ValueError ( 'LOS file not set' ) try : # if an ISCE-style los file is passed open it with GDAL LOS_enu = inc_hd_to_enu ( * rio_open ( self . _file )) except OSError : # Otherwise, treat it as an orbit / statevector file svs = np . stack ( get_sv ( self . _file , self . _time , self . _pad ), axis =- 1 ) LOS_enu = state_to_los ( svs , [ self . _lats , self . _lons , self . _heights ], out = \"lookangle\" ) if delays . shape == LOS_enu . shape : return delays / LOS_enu else : return delays / LOS_enu [ ... , - 1 ] __call__ ( delays ) Read the LOS file and convert it to look vectors Source code in RAiDER/losreader.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def __call__ ( self , delays ): ''' Read the LOS file and convert it to look vectors ''' if self . _lats is None : raise ValueError ( 'Target points not set' ) if self . _file is None : raise ValueError ( 'LOS file not set' ) try : # if an ISCE-style los file is passed open it with GDAL LOS_enu = inc_hd_to_enu ( * rio_open ( self . _file )) except OSError : # Otherwise, treat it as an orbit / statevector file svs = np . stack ( get_sv ( self . _file , self . _time , self . _pad ), axis =- 1 ) LOS_enu = state_to_los ( svs , [ self . _lats , self . _lons , self . _heights ], out = \"lookangle\" ) if delays . shape == LOS_enu . shape : return delays / LOS_enu else : return delays / LOS_enu [ ... , - 1 ] LOS Bases: ABC LOS Class definition for handling look vectors Source code in RAiDER/losreader.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 class LOS ( ABC ): ''' LOS Class definition for handling look vectors ''' def __init__ ( self ): self . _lats , self . _lons , self . _heights = None , None , None self . _look_vecs = None self . _ray_trace = False self . _is_zenith = False self . _is_projected = False def setPoints ( self , lats , lons = None , heights = None ): '''Set the pixel locations''' if ( lats is None ) and ( self . _lats is None ): raise RuntimeError ( \"You haven't given any point locations yet\" ) # Will overwrite points by default if lons is None : llh = lats # assume points are [lats lons heights] self . _lats = llh [ ... , 0 ] self . _lons = llh [ ... , 1 ] self . _heights = llh [ ... , 2 ] elif heights is None : self . _lats = lats self . _lons = lons self . _heights = np . zeros (( len ( lats ), 1 )) else : self . _lats = lats self . _lons = lons self . _heights = heights def setTime ( self , dt ): self . _time = dt def is_Zenith ( self ): return self . _is_zenith def is_Projected ( self ): return self . _is_projected def ray_trace ( self ): return self . _ray_trace setPoints ( lats , lons = None , heights = None ) Set the pixel locations Source code in RAiDER/losreader.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def setPoints ( self , lats , lons = None , heights = None ): '''Set the pixel locations''' if ( lats is None ) and ( self . _lats is None ): raise RuntimeError ( \"You haven't given any point locations yet\" ) # Will overwrite points by default if lons is None : llh = lats # assume points are [lats lons heights] self . _lats = llh [ ... , 0 ] self . _lons = llh [ ... , 1 ] self . _heights = llh [ ... , 2 ] elif heights is None : self . _lats = lats self . _lons = lons self . _heights = np . zeros (( len ( lats ), 1 )) else : self . _lats = lats self . _lons = lons self . _heights = heights Raytracing Bases: LOS Special value indicating that full raytracing will be used to calculate slant delays. Get unit look vectors pointing from the ground (target) pixels to the sensor, or to Zenith. Can be accomplished using an ISCE-style 2-band LOS file or a file containing orbital statevectors. NOTE : These line-of-sight vectors will NOT match ordinary LOS vectors for InSAR because they are in an ECEF reference frame instead of a local ENU. This is done because the construction of rays is done in ECEF rather than the local ENU. python datetime - user-requested query time. Must be compatible with the orbit file passed. Only required for a statevector file. int - integer number of seconds to pad around the user-specified time; default 3 hours Only required for a statevector file. ndarray - an x 3 array of unit look vectors, defined in an Earth-centered, earth-fixed reference frame (ECEF). Convention is vectors point from the target pixel to the sensor. ndarray - array of of the distnce from the surface to the top of the troposphere (denoted by zref) Example: from RAiDER.losreader import Raytracing import numpy as np TODO Source code in RAiDER/losreader.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 class Raytracing ( LOS ): \"\"\" Special value indicating that full raytracing will be used to calculate slant delays. Get unit look vectors pointing from the ground (target) pixels to the sensor, or to Zenith. Can be accomplished using an ISCE-style 2-band LOS file or a file containing orbital statevectors. *NOTE*: These line-of-sight vectors will NOT match ordinary LOS vectors for InSAR because they are in an ECEF reference frame instead of a local ENU. This is done because the construction of rays is done in ECEF rather than the local ENU. Args: ---------- time: python datetime - user-requested query time. Must be compatible with the orbit file passed. Only required for a statevector file. pad: int - integer number of seconds to pad around the user-specified time; default 3 hours Only required for a statevector file. Returns: ------- ndarray - an <in_shape> x 3 array of unit look vectors, defined in an Earth-centered, earth-fixed reference frame (ECEF). Convention is vectors point from the target pixel to the sensor. ndarray - array of <in_shape> of the distnce from the surface to the top of the troposphere (denoted by zref) Example: -------- >>> from RAiDER.losreader import Raytracing >>> import numpy as np >>> TODO \"\"\" def __init__ ( self , filename = None , los_convention = 'isce' , time = None , pad = 600 ): '''read in and parse a statevector file''' super () . __init__ () self . _ray_trace = True self . _file = filename self . _time = time self . _pad = pad self . _convention = los_convention if self . _convention . lower () != 'isce' : raise NotImplementedError () def getLookVectors ( self , time , pad = 3 * 60 ): ''' Calculate look vectors for raytracing ''' if self . _lats is None : raise ValueError ( 'Target points not set' ) if self . _file is None : raise ValueError ( 'LOS file not set' ) try : # if an ISCE-style los file is provided, use GDAL LOS_enu = inc_hd_to_enu ( * rio_open ( self . _file )) self . _look_vecs = enu2ecef ( LOS_enu [ ... , 0 ], LOS_enu [ ... , 1 ], LOS_enu [ ... , 2 ], self . _lats , self . _lons , self . _heights ) self . _xyz = np . stack ( lla2ecef ([ self . _lats , self . _lons , self . _heights ]), axis =- 1 ) except OSError : # Otherwise treat it as a state vector file svs = np . stack ( get_sv ( self . _file , self . _time , self . _pad ), axis =- 1 ) self . _look_vecs , self . _xyz = state_to_los ( svs , [ self . _lats , self . _lons , self . _heights ], out = \"ecef\" ) def getIntersectionWithHeight ( self , height ): \"\"\" This function computes the intersection point of a ray at a height level \"\"\" # We just leverage the same code as finding top of atmosphere here return getTopOfAtmosphere ( self . _xyz , self . _look_vecs , height ) def getIntersectionWithLevels ( self , levels ): \"\"\" This function returns the points at which rays intersect the given height levels. This way we have same number of points in each ray and only at level transitions. For targets that are above a given height level, the ray points are set to nan to indicate that it does not contribute to the integration of rays. Output: rays: (self._lats.shape, len(levels), 3) \"\"\" rays = np . zeros ( list ( self . _lats . shape ) + [ len ( levels ), 3 ]) # This can be further vectorized, if there is enough memory for ind , z in enumerate ( levels ): rays [ ... , ind , :] = self . getIntersectionWithHeight ( z ) # Set pixels above level to nan value = rays [ ... , ind , :] value [ self . _heights > z , :] = np . nan return rays def calculateDelays ( self , delays ): ''' Here \"delays\" is point-wise delays (i.e. refractivities), not integrated ZTD/STD. ''' # Create rays (Use getIntersectionWithLevels above) # Interpolate delays to rays # Integrate along rays # Return STD raise NotImplementedError __init__ ( filename = None , los_convention = 'isce' , time = None , pad = 600 ) read in and parse a statevector file Source code in RAiDER/losreader.py 179 180 181 182 183 184 185 186 187 188 def __init__ ( self , filename = None , los_convention = 'isce' , time = None , pad = 600 ): '''read in and parse a statevector file''' super () . __init__ () self . _ray_trace = True self . _file = filename self . _time = time self . _pad = pad self . _convention = los_convention if self . _convention . lower () != 'isce' : raise NotImplementedError () calculateDelays ( delays ) Here \"delays\" is point-wise delays (i.e. refractivities), not integrated ZTD/STD. Source code in RAiDER/losreader.py 265 266 267 268 269 270 271 272 273 274 def calculateDelays ( self , delays ): ''' Here \"delays\" is point-wise delays (i.e. refractivities), not integrated ZTD/STD. ''' # Create rays (Use getIntersectionWithLevels above) # Interpolate delays to rays # Integrate along rays # Return STD raise NotImplementedError getIntersectionWithHeight ( height ) This function computes the intersection point of a ray at a height level Source code in RAiDER/losreader.py 230 231 232 233 234 235 236 def getIntersectionWithHeight ( self , height ): \"\"\" This function computes the intersection point of a ray at a height level \"\"\" # We just leverage the same code as finding top of atmosphere here return getTopOfAtmosphere ( self . _xyz , self . _look_vecs , height ) getIntersectionWithLevels ( levels ) This function returns the points at which rays intersect the given height levels. This way we have same number of points in each ray and only at level transitions. For targets that are above a given height level, the ray points are set to nan to indicate that it does not contribute to the integration of rays. Output rays: (self._lats.shape, len(levels), 3) Source code in RAiDER/losreader.py 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 def getIntersectionWithLevels ( self , levels ): \"\"\" This function returns the points at which rays intersect the given height levels. This way we have same number of points in each ray and only at level transitions. For targets that are above a given height level, the ray points are set to nan to indicate that it does not contribute to the integration of rays. Output: rays: (self._lats.shape, len(levels), 3) \"\"\" rays = np . zeros ( list ( self . _lats . shape ) + [ len ( levels ), 3 ]) # This can be further vectorized, if there is enough memory for ind , z in enumerate ( levels ): rays [ ... , ind , :] = self . getIntersectionWithHeight ( z ) # Set pixels above level to nan value = rays [ ... , ind , :] value [ self . _heights > z , :] = np . nan return rays getLookVectors ( time , pad = 3 * 60 ) Calculate look vectors for raytracing Source code in RAiDER/losreader.py 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 def getLookVectors ( self , time , pad = 3 * 60 ): ''' Calculate look vectors for raytracing ''' if self . _lats is None : raise ValueError ( 'Target points not set' ) if self . _file is None : raise ValueError ( 'LOS file not set' ) try : # if an ISCE-style los file is provided, use GDAL LOS_enu = inc_hd_to_enu ( * rio_open ( self . _file )) self . _look_vecs = enu2ecef ( LOS_enu [ ... , 0 ], LOS_enu [ ... , 1 ], LOS_enu [ ... , 2 ], self . _lats , self . _lons , self . _heights ) self . _xyz = np . stack ( lla2ecef ([ self . _lats , self . _lons , self . _heights ]), axis =- 1 ) except OSError : # Otherwise treat it as a state vector file svs = np . stack ( get_sv ( self . _file , self . _time , self . _pad ), axis =- 1 ) self . _look_vecs , self . _xyz = state_to_los ( svs , [ self . _lats , self . _lons , self . _heights ], out = \"ecef\" ) Zenith Bases: LOS Class definition for a \"Zenith\" object. Source code in RAiDER/losreader.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 class Zenith ( LOS ): \"\"\" Class definition for a \"Zenith\" object. \"\"\" def __init__ ( self ): super () . __init__ () self . _is_zenith = True def setLookVectors ( self ): '''Set point locations and calculate Zenith look vectors''' if self . _lats is None : raise ValueError ( 'Target points not set' ) if self . _look_vecs is None : self . _look_vecs = getZenithLookVecs ( self . _lats , self . _lons , self . _heights ) def __call__ ( self , delays ): '''Placeholder method for consistency with the other classes''' return delays __call__ ( delays ) Placeholder method for consistency with the other classes Source code in RAiDER/losreader.py 91 92 93 def __call__ ( self , delays ): '''Placeholder method for consistency with the other classes''' return delays setLookVectors () Set point locations and calculate Zenith look vectors Source code in RAiDER/losreader.py 83 84 85 86 87 88 def setLookVectors ( self ): '''Set point locations and calculate Zenith look vectors''' if self . _lats is None : raise ValueError ( 'Target points not set' ) if self . _look_vecs is None : self . _look_vecs = getZenithLookVecs ( self . _lats , self . _lons , self . _heights ) cut_times ( times , ref_time , pad = 3600 * 3 ) Slice the orbit file around the reference aquisition time. This is done by default using a three-hour window, which for Sentinel-1 empirically works out to be roughly the largest window allowed by the orbit time. times: Nt x 1 ndarray - Vector of orbit times as datetime ref_time: datetime - Reference time pad: int - integer time in seconds to use as padding idx: Nt x 1 logical ndarray - a mask of times within the padded request time. Source code in RAiDER/losreader.py 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 def cut_times ( times , ref_time , pad = 3600 * 3 ): \"\"\" Slice the orbit file around the reference aquisition time. This is done by default using a three-hour window, which for Sentinel-1 empirically works out to be roughly the largest window allowed by the orbit time. Args: ---------- times: Nt x 1 ndarray - Vector of orbit times as datetime ref_time: datetime - Reference time pad: int - integer time in seconds to use as padding Returns: ------- idx: Nt x 1 logical ndarray - a mask of times within the padded request time. \"\"\" diff = np . array ( [( x - ref_time ) . total_seconds () for x in times ] ) return np . abs ( diff ) < pad getTopOfAtmosphere ( xyz , look_vecs , toaheight , factor = None ) Get ray intersection at given height. We use simple Newton-Raphson for this computation. This cannot be done exactly since closed form expression from xyz to llh is super compliated. If a factor (cos of inc angle) is provided - iterations are lot faster. If factor is not provided solutions converges to - 0.01 mm at heights near zero in 10 iterations - 10 cm at heights above 40km in 10 iterations If factor is know, we converge in 3 iterations to less than a micron. Source code in RAiDER/losreader.py 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 def getTopOfAtmosphere ( xyz , look_vecs , toaheight , factor = None ): \"\"\" Get ray intersection at given height. We use simple Newton-Raphson for this computation. This cannot be done exactly since closed form expression from xyz to llh is super compliated. If a factor (cos of inc angle) is provided - iterations are lot faster. If factor is not provided solutions converges to - 0.01 mm at heights near zero in 10 iterations - 10 cm at heights above 40km in 10 iterations If factor is know, we converge in 3 iterations to less than a micron. \"\"\" if factor is not None : maxIter = 3 else : maxIter = 10 factor = 1. # Guess top point pos = xyz + toaheight * look_vecs for niter in range ( 10 ): pos_llh = ecef2lla ( pos [ ... , 0 ], pos [ ... , 1 ], pos [ ... , 2 ]) pos = pos + look_vecs * (( toaheight - pos_llh [ 2 ]) / factor )[ ... , None ] # This is for debugging the approach # print(\"Stats for TOA computation: \", toaheight, # toaheight - np.nanmin(pos_llh[2]), # toaheight - np.nanmax(pos_llh[2]), # ) # The converged solution represents top of the rays return pos getZenithLookVecs ( lats , lons , heights ) Returns look vectors when Zenith is used. Parameters: Name Type Description Default lats/lons/heights ndarray Numpy arrays containing WGS-84 target locations required Returns: Name Type Description zenLookVecs ndarray (in_shape) x 3 unit look vectors in an ECEF reference frame Source code in RAiDER/losreader.py 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 def getZenithLookVecs ( lats , lons , heights ): ''' Returns look vectors when Zenith is used. Args: lats/lons/heights (ndarray): - Numpy arrays containing WGS-84 target locations Returns: zenLookVecs (ndarray): - (in_shape) x 3 unit look vectors in an ECEF reference frame ''' x = np . cos ( np . radians ( lats )) * np . cos ( np . radians ( lons )) y = np . cos ( np . radians ( lats )) * np . sin ( np . radians ( lons )) z = np . sin ( np . radians ( lats )) return np . stack ([ x , y , z ], axis =- 1 ) get_radar_pos ( llh , orb , out = 'lookangle' ) Calculate the coordinate of the sensor in ECEF at the time corresponding to ***. orb: isce3.core.Orbit - Nt x 7 matrix of statevectors: [t x y z vx vy vz] llh: ndarray - position of the target in LLH out: str - either lookangle or ecef for vector if out == \"lookangle\" los: ndarray - Satellite incidence angle sr: ndarray - Slant range in meters if out == \"ecef\" los_xyz: ndarray - Satellite LOS in ECEF from target to satellite targ_xyz: ndarray - Target XYZ positions in ECEF Source code in RAiDER/losreader.py 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 def get_radar_pos ( llh , orb , out = \"lookangle\" ): ''' Calculate the coordinate of the sensor in ECEF at the time corresponding to ***. Args: ---------- orb: isce3.core.Orbit - Nt x 7 matrix of statevectors: [t x y z vx vy vz] llh: ndarray - position of the target in LLH out: str - either lookangle or ecef for vector Returns: ------- if out == \"lookangle\" los: ndarray - Satellite incidence angle sr: ndarray - Slant range in meters if out == \"ecef\" los_xyz: ndarray - Satellite LOS in ECEF from target to satellite targ_xyz: ndarray - Target XYZ positions in ECEF ''' if out not in [ \"lookangle\" , \"ecef\" ]: raise ValueError ( f \"out kwarg must be lookangle or ecef - not { out } \" ) num_iteration = 30 residual_threshold = 1.0e-7 # Get xyz positions of targets here targ_xyz = np . stack ( lla2ecef ( llh [:, 1 ], llh [:, 0 ], llh [:, 2 ]), axis =- 1 ) # Get some isce3 constants for this inversion # TODO - Assuming right-looking for now elp = isce . core . Ellipsoid () dop = isce . core . LUT2d () look = isce . core . LookSide . Right # Iterate for each point # TODO - vectorize / parallelize sr = np . empty (( llh . shape [ 0 ],), dtype = np . float64 ) if out == \"lookangle\" : output = np . empty (( llh . shape [ 0 ],), dtype = np . float64 ) else : output = np . empty (( llh . shape [ 0 ], 3 ), dtype = np . float64 ) for ind , pt in enumerate ( llh ): if not any ( np . isnan ( pt )): # ISCE3 always uses xy convention inp = np . array ([ np . deg2rad ( pt [ 1 ]), np . deg2rad ( pt [ 0 ]), pt [ 2 ]]) # Local normal vector nv = elp . n_vector ( inp [ 0 ], inp [ 1 ]) # Wavelength does not matter for zero doppler try : aztime , slant_range = isce . geometry . geo2rdr ( inp , elp , orb , dop , 0.06 , look , threshold = residual_threshold , maxiter = num_iteration , delta_range = 10.0 ) sat_xyz , _ = orb . interpolate ( aztime ) sr [ ind ] = slant_range delta = sat_xyz - targ_xyz [ ind , :] if out == \"lookangle\" : # TODO - if we only ever need cos(lookang), # skip the arccos here and cos above delta = delta / np . linalg . norm ( delta ) output [ ind ] = np . rad2deg ( np . arccos ( np . dot ( delta , nv )) ) else : output [ ind , :] = ( sat_xyz - targ_xyz ) / slant_range except Exception as e : raise e sat_xyz [ ind , :] = np . nan sr [ ind ] = np . nan output [ ind , ... ] = np . nan else : sat_xyz [ ind , :] = np . nan sr [ ind ] = np . nan output [ ind , ... ] = np . nan # If lookangle is requested if out == \"lookangle\" : return output , sr elif out == \"ecef\" : return output , targ_xyz else : raise NotImplementedError ( \"Unexpected logic in get_radar_pos\" ) get_sv ( los_file , ref_time , pad = 3 * 60 ) Read an LOS file and return orbital state vectors Parameters: Name Type Description Default los_file str user-passed file containing either look vectors or statevectors for the sensor required ref_time datetime User-requested datetime; if not encompassed by the orbit times will raise a ValueError required pad int number of seconds to keep around the requested time 3 * 60 Returns: Name Type Description svs list of ndarrays the times, x/y/z positions and velocities of the sensor for the given window around the reference time Source code in RAiDER/losreader.py 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 def get_sv ( los_file , ref_time , pad = 3 * 60 ): \"\"\" Read an LOS file and return orbital state vectors Args: los_file (str): - user-passed file containing either look vectors or statevectors for the sensor ref_time (datetime):- User-requested datetime; if not encompassed by the orbit times will raise a ValueError pad (int): - number of seconds to keep around the requested time Returns: svs (list of ndarrays): - the times, x/y/z positions and velocities of the sensor for the given window around the reference time \"\"\" try : svs = read_txt_file ( los_file ) except ValueError : try : svs = read_ESA_Orbit_file ( los_file ) except BaseException : try : svs = read_shelve ( los_file ) except BaseException : raise ValueError ( 'get_sv: I cannot parse the statevector file {} ' . format ( los_file ) ) if ref_time : idx = cut_times ( svs [ 0 ], ref_time , pad = pad ) svs = [ d [ idx ] for d in svs ] return svs inc_hd_to_enu ( incidence , heading ) Convert incidence and heading to line-of-sight vectors from the ground to the top of the troposphere. Parameters: Name Type Description Default incidence ndarray - incidence angle in deg from vertical required heading ndarray - heading angle in deg clockwise from north required lats/lons/heights ndarray - WGS84 ellipsoidal target (ground pixel) locations required Returns: Name Type Description LOS ndarray - (input_shape) x 3 array of unit look vectors in local ENU Algorithm referenced from http://earthdef.caltech.edu/boards/4/topics/327 Source code in RAiDER/losreader.py 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 def inc_hd_to_enu ( incidence , heading ): ''' Convert incidence and heading to line-of-sight vectors from the ground to the top of the troposphere. Args: incidence: ndarray - incidence angle in deg from vertical heading: ndarray - heading angle in deg clockwise from north lats/lons/heights: ndarray - WGS84 ellipsoidal target (ground pixel) locations Returns: LOS: ndarray - (input_shape) x 3 array of unit look vectors in local ENU Algorithm referenced from http://earthdef.caltech.edu/boards/4/topics/327 ''' if np . any ( incidence < 0 ): raise ValueError ( 'inc_hd_to_enu: Incidence angle cannot be less than 0' ) east = sind ( incidence ) * cosd ( heading + 90 ) north = sind ( incidence ) * sind ( heading + 90 ) up = cosd ( incidence ) return np . stack (( east , north , up ), axis =- 1 ) read_ESA_Orbit_file ( filename ) Read orbit data from an orbit file supplied by ESA filename: str - string of the orbit filename Nt x 1 ndarray - a numpy vector with Nt elements containing time in python datetime x, y, z: Nt x 1 ndarrays - x/y/z positions of the sensor at the times t vx, vy, vz: Nt x 1 ndarrays - x/y/z velocities of the sensor at the times t Source code in RAiDER/losreader.py 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 def read_ESA_Orbit_file ( filename ): ''' Read orbit data from an orbit file supplied by ESA Args: ---------- filename: str - string of the orbit filename Returns: ------- t: Nt x 1 ndarray - a numpy vector with Nt elements containing time in python datetime x, y, z: Nt x 1 ndarrays - x/y/z positions of the sensor at the times t vx, vy, vz: Nt x 1 ndarrays - x/y/z velocities of the sensor at the times t ''' tree = ET . parse ( filename ) root = tree . getroot () data_block = root [ 1 ] numOSV = len ( data_block [ 0 ]) t = [] x = np . ones ( numOSV ) y = np . ones ( numOSV ) z = np . ones ( numOSV ) vx = np . ones ( numOSV ) vy = np . ones ( numOSV ) vz = np . ones ( numOSV ) for i , st in enumerate ( data_block [ 0 ]): t . append ( datetime . datetime . strptime ( st [ 1 ] . text , 'UTC=%Y-%m- %d T%H:%M:%S. %f ' ) ) x [ i ] = float ( st [ 4 ] . text ) y [ i ] = float ( st [ 5 ] . text ) z [ i ] = float ( st [ 6 ] . text ) vx [ i ] = float ( st [ 7 ] . text ) vy [ i ] = float ( st [ 8 ] . text ) vz [ i ] = float ( st [ 9 ] . text ) t = np . array ( t ) return [ t , x , y , z , vx , vy , vz ] read_txt_file ( filename ) Read a 7-column text file containing orbit statevectors. Time should be denoted as integer time in seconds since the reference epoch (user-requested time). Parameters: Name Type Description Default filename str user-supplied space-delimited text file with no header containing orbital statevectors as 7 columns: - time in seconds since the user-supplied epoch - x / y / z locations in ECEF cartesian coordinates - vx / vy / vz velocities in m/s in ECEF coordinates required Returns: Name Type Description svs list a length-7 list of numpy vectors containing the above variables Source code in RAiDER/losreader.py 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 def read_txt_file ( filename ): ''' Read a 7-column text file containing orbit statevectors. Time should be denoted as integer time in seconds since the reference epoch (user-requested time). Args: filename (str): - user-supplied space-delimited text file with no header containing orbital statevectors as 7 columns: - time in seconds since the user-supplied epoch - x / y / z locations in ECEF cartesian coordinates - vx / vy / vz velocities in m/s in ECEF coordinates Returns: svs (list): - a length-7 list of numpy vectors containing the above variables ''' t = list () x = list () y = list () z = list () vx = list () vy = list () vz = list () with open ( filename , 'r' ) as f : for line in f : try : parts = line . strip () . split () t_ = datetime . datetime . fromisoformat ( parts [ 0 ]) x_ , y_ , z_ , vx_ , vy_ , vz_ = [ float ( t ) for t in parts [ 1 :]] except ValueError : raise ValueError ( \"I need {} to be a 7 column text file, with \" . format ( filename ) + \"columns t, x, y, z, vx, vy, vz (Couldn't parse line \" + \" {} )\" . format ( repr ( line ))) t . append ( t_ ) x . append ( x_ ) y . append ( y_ ) z . append ( z_ ) vx . append ( vx_ ) vy . append ( vy_ ) vz . append ( vz_ ) if len ( t ) < 4 : raise ValueError ( 'read_txt_file: File {} does not have enough statevectors' . format ( filename )) return [ np . array ( a ) for a in [ t , x , y , z , vx , vy , vz ]] state_to_los ( svs , llh_targets , out = 'lookangle' ) Converts information from a state vector for a satellite orbit, given in terms of position and velocity, to line-of-sight information at each (lon,lat, height) coordinate requested by the user. svs - t, x, y, z, vx, vy, vz - time, position, and velocity in ECEF of the sensor llh_targets - lats, lons, heights - Ellipsoidal (WGS84) positions of target ground pixels LOS - * x 3 matrix of LOS unit vectors in ECEF ( not ENU) Example: import datetime import numpy from RAiDER.utilFcns import rio_open import RAiDER.losreader as losr lats, lons, heights = np.array([-76.1]), np.array([36.83]), np.array([0]) time = datetime.datetime(2018,11,12,23,0,0) download the orbit file beforehand esa_orbit_file = 'S1A_OPER_AUX_POEORB_OPOD_20181203T120749_V20181112T225942_20181114T005942.EOF' svs = losr.read_ESA_Orbit_file(esa_orbit_file) LOS = losr.state_to_los(*svs, [lats, lons, heights], xyz) Source code in RAiDER/losreader.py 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 def state_to_los ( svs , llh_targets , out = \"lookangle\" ): ''' Converts information from a state vector for a satellite orbit, given in terms of position and velocity, to line-of-sight information at each (lon,lat, height) coordinate requested by the user. Args: ---------- svs - t, x, y, z, vx, vy, vz - time, position, and velocity in ECEF of the sensor llh_targets - lats, lons, heights - Ellipsoidal (WGS84) positions of target ground pixels Returns: ------- LOS - * x 3 matrix of LOS unit vectors in ECEF (*not* ENU) Example: >>> import datetime >>> import numpy >>> from RAiDER.utilFcns import rio_open >>> import RAiDER.losreader as losr >>> lats, lons, heights = np.array([-76.1]), np.array([36.83]), np.array([0]) >>> time = datetime.datetime(2018,11,12,23,0,0) >>> # download the orbit file beforehand >>> esa_orbit_file = 'S1A_OPER_AUX_POEORB_OPOD_20181203T120749_V20181112T225942_20181114T005942.EOF' >>> svs = losr.read_ESA_Orbit_file(esa_orbit_file) >>> LOS = losr.state_to_los(*svs, [lats, lons, heights], xyz) ''' if out not in [ \"lookangle\" , \"ecef\" ]: raise ValueError ( f \"Output type can be lookangle or ecef - not { out } \" ) # check the inputs if np . min ( svs . shape ) < 4 : raise RuntimeError ( 'state_to_los: At least 4 state vectors are required' ' for orbit interpolation' ) # Convert svs to isce3 orbit orb = isce . core . Orbit ([ isce . core . StateVector ( isce . core . DateTime ( row [ 0 ]), row [ 1 : 4 ], row [ 4 : 7 ] ) for row in svs ]) # Flatten the input array for convenience in_shape = llh_targets [ 0 ] . shape target_llh = np . stack ([ x . flatten () for x in llh_targets ], axis =- 1 ) Npts = len ( target_llh ) # Iterate through targets and compute LOS if out == \"lookangle\" : los_ang , slant_range = get_radar_pos ( target_llh , orb , out = \"lookangle\" ) los_factor = np . cos ( np . deg2rad ( los_ang )) . reshape ( in_shape ) return los_factor elif out == \"ecef\" : los_xyz , targ_xyz = get_radar_pos ( target_llh , orb , out = \"ecef\" ) return los_xyz , targ_xyz else : raise NotImplementedError ( \"Unexpected logic in state_to_los\" ) makePoints __file__ = '/home/jhkennedy/Documents/Code/contributing/RAiDER/tools/RAiDER/makePoints.cpython-310-x86_64-linux-gnu.so' str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __name__ = 'RAiDER.makePoints' str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __package__ = 'RAiDER' str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __test__ = {} dict() -> new empty dictionary dict(mapping) -> new dictionary initialized from a mapping object's (key, value) pairs dict(iterable) -> new dictionary initialized as if via: d = {} for k, v in iterable: d[k] = v dict(**kwargs) -> new dictionary initialized with the name=value pairs in the keyword argument list. For example: dict(one=1, two=2) makePoints0D () builtin Fast cython code to create the rays needed for ray-tracing. Inputs: max_len: maximum length of the rays Rays_SP: 1 x 3 numpy array of the location of the ground pixels in an earth-centered, earth-fixed coordinate system Rays_SLV: 1 x 3 numpy array of the look vectors pointing from the ground pixel to the sensor stepSize: Distance between points along the ray-path Output ray: a 3 x Npts array containing the rays tracing a path from the ground pixels, along the line-of-sight vectors, up to the maximum length specified. makePoints1D () builtin Fast cython code to create the rays needed for ray-tracing. Inputs: max_len: maximum length of the rays Rays_SP: Nx x 3 numpy array of the location of the ground pixels in an earth-centered, earth-fixed coordinate system Rays_SLV: Nx x 3 numpy array of the look vectors pointing from the ground pixel to the sensor stepSize: Distance between points along the ray-path Output ray: a Nx x 3 x Npts array containing the rays tracing a path from the ground pixels, along the line-of-sight vectors, up to the maximum length specified. makePoints2D () builtin Fast cython code to create the rays needed for ray-tracing. Inputs: max_len: maximum length of the rays Rays_SP: Nx x Ny x 3 numpy array of the location of the ground pixels in an earth-centered, earth-fixed coordinate system Rays_SLV: Nx x Ny x 3 numpy array of the look vectors pointing from the ground pixel to the sensor stepSize: Distance between points along the ray-path Output ray: a Nx x Ny x 3 x Npts array containing the rays tracing a path from the ground pixels, along the line-of-sight vectors, up to the maximum length specified. makePoints3D () builtin Fast cython code to create the rays needed for ray-tracing Inputs: max_len: maximum length of the rays Rays_SP: Nx x Ny x Nz x 3 numpy array of the location of the ground pixels in an earth-centered, earth-fixed coordinate system Rays_SLV: Nx x Ny x Nz x 3 numpy array of the look vectors pointing from the ground pixel to the sensor stepSize: Distance between points along the ray-path Output ray: a Nx x Ny x Nz x 3 x Npts array containing the rays tracing a path from the ground pixels, along the line-of-sight vectors, up to the maximum length specified. models ecmwf ECMWF Bases: WeatherModel Implement ECMWF models Source code in RAiDER/models/ecmwf.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 class ECMWF ( WeatherModel ): ''' Implement ECMWF models ''' def __init__ ( self ): # initialize a weather model WeatherModel . __init__ ( self ) # model constants self . _k1 = 0.776 # [K/Pa] self . _k2 = 0.233 # [K/Pa] self . _k3 = 3.75e3 # [K^2/Pa] self . _time_res = 1 self . _lon_res = 0.2 self . _lat_res = 0.2 self . _proj = CRS . from_epsg ( 4326 ) self . _model_level_type = 'ml' # Default def setLevelType ( self , levelType ): '''Set the level type to model levels or pressure levels''' if levelType in [ 'ml' , 'pl' ]: self . _model_level_type = levelType else : raise RuntimeError ( 'Level type {} is not recognized' . format ( levelType )) if levelType == 'ml' : self . __model_levels__ () else : self . __pressure_levels__ () def __pressure_levels__ ( self ): self . _zlevels = np . flipud ( LEVELS_25_HEIGHTS ) self . _levels = len ( self . _zlevels ) def __model_levels__ ( self ): self . _levels = 137 self . _zlevels = np . flipud ( LEVELS_137_HEIGHTS ) self . _a = A_137_HRES self . _b = B_137_HRES def load_weather ( self , * args , ** kwargs ): ''' Consistent class method to be implemented across all weather model types. As a result of calling this method, all of the variables (x, y, z, p, q, t, wet_refractivity, hydrostatic refractivity, e) should be fully populated. ''' self . _load_model_level ( * self . files ) def _load_model_level ( self , fname ): # read data from netcdf file lats , lons , xs , ys , t , q , lnsp , z = self . _makeDataCubes ( fname , verbose = False ) # ECMWF appears to give me this backwards if lats [ 0 ] > lats [ 1 ]: z = z [:: - 1 ] lnsp = lnsp [:: - 1 ] t = t [:, :: - 1 ] q = q [:, :: - 1 ] lats = lats [:: - 1 ] # Lons is usually ok, but we'll throw in a check to be safe if lons [ 0 ] > lons [ 1 ]: z = z [ ... , :: - 1 ] lnsp = lnsp [ ... , :: - 1 ] t = t [ ... , :: - 1 ] q = q [ ... , :: - 1 ] lons = lons [:: - 1 ] # pyproj gets fussy if the latitude is wrong, plus our # interpolator isn't clever enough to pick up on the fact that # they are the same lons [ lons > 180 ] -= 360 self . _t = t self . _q = q geo_hgt , pres , hgt = self . _calculategeoh ( z , lnsp ) # re-assign lons, lats to match heights _lons = np . broadcast_to ( lons [ np . newaxis , np . newaxis , :], hgt . shape ) _lats = np . broadcast_to ( lats [ np . newaxis , :, np . newaxis ], hgt . shape ) # ys is latitude self . _get_heights ( _lats , hgt ) h = self . _zs . copy () # We want to support both pressure levels and true pressure grids. # If the shape has one dimension, we'll scale it up to act as a # grid, otherwise we'll leave it alone. if len ( pres . shape ) == 1 : self . _p = np . broadcast_to ( pres [:, np . newaxis , np . newaxis ], self . _zs . shape ) else : self . _p = pres # Re-structure everything from (heights, lats, lons) to (lons, lats, heights) self . _p = np . transpose ( self . _p , ( 1 , 2 , 0 )) self . _t = np . transpose ( self . _t , ( 1 , 2 , 0 )) self . _q = np . transpose ( self . _q , ( 1 , 2 , 0 )) h = np . transpose ( h , ( 1 , 2 , 0 )) self . _lats = np . transpose ( _lats , ( 1 , 2 , 0 )) self . _lons = np . transpose ( _lons , ( 1 , 2 , 0 )) # Flip all the axis so that zs are in order from bottom to top # lats / lons are simply replicated to all heights so they don't need flipped self . _p = np . flip ( self . _p , axis = 2 ) self . _t = np . flip ( self . _t , axis = 2 ) self . _q = np . flip ( self . _q , axis = 2 ) self . _ys = self . _lats . copy () self . _xs = self . _lons . copy () self . _zs = np . flip ( h , axis = 2 ) def _fetch ( self , out ): ''' Fetch a weather model from ECMWF ''' # bounding box plus a buffer lat_min , lat_max , lon_min , lon_max = self . _ll_bounds # execute the search at ECMWF try : self . _get_from_ecmwf ( lat_min , lat_max , self . _lat_res , lon_min , lon_max , self . _lon_res , self . _time , out ) except Exception as e : logger . warning ( 'Query point bounds are {} / {} / {} / {} ' . format ( lat_min , lat_max , lon_min , lon_max )) logger . warning ( 'Query time: {} ' . format ( self . _time )) logger . exception ( e ) def _get_from_ecmwf ( self , lat_min , lat_max , lat_step , lon_min , lon_max , lon_step , time , out ): import ecmwfapi server = ecmwfapi . ECMWFDataServer () corrected_date = util . round_date ( time , datetime . timedelta ( hours = 6 )) server . retrieve ({ \"class\" : self . _classname , # ERA-Interim 'dataset' : self . _dataset , \"expver\" : \" {} \" . format ( self . _expver ), # They warn me against all, but it works well \"levelist\" : 'all' , \"levtype\" : \"ml\" , # Model levels \"param\" : \"lnsp/q/z/t\" , # Necessary variables \"stream\" : \"oper\" , # date: Specify a single date as \"2015-08-01\" or a period as # \"2015-08-01/to/2015-08-31\". \"date\" : datetime . datetime . strftime ( corrected_date , \"%Y-%m- %d \" ), # type: Use an (analysis) unless you have a particular reason to # use fc (forecast). \"type\" : \"an\" , # time: With type=an, time can be any of # \"00:00:00/06:00:00/12:00:00/18:00:00\". With type=fc, time can # be any of \"00:00:00/12:00:00\", \"time\" : datetime . time . strftime ( corrected_date . time (), \"%H:%M:%S\" ), # step: With type=an, step is always \"0\". With type=fc, step can # be any of \"3/6/9/12\". \"step\" : \"0\" , # grid: Only regular lat/lon grids are supported. \"grid\" : ' {} / {} ' . format ( lat_step , lon_step ), \"area\" : ' {} / {} / {} / {} ' . format ( lat_max , lon_min , lat_min , lon_max ), # area: N/W/S/E \"format\" : \"netcdf\" , \"resol\" : \"av\" , \"target\" : out , # target: the name of the output file. }) def _get_from_cds ( self , lat_min , lat_max , lon_min , lon_max , acqTime , outname ): import cdsapi c = cdsapi . Client ( verify = 0 ) if self . _model_level_type == 'pl' : var = [ 'z' , 'q' , 't' ] levType = 'pressure_level' else : var = \"129/130/133/152\" # 'lnsp', 'q', 'z', 't' levType = 'model_level' bbox = [ lat_max , lon_min , lat_min , lon_max ] # round to the closest legal time corrected_date = util . round_date ( acqTime , datetime . timedelta ( hours = self . _time_res )) # I referenced https://confluence.ecmwf.int/display/CKB/How+to+download+ERA5 dataDict = { \"class\" : \"ea\" , \"expver\" : \"1\" , \"levelist\" : 'all' , \"levtype\" : \" {} \" . format ( self . _model_level_type ), # 'ml' for model levels or 'pl' for pressure levels 'param' : var , \"stream\" : \"oper\" , \"type\" : \"an\" , \"date\" : \" {} \" . format ( corrected_date . strftime ( '%Y-%m- %d ' )), \"time\" : \" {} \" . format ( datetime . time . strftime ( corrected_date . time (), '%H:%M' )), # step: With type=an, step is always \"0\". With type=fc, step can # be any of \"3/6/9/12\". \"step\" : \"0\" , \"area\" : bbox , \"grid\" : [ 0.25 , .25 ], \"format\" : \"netcdf\" } try : c . retrieve ( 'reanalysis-era5-complete' , dataDict , outname ) except Exception as e : logger . warning ( 'Query point bounds are {} / {} latitude and {} / {} longitude' . format ( lat_min , lat_max , lon_min , lon_max )) logger . warning ( 'Query time: {} ' . format ( acqTime )) logger . exception ( e ) raise Exception def _download_ecmwf ( self , lat_min , lat_max , lat_step , lon_min , lon_max , lon_step , time , out ): from ecmwfapi import ECMWFService server = ECMWFService ( \"mars\" ) # round to the closest legal time corrected_date = util . round_date ( time , datetime . timedelta ( hours = self . _time_res )) if self . _model_level_type == 'ml' : param = \"129/130/133/152\" else : param = \"129.128/130.128/133.128/152\" server . execute ( { 'class' : self . _classname , 'dataset' : self . _dataset , 'expver' : \" {} \" . format ( self . _expver ), 'resol' : \"av\" , 'stream' : \"oper\" , 'type' : \"an\" , 'levelist' : \"all\" , 'levtype' : \" {} \" . format ( self . _model_level_type ), 'param' : param , 'date' : datetime . datetime . strftime ( corrected_date , \"%Y-%m- %d \" ), 'time' : \" {} \" . format ( datetime . time . strftime ( corrected_date . time (), '%H:%M' )), 'step' : \"0\" , 'grid' : \" {} / {} \" . format ( lon_step , lat_step ), 'area' : \" {} / {} / {} / {} \" . format ( lat_max , util . floorish ( lon_min , 0.1 ), util . floorish ( lat_min , 0.1 ), lon_max ), 'format' : \"netcdf\" , }, out ) def _load_pressure_level ( self , filename , * args , ** kwargs ): with xr . open_dataset ( filename ) as block : # Pull the data z = np . squeeze ( block [ 'z' ] . values ) t = np . squeeze ( block [ 't' ] . values ) q = np . squeeze ( block [ 'q' ] . values ) lats = np . squeeze ( block . latitude . values ) lons = np . squeeze ( block . longitude . values ) levels = np . squeeze ( block . level . values ) * 100 z = np . flip ( z , axis = 1 ) # ECMWF appears to give me this backwards if lats [ 0 ] > lats [ 1 ]: z = z [:: - 1 ] t = t [:, :: - 1 ] q = q [:, :: - 1 ] lats = lats [:: - 1 ] # Lons is usually ok, but we'll throw in a check to be safe if lons [ 0 ] > lons [ 1 ]: z = z [ ... , :: - 1 ] t = t [ ... , :: - 1 ] q = q [ ... , :: - 1 ] lons = lons [:: - 1 ] # pyproj gets fussy if the latitude is wrong, plus our # interpolator isn't clever enough to pick up on the fact that # they are the same lons [ lons > 180 ] -= 360 self . _t = t self . _q = q geo_hgt = z / self . _g0 # re-assign lons, lats to match heights _lons = np . broadcast_to ( lons [ np . newaxis , np . newaxis , :], geo_hgt . shape ) _lats = np . broadcast_to ( lats [ np . newaxis , :, np . newaxis ], geo_hgt . shape ) # correct heights for latitude self . _get_heights ( _lats , geo_hgt ) self . _p = np . broadcast_to ( levels [:, np . newaxis , np . newaxis ], self . _zs . shape ) # Re-structure everything from (heights, lats, lons) to (lons, lats, heights) self . _p = np . transpose ( self . _p ) self . _t = np . transpose ( self . _t ) self . _q = np . transpose ( self . _q ) self . _lats = np . transpose ( _lats ) self . _lons = np . transpose ( _lons ) self . _ys = self . _lats . copy () self . _xs = self . _lons . copy () self . _zs = np . transpose ( self . _zs ) # check this # data cube format should be lats,lons,heights self . _lats = self . _lats . swapaxes ( 0 , 1 ) self . _lons = self . _lons . swapaxes ( 0 , 1 ) self . _xs = self . _xs . swapaxes ( 0 , 1 ) self . _ys = self . _ys . swapaxes ( 0 , 1 ) self . _zs = self . _zs . swapaxes ( 0 , 1 ) self . _p = self . _p . swapaxes ( 0 , 1 ) self . _q = self . _q . swapaxes ( 0 , 1 ) self . _t = self . _t . swapaxes ( 0 , 1 ) # For some reason z is opposite the others self . _p = np . flip ( self . _p , axis = 2 ) self . _t = np . flip ( self . _t , axis = 2 ) self . _q = np . flip ( self . _q , axis = 2 ) def _makeDataCubes ( self , fname , verbose = False ): ''' Create a cube of data representing temperature and relative humidity at specified pressure levels ''' # get ll_bounds S , N , W , E = self . _ll_bounds with xr . open_dataset ( fname ) as ds : ds = ds . assign_coords ( longitude = ((( ds . longitude + 180 ) % 360 ) - 180 )) # mask based on query bounds m1 = ( S <= ds . latitude ) & ( N >= ds . latitude ) m2 = ( W <= ds . longitude ) & ( E >= ds . longitude ) block = ds . where ( m1 & m2 , drop = True ) # Pull the data z = np . squeeze ( block [ 'z' ] . values )[ 0 , ... ] t = np . squeeze ( block [ 't' ] . values ) q = np . squeeze ( block [ 'q' ] . values ) lnsp = np . squeeze ( block [ 'lnsp' ] . values )[ 0 , ... ] lats = np . squeeze ( block . latitude . values ) lons = np . squeeze ( block . longitude . values ) xs = lons . copy () ys = lats . copy () if z . size == 0 : raise RuntimeError ( 'There is no data in z, ' 'you may have a problem with your mask' ) return lats , lons , xs , ys , t , q , lnsp , z load_weather ( * args , ** kwargs ) Consistent class method to be implemented across all weather model types. As a result of calling this method, all of the variables (x, y, z, p, q, t, wet_refractivity, hydrostatic refractivity, e) should be fully populated. Source code in RAiDER/models/ecmwf.py 64 65 66 67 68 69 70 71 def load_weather ( self , * args , ** kwargs ): ''' Consistent class method to be implemented across all weather model types. As a result of calling this method, all of the variables (x, y, z, p, q, t, wet_refractivity, hydrostatic refractivity, e) should be fully populated. ''' self . _load_model_level ( * self . files ) setLevelType ( levelType ) Set the level type to model levels or pressure levels Source code in RAiDER/models/ecmwf.py 42 43 44 45 46 47 48 49 50 51 52 def setLevelType ( self , levelType ): '''Set the level type to model levels or pressure levels''' if levelType in [ 'ml' , 'pl' ]: self . _model_level_type = levelType else : raise RuntimeError ( 'Level type {} is not recognized' . format ( levelType )) if levelType == 'ml' : self . __model_levels__ () else : self . __pressure_levels__ () era5 ERA5 Bases: ECMWF Source code in RAiDER/models/era5.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 class ERA5 ( ECMWF ): # I took this from # https://www.ecmwf.int/en/forecasts/documentation-and-support/137-model-levels. def __init__ ( self ): ECMWF . __init__ ( self ) self . _humidityType = 'q' self . _expver = '0001' self . _classname = 'ea' self . _dataset = 'era5' self . _Name = 'ERA-5' self . _proj = CRS . from_epsg ( 4326 ) # Tuple of min/max years where data is available. self . _valid_range = ( datetime . datetime ( 1950 , 1 , 1 ), \"Present\" ) # Availability lag time in days self . _lag_time = datetime . timedelta ( days = 30 ) # Default, need to change to ml self . setLevelType ( 'pl' ) def _fetch ( self , out ): ''' Fetch a weather model from ECMWF ''' # bounding box plus a buffer lat_min , lat_max , lon_min , lon_max = self . _ll_bounds time = self . _time # execute the search at ECMWF try : self . _get_from_cds ( lat_min , lat_max , lon_min , lon_max , time , out ) except Exception as e : logger . warning ( e ) raise RuntimeError ( 'Could not access or download from the CDS API' ) def load_weather ( self , * args , ** kwargs ): '''Load either pressure or model level data''' if self . _model_level_type == 'pl' : self . _load_pressure_level ( * self . files , * args , ** kwargs ) elif self . _model_level_type == 'ml' : self . _load_model_level ( * self . files , * args , ** kwargs ) else : raise RuntimeError ( ' {} is not a valid model type' . format ( self . _model_level_type ) ) load_weather ( * args , ** kwargs ) Load either pressure or model level data Source code in RAiDER/models/era5.py 46 47 48 49 50 51 52 53 54 55 def load_weather ( self , * args , ** kwargs ): '''Load either pressure or model level data''' if self . _model_level_type == 'pl' : self . _load_pressure_level ( * self . files , * args , ** kwargs ) elif self . _model_level_type == 'ml' : self . _load_model_level ( * self . files , * args , ** kwargs ) else : raise RuntimeError ( ' {} is not a valid model type' . format ( self . _model_level_type ) ) generateGACOSVRT convertAllFiles ( dirLoc ) convert all RSC files to VRT files contained in dirLoc Source code in RAiDER/models/generateGACOSVRT.py 45 46 47 48 49 50 51 52 def convertAllFiles ( dirLoc ): ''' convert all RSC files to VRT files contained in dirLoc ''' import glob files = glob . glob ( '*.rsc' ) for f in files : makeVRT ( f ) makeVRT ( filename , dtype = 'Float32' ) Use an RSC file to create a GDAL-compatible VRT file for opening GACOS weather model files Source code in RAiDER/models/generateGACOSVRT.py 6 7 8 9 10 11 12 def makeVRT ( filename , dtype = 'Float32' ): ''' Use an RSC file to create a GDAL-compatible VRT file for opening GACOS weather model files ''' fields = readRSC ( filename ) string = vrtStr ( fields [ 'XMAX' ], fields [ 'YMAX' ], fields [ 'X_FIRST' ], fields [ 'Y_FIRST' ], fields [ 'X_STEP' ], fields [ 'Y_STEP' ], filename . replace ( '.rsc' , '' ), dtype = dtype ) writeStringToFile ( string , filename . replace ( '.rsc' , '' ) . replace ( '.ztd' , '' ) + '.vrt' ) writeStringToFile ( string , filename ) Write a string to a VRT file Source code in RAiDER/models/generateGACOSVRT.py 15 16 17 18 19 20 def writeStringToFile ( string , filename ): ''' Write a string to a VRT file ''' with open ( filename , 'w' ) as f : f . write ( string ) gmao GMAO Bases: WeatherModel Source code in RAiDER/models/gmao.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 class GMAO ( WeatherModel ): # I took this from GMAO model level weblink # https://opendap.nccs.nasa.gov/dods/GEOS-5/fp/0.25_deg/assim/inst3_3d_asm_Nv def __init__ ( self ): # initialize a weather model WeatherModel . __init__ ( self ) self . _humidityType = 'q' self . _model_level_type = 'ml' # Default, pressure levels are 'pl' self . _classname = 'gmao' self . _dataset = 'gmao' # Tuple of min/max years where data is available. self . _valid_range = ( dt . datetime ( 2014 , 2 , 20 ), \"Present\" ) self . _lag_time = dt . timedelta ( hours = 24.0 ) # Availability lag time in hours # model constants self . _k1 = 0.776 # [K/Pa] self . _k2 = 0.233 # [K/Pa] self . _k3 = 3.75e3 # [K^2/Pa] self . _time_res = 1 # horizontal grid spacing self . _lat_res = 0.25 self . _lon_res = 0.3125 self . _x_res = 0.3125 self . _y_res = 0.25 self . _zlevels = np . flipud ( LEVELS_137_HEIGHTS ) self . _Name = 'GMAO' self . files = None self . _bounds = None # Projection self . _proj = CRS . from_epsg ( 4326 ) def _fetch ( self , out ): ''' Fetch weather model data from GMAO ''' time = self . _time # calculate the array indices for slicing the GMAO variable arrays lat_min_ind = int (( self . _ll_bounds [ 0 ] - ( - 90.0 )) / self . _lat_res ) lat_max_ind = int (( self . _ll_bounds [ 1 ] - ( - 90.0 )) / self . _lat_res ) lon_min_ind = int (( self . _ll_bounds [ 2 ] - ( - 180.0 )) / self . _lon_res ) lon_max_ind = int (( self . _ll_bounds [ 3 ] - ( - 180.0 )) / self . _lon_res ) T0 = dt . datetime ( 2017 , 12 , 1 , 0 , 0 , 0 ) # round time to nearest third hour time1 = time time = round_time ( time , 3 * 60 * 60 ) if not time1 == time : logger . warning ( 'Rounded given hour from %d to %d ' , time1 . hour , time . hour ) DT = time - T0 time_ind = int ( DT . total_seconds () / 3600.0 / 3.0 ) ml_min = 0 ml_max = 71 if time >= T0 : # open the dataset and pull the data url = 'https://opendap.nccs.nasa.gov/dods/GEOS-5/fp/0.25_deg/assim/inst3_3d_asm_Nv' session = pydap . cas . urs . setup_session ( 'username' , 'password' , check_url = url ) ds = pydap . client . open_url ( url , session = session ) qv = ds [ 'qv' ] . array [ time_ind , ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 ) ] . data [ 0 ] p = ds [ 'pl' ] . array [ time_ind , ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 ) ] . data [ 0 ] t = ds [ 't' ] . array [ time_ind , ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 ) ] . data [ 0 ] h = ds [ 'h' ] . array [ time_ind , ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 ) ] . data [ 0 ] else : root = 'https://portal.nccs.nasa.gov/datashare/gmao/geos-fp/das/Y {} /M {:02d} /D {:02d} ' base = f 'GEOS.fp.asm.inst3_3d_asm_Nv. { time . strftime ( \"%Y%m %d \" ) } _ { time . hour : 02 } 00.V01.nc4' url = f ' { root . format ( time . year , time . month , time . day ) } / { base } ' f = ' {} _raw {} ' . format ( * os . path . splitext ( out )) if not os . path . exists ( f ): logger . info ( 'Fetching URL: %s ' , url ) session = requests_retry_session () resp = session . get ( url , stream = True ) assert resp . ok , f 'Could not access url for time: { time } ' with open ( f , 'wb' ) as fh : shutil . copyfileobj ( resp . raw , fh ) else : logger . warning ( 'Weather model already exists, skipping download' ) with h5py . File ( f , 'r' ) as ds : q = ds [ 'QV' ][ 0 , :, lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )] p = ds [ 'PL' ][ 0 , :, lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )] t = ds [ 'T' ][ 0 , :, lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )] h = ds [ 'H' ][ 0 , :, lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )] os . remove ( f ) lats = np . arange ( ( - 90 + lat_min_ind * self . _lat_res ), ( - 90 + ( lat_max_ind + 1 ) * self . _lat_res ), self . _lat_res ) lons = np . arange ( ( - 180 + lon_min_ind * self . _lon_res ), ( - 180 + ( lon_max_ind + 1 ) * self . _lon_res ), self . _lon_res ) try : # Note that lat/lon gets written twice for GMAO because they are the same as y/x writeWeatherVars2NETCDF4 ( self , lats , lons , h , qv , p , t , outName = out ) except Exception : logger . exception ( \"Unable to save weathermodel to file\" ) def load_weather ( self , f = None ): ''' Consistent class method to be implemented across all weather model types. As a result of calling this method, all of the variables (x, y, z, p, q, t, wet_refractivity, hydrostatic refractivity, e) should be fully populated. ''' if f is None : f = self . files [ 0 ] self . _load_model_level ( f ) def _load_model_level ( self , filename ): ''' Get the variables from the GMAO link using OpenDAP ''' # adding the import here should become absolute when transition to netcdf from netCDF4 import Dataset with Dataset ( filename , mode = 'r' ) as f : lons = np . array ( f . variables [ 'x' ][:]) lats = np . array ( f . variables [ 'y' ][:]) h = np . array ( f . variables [ 'H' ][:]) q = np . array ( f . variables [ 'QV' ][:]) p = np . array ( f . variables [ 'PL' ][:]) t = np . array ( f . variables [ 'T' ][:]) # restructure the 3-D lat/lon/h in regular grid _lons = np . broadcast_to ( lons [ np . newaxis , np . newaxis , :], t . shape ) _lats = np . broadcast_to ( lats [ np . newaxis , :, np . newaxis ], t . shape ) # Re-structure everything from (heights, lats, lons) to (lons, lats, heights) p = np . transpose ( p ) q = np . transpose ( q ) t = np . transpose ( t ) h = np . transpose ( h ) _lats = np . transpose ( _lats ) _lons = np . transpose ( _lons ) # check this # data cube format should be lats,lons,heights p = p . swapaxes ( 0 , 1 ) q = q . swapaxes ( 0 , 1 ) t = t . swapaxes ( 0 , 1 ) h = h . swapaxes ( 0 , 1 ) _lats = _lats . swapaxes ( 0 , 1 ) _lons = _lons . swapaxes ( 0 , 1 ) # For some reason z is opposite the others p = np . flip ( p , axis = 2 ) q = np . flip ( q , axis = 2 ) t = np . flip ( t , axis = 2 ) h = np . flip ( h , axis = 2 ) _lats = np . flip ( _lats , axis = 2 ) _lons = np . flip ( _lons , axis = 2 ) # assign the regular-grid (lat/lon/h) variables self . _p = p self . _q = q self . _t = t self . _lats = _lats self . _lons = _lons self . _xs = _lons self . _ys = _lats self . _zs = h load_weather ( f = None ) Consistent class method to be implemented across all weather model types. As a result of calling this method, all of the variables (x, y, z, p, q, t, wet_refractivity, hydrostatic refractivity, e) should be fully populated. Source code in RAiDER/models/gmao.py 151 152 153 154 155 156 157 158 159 160 def load_weather ( self , f = None ): ''' Consistent class method to be implemented across all weather model types. As a result of calling this method, all of the variables (x, y, z, p, q, t, wet_refractivity, hydrostatic refractivity, e) should be fully populated. ''' if f is None : f = self . files [ 0 ] self . _load_model_level ( f ) hres HRES Bases: ECMWF Implement ECMWF models Source code in RAiDER/models/hres.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 class HRES ( ECMWF ): ''' Implement ECMWF models ''' def __init__ ( self , level_type = 'ml' ): # initialize a weather model WeatherModel . __init__ ( self ) # model constants self . _k1 = 0.776 # [K/Pa] self . _k2 = 0.233 # [K/Pa] self . _k3 = 3.75e3 # [K^2/Pa] self . _time_res = 6 # 9 km horizontal grid spacing. This is only used for extending the download-buffer, i.e. not in subsequent processing. self . _lon_res = 9. / 111 # 0.08108115 self . _lat_res = 9. / 111 # 0.08108115 self . _x_res = 9. / 111 # 0.08108115 self . _y_res = 9. / 111 # 0.08108115 self . _humidityType = 'q' # Default, pressure levels are 'pl' self . _expver = '1' self . _classname = 'od' self . _dataset = 'hres' self . _Name = 'HRES' self . _proj = CRS . from_epsg ( 4326 ) # Tuple of min/max years where data is available. self . _valid_range = ( datetime . datetime ( 1983 , 4 , 20 ), \"Present\" ) # Availability lag time in days self . _lag_time = datetime . timedelta ( hours = 6 ) self . setLevelType ( 'ml' ) def update_a_b ( self ): # Before 2013-06-26, there were only 91 model levels. The mapping coefficients below are extracted # based on https://www.ecmwf.int/en/forecasts/documentation-and-support/91-model-levels self . _levels = 91 self . _zlevels = np . flipud ( LEVELS_91_HEIGHTS ) self . _a = A_91_HRES self . _b = B_91_HRES def load_weather ( self , filename = None ): ''' Consistent class method to be implemented across all weather model types. As a result of calling this method, all of the variables (x, y, z, p, q, t, wet_refractivity, hydrostatic refractivity, e) should be fully populated. ''' if filename is None : filename = self . files [ 0 ] if self . _model_level_type == 'ml' : if ( self . _time < datetime . datetime ( 2013 , 6 , 26 , 0 , 0 , 0 )): self . update_a_b () self . _load_model_level ( filename ) elif self . _model_level_type == 'pl' : self . _load_pressure_levels ( filename ) def _fetch ( self , out ): ''' Fetch a weather model from ECMWF ''' # bounding box plus a buffer lat_min , lat_max , lon_min , lon_max = self . _ll_bounds time = self . _time if ( time < datetime . datetime ( 2013 , 6 , 26 , 0 , 0 , 0 )): self . update_a_b () # execute the search at ECMWF self . _download_ecmwf ( lat_min , lat_max , self . _lat_res , lon_min , lon_max , self . _lon_res , time , out ) load_weather ( filename = None ) Consistent class method to be implemented across all weather model types. As a result of calling this method, all of the variables (x, y, z, p, q, t, wet_refractivity, hydrostatic refractivity, e) should be fully populated. Source code in RAiDER/models/hres.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def load_weather ( self , filename = None ): ''' Consistent class method to be implemented across all weather model types. As a result of calling this method, all of the variables (x, y, z, p, q, t, wet_refractivity, hydrostatic refractivity, e) should be fully populated. ''' if filename is None : filename = self . files [ 0 ] if self . _model_level_type == 'ml' : if ( self . _time < datetime . datetime ( 2013 , 6 , 26 , 0 , 0 , 0 )): self . update_a_b () self . _load_model_level ( filename ) elif self . _model_level_type == 'pl' : self . _load_pressure_levels ( filename ) hrrr HRRR Bases: WeatherModel Source code in RAiDER/models/hrrr.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 class HRRR ( WeatherModel ): def __init__ ( self ): # initialize a weather model super () . __init__ () self . _humidityType = 'q' self . _model_level_type = 'pl' # Default, pressure levels are 'pl' self . _expver = '0001' self . _classname = 'hrrr' self . _dataset = 'hrrr' self . _time_res = 1 # Tuple of min/max years where data is available. self . _valid_range = ( datetime . datetime ( 2016 , 7 , 15 ), \"Present\" ) self . _lag_time = datetime . timedelta ( hours = 3 ) # Availability lag time in days # model constants: TODO: need to update/double-check these self . _k1 = 0.776 # [K/Pa] self . _k2 = 0.233 # [K/Pa] self . _k3 = 3.75e3 # [K^2/Pa] # 3 km horizontal grid spacing self . _lat_res = 3. / 111 self . _lon_res = 3. / 111 self . _x_res = 3. self . _y_res = 3. self . _Nproc = 1 self . _Name = 'HRRR' self . _Npl = 0 self . files = None self . _bounds = None self . _zlevels = np . flipud ( LEVELS_137_HEIGHTS ) # Projection # See https://github.com/blaylockbk/pyBKB_v2/blob/master/demos/HRRR_earthRelative_vs_gridRelative_winds.ipynb and code lower down # '262.5:38.5:38.5:38.5 237.280472:1799:3000.00 21.138123:1059:3000.00' # 'lov:latin1:latin2:latd lon1:nx:dx lat1:ny:dy' # LCC parameters lon0 = 262.5 lat0 = 38.5 lat1 = 38.5 lat2 = 38.5 x0 = 0 y0 = 0 earth_radius = 6371229 p1 = CRS ( f '+proj=lcc +lat_1= { lat1 } +lat_2= { lat2 } +lat_0= { lat0 } ' \\ f '+lon_0= { lon0 } +x_0= { x0 } +y_0= { y0 } +a= { earth_radius } ' \\ f '+b= { earth_radius } +units=m +no_defs' ) self . _proj = p1 def _fetch ( self , out ): ''' Fetch weather model data from HRRR ''' # bounding box plus a buffer time = self . _time self . files = self . _download_hrrr_file ( time , out ) def load_weather ( self , * args , filename = None , ** kwargs ): ''' Load a weather model into a python weatherModel object, from self.files if no filename is passed. ''' if filename is None : filename = self . files # read data from grib file try : ds = xarray . open_dataset ( filename , engine = 'cfgrib' ) except EOFError : ds = xarray . open_dataset ( filename , engine = 'netcdf4' ) pl = np . array ([ self . _convertmb2Pa ( p ) for p in ds . levels . values ]) xArr = ds [ 'x' ] . values yArr = ds [ 'y' ] . values lats = ds [ 'lats' ] . values lons = ds [ 'lons' ] . values temps = ds [ 't' ] . values . transpose ( 1 , 2 , 0 ) qs = ds [ 'q' ] . values . transpose ( 1 , 2 , 0 ) geo_hgt = ds [ 'z' ] . values . transpose ( 1 , 2 , 0 ) Ny , Nx = lats . shape lons [ lons > 180 ] -= 360 # data cube format should be lats,lons,heights _xs = np . broadcast_to ( xArr [ np . newaxis , :, np . newaxis ], geo_hgt . shape ) _ys = np . broadcast_to ( yArr [:, np . newaxis , np . newaxis ], geo_hgt . shape ) _lons = np . broadcast_to ( lons [ ... , np . newaxis ], geo_hgt . shape ) _lats = np . broadcast_to ( lats [ ... , np . newaxis ], geo_hgt . shape ) # correct for latitude self . _get_heights ( _lats , geo_hgt ) self . _t = temps self . _q = qs self . _p = np . broadcast_to ( pl [ np . newaxis , np . newaxis , :], geo_hgt . shape ) self . _xs = _xs self . _ys = _ys self . _lats = _lats self . _lons = _lons def _makeDataCubes ( self , filename , out = None ): ''' Create a cube of data representing temperature and relative humidity at specified pressure levels ''' if out is None : out = filename # Get profile information from gdal prof = rio_profile ( str ( filename )) # Now get bounds S , N , W , E = self . _ll_bounds # open the dataset and pull the data ds = xarray . open_dataset ( filename , engine = 'cfgrib' , filter_by_keys = { 'typeOfLevel' : 'isobaricInhPa' }) # Determine mask based on query bounds lats = ds [ \"latitude\" ] . to_numpy () lons = ds [ \"longitude\" ] . to_numpy () levels = ds [ \"isobaricInhPa\" ] . to_numpy () shp = lats . shape lons [ lons > 180.0 ] -= 360. m1 = ( S <= lats ) & ( N >= lats ) & \\ ( W <= lons ) & ( E >= lons ) if np . sum ( m1 ) == 0 : raise RuntimeError ( 'Area of Interest has no overlap with the HRRR model available extent' ) # Y extent m1_y = np . argwhere ( np . sum ( m1 , axis = 1 ) != 0 ) y_min = max ( m1_y [ 0 ][ 0 ] - 2 , 0 ) y_max = min ( m1_y [ - 1 ][ 0 ] + 3 , shp [ 0 ]) m1_y = None # X extent m1_x = np . argwhere ( np . sum ( m1 , axis = 0 ) != 0 ) x_min = max ( m1_x [ 0 ][ 0 ] - 2 , 0 ) x_max = min ( m1_x [ - 1 ][ 0 ] + 3 , shp [ 1 ]) m1_x = None m1 = None # Coordinate arrays # HRRR GRIB has data in south-up format trans = prof [ \"transform\" ] . to_gdal () xArr = trans [ 0 ] + ( np . arange ( x_min , x_max ) + 0.5 ) * trans [ 1 ] yArr = trans [ 3 ] + ( prof [ \"height\" ] * trans [ 5 ]) - ( np . arange ( y_min , y_max ) + 0.5 ) * trans [ 5 ] lats = lats [ y_min : y_max , x_min : x_max ] lons = lons [ y_min : y_max , x_min : x_max ] # Data variables t = ds [ 't' ][:, y_min : y_max , x_min : x_max ] . to_numpy () z = ds [ 'gh' ][:, y_min : y_max , x_min : x_max ] . to_numpy () q = ds [ 'q' ][:, y_min : y_max , x_min : x_max ] . to_numpy () ds . close () # This section is purely for flipping arrays as needed # to match ECMWF reader is doing # All flips are views - no extra memory use # Lon -> From west to east # Lat -> From south to north (ECMWF reads north to south and flips it # load_weather) - So we do south to north here # Pres -> High to Los - (ECWMF does now to high and flips it back) - so # we do high to low # Data is currently in [levels, y, x] order flip_axes = [] if levels [ - 1 ] > levels [ 0 ]: flip_axes . append ( 0 ) levels = np . flip ( levels ) if lats [ 0 , 0 ] > lats [ - 1 , 0 ]: flip_axes . append ( 1 ) lats = np . flip ( lats , 0 ) yArr = np . flip ( yArr ) if lons [ 0 , 0 ] > lons [ 0 , - 1 ]: flip_axes . append ( 2 ) lons = np . flip ( lons , 1 ) xArr = np . flip ( xArr ) flip_axes = tuple ( flip_axes ) if flip_axes : t = np . flip ( t , flip_axes ) z = np . flip ( z , flip_axes ) q = np . flip ( q , flip_axes ) # Create output dataset ds_new = xarray . Dataset ( data_vars = dict ( t = ([ \"level\" , \"y\" , \"x\" ], t , { \"grid_mapping\" : \"proj\" }), z = ([ \"level\" , \"y\" , \"x\" ], z , { \"grid_mapping\" : \"proj\" }), q = ([ \"level\" , \"y\" , \"x\" ], q , { \"grid_mapping\" : \"proj\" }), lats = ([ \"y\" , \"x\" ], lats ), lons = ([ \"y\" , \"x\" ], lons ), ), coords = dict ( levels = ([ \"level\" ], levels , { \"units\" : \"millibars\" , \"long_name\" : \"pressure_level\" , \"axis\" : \"Z\" }), x = ([ \"x\" ], xArr , { \"standard_name\" : \"projection_x_coordinate\" , \"units\" : \"m\" , \"axis\" : \"X\" }), y = ([ \"y\" ], yArr , { \"standard_name\" : \"projection_y_coordinate\" , \"units\" : \"m\" , \"axis\" : \"Y\" }), ), attrs = { 'Conventions' : 'CF-1.7' , 'Weather_model' : 'HRRR' , } ) # Write projection of output ds_new [ \"proj\" ] = int () for k , v in self . _proj . to_cf () . items (): ds_new . proj . attrs [ k ] = v ds_new . to_netcdf ( out , engine = 'netcdf4' ) def _download_hrrr_file ( self , DATE , out , model = 'hrrr' , product = 'prs' , fxx = 0 , verbose = False ): ''' Download a HRRR model ''' H = Herbie ( DATE . strftime ( '%Y-%m- %d %H:%M' ), model = model , product = product , overwrite = False , verbose = True , save_dir = Path ( os . path . dirname ( out )), ) pf = H . download ( \":(SPFH|PRES|TMP|HGT):\" , verbose = verbose ) self . _makeDataCubes ( pf , out ) return out load_weather ( * args , filename = None , ** kwargs ) Load a weather model into a python weatherModel object, from self.files if no filename is passed. Source code in RAiDER/models/hrrr.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def load_weather ( self , * args , filename = None , ** kwargs ): ''' Load a weather model into a python weatherModel object, from self.files if no filename is passed. ''' if filename is None : filename = self . files # read data from grib file try : ds = xarray . open_dataset ( filename , engine = 'cfgrib' ) except EOFError : ds = xarray . open_dataset ( filename , engine = 'netcdf4' ) pl = np . array ([ self . _convertmb2Pa ( p ) for p in ds . levels . values ]) xArr = ds [ 'x' ] . values yArr = ds [ 'y' ] . values lats = ds [ 'lats' ] . values lons = ds [ 'lons' ] . values temps = ds [ 't' ] . values . transpose ( 1 , 2 , 0 ) qs = ds [ 'q' ] . values . transpose ( 1 , 2 , 0 ) geo_hgt = ds [ 'z' ] . values . transpose ( 1 , 2 , 0 ) Ny , Nx = lats . shape lons [ lons > 180 ] -= 360 # data cube format should be lats,lons,heights _xs = np . broadcast_to ( xArr [ np . newaxis , :, np . newaxis ], geo_hgt . shape ) _ys = np . broadcast_to ( yArr [:, np . newaxis , np . newaxis ], geo_hgt . shape ) _lons = np . broadcast_to ( lons [ ... , np . newaxis ], geo_hgt . shape ) _lats = np . broadcast_to ( lats [ ... , np . newaxis ], geo_hgt . shape ) # correct for latitude self . _get_heights ( _lats , geo_hgt ) self . _t = temps self . _q = qs self . _p = np . broadcast_to ( pl [ np . newaxis , np . newaxis , :], geo_hgt . shape ) self . _xs = _xs self . _ys = _ys self . _lats = _lats self . _lons = _lons merra2 MERRA2 Bases: WeatherModel Source code in RAiDER/models/merra2.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 class MERRA2 ( WeatherModel ): # I took this from MERRA-2 model level weblink # https://goldsmr5.gesdisc.eosdis.nasa.gov:443/opendap/MERRA2/M2I3NVASM.5.12.4/ def __init__ ( self ): import calendar # initialize a weather model WeatherModel . __init__ ( self ) self . _humidityType = 'q' self . _model_level_type = 'ml' # Default, pressure levels are 'pl' self . _classname = 'merra2' self . _dataset = 'merra2' # Tuple of min/max years where data is available. utcnow = dt . datetime . utcnow () enddate = dt . datetime ( utcnow . year , utcnow . month , 15 ) - dt . timedelta ( days = 60 ) enddate = dt . datetime ( enddate . year , enddate . month , calendar . monthrange ( enddate . year , enddate . month )[ 1 ]) self . _valid_range = ( dt . datetime ( 1980 , 1 , 1 ), \"Present\" ) lag_time = utcnow - enddate self . _lag_time = dt . timedelta ( days = lag_time . days ) # Availability lag time in days self . _time_res = 1 # model constants self . _k1 = 0.776 # [K/Pa] self . _k2 = 0.233 # [K/Pa] self . _k3 = 3.75e3 # [K^2/Pa] # horizontal grid spacing self . _lat_res = 0.5 self . _lon_res = 0.625 self . _x_res = 0.625 self . _y_res = 0.5 self . _Name = 'MERRA2' self . files = None self . _bounds = None self . _zlevels = np . flipud ( LEVELS_137_HEIGHTS ) # Projection self . _proj = CRS . from_epsg ( 4326 ) def _fetch ( self , out ): ''' Fetch weather model data from GMAO: note we only extract the lat/lon bounds for this weather model; fetching data is not needed here as we don't actually download any data using OpenDAP ''' time = self . _time # check whether the file already exists if os . path . exists ( out ): return # calculate the array indices for slicing the GMAO variable arrays lat_min_ind = int (( self . _ll_bounds [ 0 ] - ( - 90.0 )) / self . _lat_res ) lat_max_ind = int (( self . _ll_bounds [ 1 ] - ( - 90.0 )) / self . _lat_res ) lon_min_ind = int (( self . _ll_bounds [ 2 ] - ( - 180.0 )) / self . _lon_res ) lon_max_ind = int (( self . _ll_bounds [ 3 ] - ( - 180.0 )) / self . _lon_res ) lats = np . arange ( ( - 90 + lat_min_ind * self . _lat_res ), ( - 90 + ( lat_max_ind + 1 ) * self . _lat_res ), self . _lat_res ) lons = np . arange ( ( - 180 + lon_min_ind * self . _lon_res ), ( - 180 + ( lon_max_ind + 1 ) * self . _lon_res ), self . _lon_res ) if time . year < 1992 : url_sub = 100 elif time . year < 2001 : url_sub = 200 elif time . year < 2011 : url_sub = 300 else : url_sub = 400 T0 = dt . datetime ( time . year , time . month , time . day , 0 , 0 , 0 ) DT = time - T0 time_ind = int ( DT . total_seconds () / 3600.0 / 3.0 ) ml_min = 0 ml_max = 71 # Earthdata credentials earthdata_usr , earthdata_pwd = read_EarthData_loginInfo ( EARTHDATA_RC ) # open the dataset and pull the data url = 'https://goldsmr5.gesdisc.eosdis.nasa.gov:443/opendap/MERRA2/M2I3NVASM.5.12.4/' + time . strftime ( '%Y/%m' ) + '/MERRA2_' + str ( url_sub ) + '.inst3_3d_asm_Nv.' + time . strftime ( '%Y%m %d ' ) + '.nc4' session = pydap . cas . urs . setup_session ( earthdata_usr , earthdata_pwd , check_url = url ) ds = pydap . client . open_url ( url , session = session ) ############# The MERRA-2 server changes the pydap data retrieval format frequently between these two formats; so better to retain both of them rather than only using either one of them ############# try : q = ds [ 'QV' ] . data [ 0 ][ time_ind , ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )][ 0 ] p = ds [ 'PL' ] . data [ 0 ][ time_ind , ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )][ 0 ] t = ds [ 'T' ] . data [ 0 ][ time_ind , ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )][ 0 ] h = ds [ 'H' ] . data [ 0 ][ time_ind , ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )][ 0 ] except IndexError : q = ds [ 'QV' ] . data [ time_ind , ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )][ 0 ] p = ds [ 'PL' ] . data [ time_ind , ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )][ 0 ] t = ds [ 'T' ] . data [ time_ind , ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )][ 0 ] h = ds [ 'H' ] . data [ time_ind , ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )][ 0 ] except AttributeError : q = ds [ 'QV' ][ time_ind , ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )][ 0 ] p = ds [ 'PL' ][ time_ind , ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )][ 0 ] t = ds [ 'T' ][ time_ind , ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )][ 0 ] h = ds [ 'H' ][ time_ind , ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )][ 0 ] except BaseException : logger . exception ( \"MERRA-2: Unable to read weathermodel data\" ) ######################################################################################################################## try : writeWeatherVars2NETCDF4 ( self , lats , lons , h , q , p , t , outName = out ) except Exception as e : logger . debug ( e ) logger . exception ( \"MERRA-2: Unable to save weathermodel to file\" ) raise RuntimeError ( 'MERRA-2 failed with the following error: {} ' . format ( e )) def load_weather ( self , * args , f = None , ** kwargs ): ''' Consistent class method to be implemented across all weather model types. As a result of calling this method, all of the variables (x, y, z, p, q, t, wet_refractivity, hydrostatic refractivity, e) should be fully populated. ''' if f is None : f = self . files [ 0 ] self . _load_model_level ( f ) def _load_model_level ( self , filename ): ''' Get the variables from the GMAO link using OpenDAP ''' # adding the import here should become absolute when transition to netcdf from netCDF4 import Dataset with Dataset ( filename , mode = 'r' ) as f : lons = np . array ( f . variables [ 'x' ][:]) lats = np . array ( f . variables [ 'y' ][:]) h = np . array ( f . variables [ 'H' ][:]) q = np . array ( f . variables [ 'QV' ][:]) p = np . array ( f . variables [ 'PL' ][:]) t = np . array ( f . variables [ 'T' ][:]) # restructure the 3-D lat/lon/h in regular grid _lons = np . broadcast_to ( lons [ np . newaxis , np . newaxis , :], t . shape ) _lats = np . broadcast_to ( lats [ np . newaxis , :, np . newaxis ], t . shape ) # Re-structure everything from (heights, lats, lons) to (lons, lats, heights) p = np . transpose ( p ) q = np . transpose ( q ) t = np . transpose ( t ) h = np . transpose ( h ) _lats = np . transpose ( _lats ) _lons = np . transpose ( _lons ) # check this # data cube format should be lats,lons,heights p = p . swapaxes ( 0 , 1 ) q = q . swapaxes ( 0 , 1 ) t = t . swapaxes ( 0 , 1 ) h = h . swapaxes ( 0 , 1 ) _lats = _lats . swapaxes ( 0 , 1 ) _lons = _lons . swapaxes ( 0 , 1 ) # For some reason z is opposite the others p = np . flip ( p , axis = 2 ) q = np . flip ( q , axis = 2 ) t = np . flip ( t , axis = 2 ) h = np . flip ( h , axis = 2 ) _lats = np . flip ( _lats , axis = 2 ) _lons = np . flip ( _lons , axis = 2 ) # assign the regular-grid (lat/lon/h) variables self . _p = p self . _q = q self . _t = t self . _lats = _lats self . _lons = _lons self . _xs = _lons self . _ys = _lats self . _zs = h load_weather ( * args , f = None , ** kwargs ) Consistent class method to be implemented across all weather model types. As a result of calling this method, all of the variables (x, y, z, p, q, t, wet_refractivity, hydrostatic refractivity, e) should be fully populated. Source code in RAiDER/models/merra2.py 148 149 150 151 152 153 154 155 156 157 def load_weather ( self , * args , f = None , ** kwargs ): ''' Consistent class method to be implemented across all weather model types. As a result of calling this method, all of the variables (x, y, z, p, q, t, wet_refractivity, hydrostatic refractivity, e) should be fully populated. ''' if f is None : f = self . files [ 0 ] self . _load_model_level ( f ) model_levels Pre-defined model levels and a, b constants for the different weather models NOTE : The fixed heights used here are from ECMWF's geometric altitudes (https://www.ecmwf.int/en/forecasts/documentation-and-support/137-model-levels), where \"geopotential altitude is calculated from a mathematical model that adjusts the altitude to include the variation of gravity with height, while geometric altitude is the standard direct vertical distance above mean sea level (MSL).\" - Wikipedia.org, https://en.wikipedia.org/wiki/International_Standard_Atmosphere ncmr Created on Wed Sep 9 10:26:44 2020 @author: prashant Modified by Yang Lei, GPS/Caltech NCMR Bases: WeatherModel Implement NCMRWF NCUM (named as NCMR) model in future Source code in RAiDER/models/ncmr.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 class NCMR ( WeatherModel ): ''' Implement NCMRWF NCUM (named as NCMR) model in future ''' def __init__ ( self ): # initialize a weather model WeatherModel . __init__ ( self ) self . _humidityType = 'q' # q for specific humidity and rh for relative humidity self . _model_level_type = 'ml' # Default, pressure levels are 'pl', and model levels are \"ml\" self . _classname = 'ncmr' # name of the custom weather model self . _dataset = 'ncmr' # same name as above self . _Name = 'NCMR' # name of the new weather model (in Capital) # Tuple of min/max years where data is available. self . _valid_range = ( datetime . datetime ( 2015 , 12 , 1 ), \"Present\" ) # Availability lag time in days/hours self . _lag_time = datetime . timedelta ( hours = 6 ) self . _time_res = 1 # model constants self . _k1 = 0.776 # [K/Pa] self . _k2 = 0.233 # [K/Pa] self . _k3 = 3.75e3 # [K^2/Pa] # horizontal grid spacing self . _lon_res = .17578125 # grid spacing in longitude self . _lat_res = .11718750 # grid spacing in latitude self . _x_res = .17578125 # same as longitude self . _y_res = .11718750 # same as latitude self . _zlevels = np . flipud ( LEVELS_137_HEIGHTS ) self . _bounds = None # Projection self . _proj = CRS . from_epsg ( 4326 ) def _fetch ( self , out ): ''' Fetch weather model data from NCMR: note we only extract the lat/lon bounds for this weather model; fetching data is not needed here as we don't actually download data , data exist in same system ''' time = self . _time # Auxillary function: ''' download data of the NCMR model and save it in desired location ''' self . _files = self . _download_ncmr_file ( out , time , self . _ll_bounds ) def load_weather ( self , filename = None , * args , ** kwargs ): ''' Load NCMR model variables from existing file ''' if filename is None : filename = self . files [ 0 ] # bounding box plus a buffer lat_min , lat_max , lon_min , lon_max = self . _ll_bounds self . _bounds = ( lat_min , lat_max , lon_min , lon_max ) self . _makeDataCubes ( filename ) def _download_ncmr_file ( self , out , date_time , bounding_box ): ''' Download weather model data (whole globe) from NCMR weblink, crop it to the region of interest, and save the cropped data as a standard .nc file of RAiDER (e.g. \"NCMR_YYYY_MM_DD_THH_MM_SS.nc\"); Temporarily download data from NCMR ftp 'https://ftp.ncmrwf.gov.in/pub/outgoing/SAC/NCUM_OSF/' and copied in weather_models folder ''' from netCDF4 import Dataset ############# Use these lines and modify the link when actually downloading NCMR data from a weblink ############# url , username , password = read_NCMR_loginInfo () filename = os . path . basename ( out ) url = f 'ftp:// { username } : { password } @ { url } /TEST/ { filename } ' filepath = f ' { out [: - 3 ] } _raw.nc' if not os . path . exists ( filepath ): logger . info ( 'Fetching URL: %s ' , url ) local_filename , headers = urllib . request . urlretrieve ( url , filepath , show_progress ) else : logger . warning ( 'Weather model already exists, skipping download' ) ######################################################################################################################## ############# For debugging: use pre-downloaded files; Remove/comment out it when actually downloading NCMR data from a weblink ############# # filepath = os.path.dirname(out) + '/NCUM_ana_mdllev_20180701_00z.nc' ######################################################################################################################## # calculate the array indices for slicing the GMAO variable arrays lat_min_ind = int (( self . _bounds [ 0 ] - ( - 89.94141 )) / self . _lat_res ) lat_max_ind = int (( self . _bounds [ 1 ] - ( - 89.94141 )) / self . _lat_res ) if ( self . _bounds [ 2 ] < 0.0 ): lon_min_ind = int (( self . _bounds [ 2 ] + 360.0 - ( 0.087890625 )) / self . _lon_res ) else : lon_min_ind = int (( self . _bounds [ 2 ] - ( 0.087890625 )) / self . _lon_res ) if ( self . _bounds [ 3 ] < 0.0 ): lon_max_ind = int (( self . _bounds [ 3 ] + 360.0 - ( 0.087890625 )) / self . _lon_res ) else : lon_max_ind = int (( self . _bounds [ 3 ] - ( 0.087890625 )) / self . _lon_res ) ml_min = 0 ml_max = 70 with Dataset ( filepath , 'r' , maskandscale = True ) as f : lats = f . variables [ 'latitude' ][ lat_min_ind :( lat_max_ind + 1 )] . copy () if ( self . _bounds [ 2 ] * self . _bounds [ 3 ] < 0 ): lons1 = f . variables [ 'longitude' ][ lon_min_ind :] . copy () lons2 = f . variables [ 'longitude' ][ 0 :( lon_max_ind + 1 )] . copy () lons = np . append ( lons1 , lons2 ) else : lons = f . variables [ 'longitude' ][ lon_min_ind :( lon_max_ind + 1 )] . copy () if ( self . _bounds [ 2 ] * self . _bounds [ 3 ] < 0 ): t1 = f . variables [ 'air_temperature' ][ ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :] . copy () t2 = f . variables [ 'air_temperature' ][ ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), 0 :( lon_max_ind + 1 )] . copy () t = np . append ( t1 , t2 , axis = 2 ) else : t = f . variables [ 'air_temperature' ][ ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )] . copy () # Skipping first pressure levels (below 20 meter) if ( self . _bounds [ 2 ] * self . _bounds [ 3 ] < 0 ): q1 = f . variables [ 'specific_humidity' ][( ml_min + 1 ):( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :] . copy () q2 = f . variables [ 'specific_humidity' ][( ml_min + 1 ):( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), 0 :( lon_max_ind + 1 )] . copy () q = np . append ( q1 , q2 , axis = 2 ) else : q = f . variables [ 'specific_humidity' ][( ml_min + 1 ):( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )] . copy () if ( self . _bounds [ 2 ] * self . _bounds [ 3 ] < 0 ): p1 = f . variables [ 'air_pressure' ][( ml_min + 1 ):( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :] . copy () p2 = f . variables [ 'air_pressure' ][( ml_min + 1 ):( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), 0 :( lon_max_ind + 1 )] . copy () p = np . append ( p1 , p2 , axis = 2 ) else : p = f . variables [ 'air_pressure' ][( ml_min + 1 ):( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )] . copy () level_hgt = f . variables [ 'level_height' ][( ml_min + 1 ):( ml_max + 1 )] . copy () if ( self . _bounds [ 2 ] * self . _bounds [ 3 ] < 0 ): surface_alt1 = f . variables [ 'surface_altitude' ][ lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :] . copy () surface_alt2 = f . variables [ 'surface_altitude' ][ lat_min_ind :( lat_max_ind + 1 ), 0 :( lon_max_ind + 1 )] . copy () surface_alt = np . append ( surface_alt1 , surface_alt2 , axis = 1 ) else : surface_alt = f . variables [ 'surface_altitude' ][ lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )] . copy () hgt = np . zeros ([ len ( level_hgt ), len ( surface_alt [:, 1 ]), len ( surface_alt [ 1 , :])]) for i in range ( len ( level_hgt )): hgt [ i , :, :] = surface_alt [:, :] + level_hgt [ i ] lons [ lons > 180 ] -= 360 ############# For debugging: comment it out when using pre-downloaded raw data files and don't want to remove them for test; Uncomment it when actually downloading NCMR data from a weblink ############# os . remove ( filepath ) ######################################################################################################################## try : writeWeatherVars2NETCDF4 ( self , lats , lons , hgt , q , p , t , outName = out ) except Exception : logger . exception ( \"Unable to save weathermodel to file\" ) def _makeDataCubes ( self , filename ): ''' Get the variables from the saved .nc file (named as \"NCMR_YYYY_MM_DD_THH_MM_SS.nc\") ''' from netCDF4 import Dataset # adding the import here should become absolute when transition to netcdf with Dataset ( filename , mode = 'r' ) as f : lons = np . array ( f . variables [ 'x' ][:]) lats = np . array ( f . variables [ 'y' ][:]) hgt = np . array ( f . variables [ 'H' ][:]) q = np . array ( f . variables [ 'QV' ][:]) p = np . array ( f . variables [ 'PL' ][:]) t = np . array ( f . variables [ 'T' ][:]) # re-assign lons, lats to match heights _lons = np . broadcast_to ( lons [ np . newaxis , np . newaxis , :], t . shape ) _lats = np . broadcast_to ( lats [ np . newaxis , :, np . newaxis ], t . shape ) # Re-structure everything from (heights, lats, lons) to (lons, lats, heights) _lats = np . transpose ( _lats ) _lons = np . transpose ( _lons ) t = np . transpose ( t ) q = np . transpose ( q ) p = np . transpose ( p ) hgt = np . transpose ( hgt ) # data cube format should be lats,lons,heights p = p . swapaxes ( 0 , 1 ) q = q . swapaxes ( 0 , 1 ) t = t . swapaxes ( 0 , 1 ) hgt = hgt . swapaxes ( 0 , 1 ) _lats = _lats . swapaxes ( 0 , 1 ) _lons = _lons . swapaxes ( 0 , 1 ) # assign the regular-grid variables self . _p = p self . _q = q self . _t = t self . _lats = _lats self . _lons = _lons self . _xs = _lons self . _ys = _lats self . _zs = hgt load_weather ( filename = None , * args , ** kwargs ) Load NCMR model variables from existing file Source code in RAiDER/models/ncmr.py 78 79 80 81 82 83 84 85 86 87 88 89 def load_weather ( self , filename = None , * args , ** kwargs ): ''' Load NCMR model variables from existing file ''' if filename is None : filename = self . files [ 0 ] # bounding box plus a buffer lat_min , lat_max , lon_min , lon_max = self . _ll_bounds self . _bounds = ( lat_min , lat_max , lon_min , lon_max ) self . _makeDataCubes ( filename ) plotWeather This set of functions is designed to for plotting WeatherModel class objects. It is not designed to be used on its own apart from this class. plot_pqt ( weatherObj , savefig = True , z1 = 500 , z2 = 15000 ) Create a plot with pressure, temp, and humidity at two heights Source code in RAiDER/models/plotWeather.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def plot_pqt ( weatherObj , savefig = True , z1 = 500 , z2 = 15000 ): ''' Create a plot with pressure, temp, and humidity at two heights ''' # Get the interpolator intFcn_p = Interpolator (( weatherObj . _xs , weatherObj . _ys , weatherObj . _zs ), weatherObj . _p . swapaxes ( 0 , 1 )) intFcn_e = Interpolator (( weatherObj . _xs , weatherObj . _ys , weatherObj . _zs ), weatherObj . _e . swapaxes ( 0 , 1 )) intFcn_t = Interpolator (( weatherObj . _xs , weatherObj . _ys , weatherObj . _zs ), weatherObj . _t . swapaxes ( 0 , 1 )) # get the points needed XY = np . meshgrid ( weatherObj . _xs , weatherObj . _ys ) x = XY [ 0 ] y = XY [ 1 ] z1a = np . zeros ( x . shape ) + z1 z2a = np . zeros ( x . shape ) + z2 pts1 = np . stack (( x . flatten (), y . flatten (), z1a . flatten ()), axis = 1 ) pts2 = np . stack (( x . flatten (), y . flatten (), z2a . flatten ()), axis = 1 ) p1 = intFcn_p ( pts1 ) e1 = intFcn_e ( pts1 ) t1 = intFcn_t ( pts1 ) p2 = intFcn_p ( pts2 ) e2 = intFcn_e ( pts2 ) t2 = intFcn_t ( pts2 ) # Now get the data to plot plots = [ p1 / 1e2 , e1 / 1e2 , t1 - 273.15 , p2 / 1e2 , e2 / 1e2 , t2 - 273.15 ] # titles = ('P (hPa)', 'E (hPa)'.format(z1), 'T (C)', '', '', '') titles = ( 'P (hPa)' , 'E (hPa)' , 'T (C)' , '' , '' , '' ) # setup the plot f = plt . figure ( figsize = ( 18 , 14 )) f . suptitle ( ' {} Pressure/Humidity/Temperature at height {} m and {} m (values should drop as elevation increases)' . format ( weatherObj . _Name , z1 , z2 )) xind = int ( np . floor ( weatherObj . _xs . shape [ 0 ] / 2 )) yind = int ( np . floor ( weatherObj . _ys . shape [ 0 ] / 2 )) # loop over each plot for ind , plot , title in zip ( range ( len ( plots )), plots , titles ): sp = f . add_subplot ( 3 , 3 , ind + 1 ) im = sp . imshow ( np . reshape ( plot , x . shape ), cmap = 'viridis' , extent = [ np . nanmin ( x ), np . nanmax ( x ), np . nanmin ( y ), np . nanmax ( y )], origin = 'lower' ) sp . plot ( x [ yind , xind ], y [ yind , xind ], 'ko' ) divider = mal ( sp ) cax = divider . append_axes ( \"right\" , size = \"4%\" , pad = 0.05 ) plt . colorbar ( im , cax = cax ) sp . set_title ( title ) if ind == 0 : sp . set_ylabel ( ' {} m \\n ' . format ( z1 )) if ind == 3 : sp . set_ylabel ( ' {} m \\n ' . format ( z2 )) # add plots that show each variable with height zdata = weatherObj . _zs [:] / 1000 sp = f . add_subplot ( 3 , 3 , 7 ) sp . plot ( weatherObj . _p [ yind , xind , :] / 1e2 , zdata ) sp . set_ylabel ( 'Height (km)' ) sp . set_xlabel ( 'Pressure (hPa)' ) sp = f . add_subplot ( 3 , 3 , 8 ) sp . plot ( weatherObj . _e [ yind , xind , :] / 100 , zdata ) sp . set_xlabel ( 'E (hPa)' ) sp = f . add_subplot ( 3 , 3 , 9 ) sp . plot ( weatherObj . _t [ yind , xind , :] - 273.15 , zdata ) sp . set_xlabel ( 'Temp (C)' ) plt . subplots_adjust ( top = 0.95 , bottom = 0.1 , left = 0.1 , right = 0.95 , hspace = 0.2 , wspace = 0.3 ) if savefig : plt . savefig ( ' {} _weather_hgt {} _and_ {} m.pdf' . format ( weatherObj . _Name , z1 , z2 )) return f plot_wh ( weatherObj , savefig = True , z1 = 500 , z2 = 15000 ) Create a plot with wet refractivity and hydrostatic refractivity, at two different heights Source code in RAiDER/models/plotWeather.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def plot_wh ( weatherObj , savefig = True , z1 = 500 , z2 = 15000 ): ''' Create a plot with wet refractivity and hydrostatic refractivity, at two different heights ''' # Get the interpolator intFcn_w = Interpolator (( weatherObj . _xs , weatherObj . _ys , weatherObj . _zs ), weatherObj . _wet_refractivity . swapaxes ( 0 , 1 )) intFcn_h = Interpolator (( weatherObj . _xs , weatherObj . _ys , weatherObj . _zs ), weatherObj . _hydrostatic_refractivity . swapaxes ( 0 , 1 )) # get the points needed XY = np . meshgrid ( weatherObj . _xs , weatherObj . _ys ) x = XY [ 0 ] y = XY [ 1 ] z1a = np . zeros ( x . shape ) + z1 z2a = np . zeros ( x . shape ) + z2 pts1 = np . stack (( x . flatten (), y . flatten (), z1a . flatten ()), axis = 1 ) pts2 = np . stack (( x . flatten (), y . flatten (), z2a . flatten ()), axis = 1 ) w1 = intFcn_w ( pts1 ) h1 = intFcn_h ( pts1 ) w2 = intFcn_w ( pts2 ) h2 = intFcn_h ( pts2 ) # Now get the data to plot plots = [ w1 , h1 , w2 , h2 ] # titles titles = ( 'Wet refractivity {} ' . format ( z1 ), 'Hydrostatic refractivity {} ' . format ( z1 ), ' {} ' . format ( z2 ), ' {} ' . format ( z2 )) # setup the plot f = plt . figure ( figsize = ( 14 , 10 )) f . suptitle ( ' {} Wet and Hydrostatic refractivity at height {} m and {} m' . format ( weatherObj . _Name , z1 , z2 )) # loop over each plot for ind , plot , title in zip ( range ( len ( plots )), plots , titles ): sp = f . add_subplot ( 2 , 2 , ind + 1 ) im = sp . imshow ( np . reshape ( plot , x . shape ), cmap = 'viridis' , extent = [ np . nanmin ( x ), np . nanmax ( x ), np . nanmin ( y ), np . nanmax ( y )], origin = 'lower' ) divider = mal ( sp ) cax = divider . append_axes ( \"right\" , size = \"4%\" , pad = 0.05 ) plt . colorbar ( im , cax = cax ) sp . set_title ( title ) if ind == 0 : sp . set_ylabel ( ' {} m \\n ' . format ( z1 )) if ind == 2 : sp . set_ylabel ( ' {} m \\n ' . format ( z2 )) if savefig : plt . savefig ( ' {} _refractivity_hgt {} _and_ {} m.pdf' . format ( weatherObj . _Name , z1 , z2 )) return f template customModelReader Bases: WeatherModel Source code in RAiDER/models/template.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 class customModelReader ( WeatherModel ): def __init__ ( self ): WeatherModel . __init__ ( self ) self . _humidityType = 'q' # can be \"q\" (specific humidity) or \"rh\" (relative humidity) self . _model_level_type = 'pl' # Default, pressure levels are \"pl\", and model levels are \"ml\" self . _classname = 'abcd' # name of the custom weather model self . _dataset = 'abcd' # same name as above # Tuple of min/max years where data is available. # valid range of the dataset. Users need to specify the start date and end date (can be \"present\") self . _valid_range = ( datetime . datetime ( 2016 , 7 , 15 ), \"Present\" ) # Availability lag time. Can be specified in hours \"hours=3\" or in days \"days=3\" self . _lag_time = datetime . timedelta ( hours = 3 ) # Availabile time resolution; i.e. minimum rate model is available in hours. 1 is hourly self . _time_res = 1 # model constants (these three constants are borrowed from ECMWF model and currently # set to be default for all other models, which may need to be double checked.) self . _k1 = 0.776 # [K/Pa] self . _k2 = 0.233 # [K/Pa] self . _k3 = 3.75e3 # [K^2/Pa] # horizontal grid spacing self . _lat_res = 3. / 111 # grid spacing in latitude self . _lon_res = 3. / 111 # grid spacing in longitude self . _x_res = 3. # x-direction grid spacing in the native weather model projection # (if the projection is in lat/lon, it is the same as \"self._lon_res\") self . _y_res = 3. # y-direction grid spacing in the weather model native projection # (if the projection is in lat/lon, it is the same as \"self._lat_res\") # zlevels specify fixed heights at which to interpolate the weather model variables self . _zlevels = np . flipud ( LEVELS_137_HEIGHTS ) self . _Name = 'ABCD' # name of the custom weather model (better to be capitalized) # Projections in RAiDER are defined using pyproj (python wrapper around Proj) # If the projection is defined with EPSG code, one can use \"self._proj = CRS.from_epsg(4326)\" # to replace the following lines to get \"self._proj\". # Below we show the example of HRRR model with the parameters of its Lambert Conformal Conic projection lon0 = 262.5 lat0 = 38.5 lat1 = 38.5 lat2 = 38.5 x0 = 0 y0 = 0 earth_radius = 6371229 p1 = CRS ( '+proj=lcc +lat_1= {lat1} +lat_2= {lat2} +lat_0= {lat0} +lon_0= {lon0} +x_0= {x0} +y_0= {y0} +a= {a} +b= {a} +units=m +no_defs' . format ( lat1 = lat1 , lat2 = lat2 , lat0 = lat0 , lon0 = lon0 , x0 = x0 , y0 = y0 , a = earth_radius )) self . _proj = p1 def _fetch ( self , out ): ''' Fetch weather model data from the custom weather model \"ABCD\" Inputs (no need to change in the custom weather model reader): lats - latitude lons - longitude time - datatime object (year,month,day,hour,minute,second) out - name of downloaded dataset file from the custom weather model server Nextra - buffer of latitude/longitude for determining the bounding box ''' # Auxilliary function: # download dataset of the custom weather model \"ABCD\" from a server and then save it to a file named out. # This function needs to be writen by the users. For download from the weather model server, the weather model # name, time and bounding box may be needed to retrieve the dataset; for cases where no file is actually # downloaded, e.g. the GMAO and MERRA-2 models using OpenDAP, this function can be omitted leaving the data # retrieval to the following \"load_weather\" function. self . _files = self . _download_abcd_file ( out , 'abcd' , self . _time , self . _ll_bounds ) def load_weather ( self , filename ): ''' Load weather model variables from the downloaded file named filename Inputs: filename - filename of the downloaded weather model file ''' # Auxilliary function: # read individual variables (in 3-D cube format with exactly the same dimension) from downloaded file # This function needs to be writen by the users. For downloaded file from the weather model server, # this function extracts the individual variables from the saved file named filename; # for cases where no file is actually downloaded, e.g. the GMAO and MERRA-2 models using OpenDAP, # this function retrieves the individual variables directly from the weblink of the weather model. lats , lons , xs , ys , t , q , p , hgt = self . _makeDataCubes ( filename ) # extra steps that may be needed to calculate topographic height and pressure level if not provided # directly by the weather model through the above auxilliary function \"self._makeDataCubes\" # if surface pressure (in logarithm) is provided as \"p\" along with the surface geopotential \"z\" (needs to be # added to the auxilliary function \"self._makeDataCubes\"), one can use the following line to convert to # geopotential, pressure level and geopotential height; otherwise commented out z , p , hgt = self . _calculategeoh ( z , p ) # TODO: z is undefined # if the geopotential is provided as \"z\" (needs to be added to the auxilliary function \"self._makeDataCubes\"), # one can use the following line to convert to geopotential height; otherwise, commented out hgt = z / self . _g0 # if geopotential height is provided/calculated as \"hgt\", one can use the following line to convert to # topographic height, which is then automatically assigned to \"self._zs\"; otherwise commented out self . _get_heights ( lats , hgt ) # if topographic height is provided as \"hgt\", use the following line directly; otherwise commented out self . _zs = hgt ########### ######### output of the weather model reader for delay calculations (all in 3-D data cubes) ######## # _t: temperture # _q: either relative or specific humidity # _p: must be pressure level # _xs: x-direction grid dimension of the native weather model coordinates (if in lat/lon, _xs = _lons) # _ys: y-direction grid dimension of the native weather model coordinates (if in lat/lon, _ys = _lats) # _zs: must be topographic height # _lats: latitude # _lons: longitude self . _t = t self . _q = q self . _p = p self . _xs = xs self . _ys = ys self . _lats = lats self . _lons = lons ########### def _download_abcd_file ( self , out , model_name , date_time , bounding_box ): ''' Auxilliary function: Download weather model data from a server Inputs: out - filename for saving the retrieved data file model_name - name of the custom weather model date_time - datatime object (year,month,day,hour,minute,second) bounding_box - lat/lon bounding box for the region of interest Output: out - returned filename from input ''' pass def _makeDataCubes ( self , filename ): ''' Auxilliary function: Read 3-D data cubes from downloaded file or directly from weather model weblink (in which case, there is no need to download and save any file; rather, the weblink needs to be hardcoded in the custom reader, e.g. GMAO) Input: filename - filename of the downloaded weather model file from the server Outputs: lats - latitude (3-D data cube) lons - longitude (3-D data cube) xs - x-direction grid dimension of the native weather model coordinates (3-D data cube; if in lat/lon, _xs = _lons) ys - y-direction grid dimension of the native weather model coordinates (3-D data cube; if in lat/lon, _ys = _lats) t - temperature (3-D data cube) q - humidity (3-D data cube; could be relative humidity or specific humidity) p - pressure level (3-D data cube; could be pressure level (preferred) or surface pressure) hgt - height (3-D data cube; could be geopotential height or topographic height (preferred)) ''' pass load_weather ( filename ) Load weather model variables from the downloaded file named filename Inputs: filename - filename of the downloaded weather model file Source code in RAiDER/models/template.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def load_weather ( self , filename ): ''' Load weather model variables from the downloaded file named filename Inputs: filename - filename of the downloaded weather model file ''' # Auxilliary function: # read individual variables (in 3-D cube format with exactly the same dimension) from downloaded file # This function needs to be writen by the users. For downloaded file from the weather model server, # this function extracts the individual variables from the saved file named filename; # for cases where no file is actually downloaded, e.g. the GMAO and MERRA-2 models using OpenDAP, # this function retrieves the individual variables directly from the weblink of the weather model. lats , lons , xs , ys , t , q , p , hgt = self . _makeDataCubes ( filename ) # extra steps that may be needed to calculate topographic height and pressure level if not provided # directly by the weather model through the above auxilliary function \"self._makeDataCubes\" # if surface pressure (in logarithm) is provided as \"p\" along with the surface geopotential \"z\" (needs to be # added to the auxilliary function \"self._makeDataCubes\"), one can use the following line to convert to # geopotential, pressure level and geopotential height; otherwise commented out z , p , hgt = self . _calculategeoh ( z , p ) # TODO: z is undefined # if the geopotential is provided as \"z\" (needs to be added to the auxilliary function \"self._makeDataCubes\"), # one can use the following line to convert to geopotential height; otherwise, commented out hgt = z / self . _g0 # if geopotential height is provided/calculated as \"hgt\", one can use the following line to convert to # topographic height, which is then automatically assigned to \"self._zs\"; otherwise commented out self . _get_heights ( lats , hgt ) # if topographic height is provided as \"hgt\", use the following line directly; otherwise commented out self . _zs = hgt ########### ######### output of the weather model reader for delay calculations (all in 3-D data cubes) ######## # _t: temperture # _q: either relative or specific humidity # _p: must be pressure level # _xs: x-direction grid dimension of the native weather model coordinates (if in lat/lon, _xs = _lons) # _ys: y-direction grid dimension of the native weather model coordinates (if in lat/lon, _ys = _lats) # _zs: must be topographic height # _lats: latitude # _lons: longitude self . _t = t self . _q = q self . _p = p self . _xs = xs self . _ys = ys self . _lats = lats self . _lons = lons weatherModel WeatherModel Bases: ABC Implement a generic weather model for getting estimated SAR delays Source code in RAiDER/models/weatherModel.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 class WeatherModel ( ABC ): ''' Implement a generic weather model for getting estimated SAR delays ''' def __init__ ( self ): # Initialize model-specific constants/parameters self . _k1 = None self . _k2 = None self . _k3 = None self . _humidityType = 'q' self . _a = [] self . _b = [] self . files = None self . _time_res = None # time resolution of the weather model in hours self . _lon_res = None self . _lat_res = None self . _x_res = None self . _y_res = None self . _classname = None self . _dataset = None self . _model_level_type = 'ml' self . _valid_range = ( datetime . date ( 1900 , 1 , 1 ), ) # Tuple of min/max years where data is available. self . _lag_time = datetime . timedelta ( days = 30 ) # Availability lag time in days self . _time = None self . _bbox = None # Define fixed constants self . _R_v = 461.524 self . _R_d = 287.06 # in our original code this was 287.053 self . _g0 = _g0 # gravity constant self . _zmin = _ZMIN # minimum integration height self . _zmax = _ZREF # max integration height self . _proj = None # setup data structures self . _levels = [] self . _xs = np . empty (( 1 , 1 , 1 )) # Use generic x/y/z instead of lon/lat/height self . _ys = np . empty (( 1 , 1 , 1 )) self . _zs = np . empty (( 1 , 1 , 1 )) self . _lats = None self . _lons = None self . _ll_bounds = None self . _p = None self . _q = None self . _rh = None self . _t = None self . _e = None self . _wet_refractivity = None self . _hydrostatic_refractivity = None self . _wet_ztd = None self . _hydrostatic_ztd = None def __str__ ( self ): string = ' \\n ' string += '======Weather Model class object===== \\n ' string += 'Weather model time: {} \\n ' . format ( self . _time ) string += 'Latitude resolution: {} \\n ' . format ( self . _lat_res ) string += 'Longitude resolution: {} \\n ' . format ( self . _lon_res ) string += 'Native projection: {} \\n ' . format ( self . _proj ) string += 'ZMIN: {} \\n ' . format ( self . _zmin ) string += 'ZMAX: {} \\n ' . format ( self . _zmax ) string += 'k1 = {} \\n ' . format ( self . _k1 ) string += 'k2 = {} \\n ' . format ( self . _k2 ) string += 'k3 = {} \\n ' . format ( self . _k3 ) string += 'Humidity type = {} \\n ' . format ( self . _humidityType ) string += '===================================== \\n ' string += 'Class name: {} \\n ' . format ( self . _classname ) string += 'Dataset: {} \\n ' . format ( self . _dataset ) string += '===================================== \\n ' string += 'A: {} \\n ' . format ( self . _a ) string += 'B: {} \\n ' . format ( self . _b ) if self . _p is not None : string += 'Number of points in Lon/Lat = {} / {} \\n ' . format ( * self . _p . shape [: 2 ]) string += 'Total number of grid points (3D): {} \\n ' . format ( np . prod ( self . _p . shape )) if self . _xs . size == 0 : string += 'Minimum/Maximum y: {: 4.2f} / {: 4.2f} \\n ' \\ . format ( robmin ( self . _ys ), robmax ( self . _ys )) string += 'Minimum/Maximum x: {: 4.2f} / {: 4.2f} \\n ' \\ . format ( robmin ( self . _xs ), robmax ( self . _xs )) string += 'Minimum/Maximum zs/heights: {: 10.2f} / {: 10.2f} \\n ' \\ . format ( robmin ( self . _zs ), robmax ( self . _zs )) string += '===================================== \\n ' return str ( string ) def Model ( self ): return self . _Name def fetch ( self , out , ll_bounds , time ): ''' Checks the input datetime against the valid date range for the model and then calls the model _fetch routine Args: ---------- out - ll_bounds - 4 x 1 array, SNWE time = UTC datetime ''' self . checkTime ( time ) self . set_latlon_bounds ( ll_bounds ) self . setTime ( time ) self . _fetch ( out ) @abstractmethod def _fetch ( self , out ): ''' Placeholder method. Should be implemented in each weather model type class ''' pass def setTime ( self , time , fmt = '%Y-%m- %d T%H:%M:%S' ): ''' Set the time for a weather model ''' if isinstance ( time , str ): self . _time = datetime . datetime . strptime ( time , fmt ) elif isinstance ( time , datetime . datetime ): self . _time = time else : raise ValueError ( '\"time\" must be a string or a datetime object' ) def get_latlon_bounds ( self ): raise NotImplementedError def set_latlon_bounds ( self , ll_bounds , Nextra = 2 ): ''' Need to correct lat/lon bounds because not all of the weather models have valid data exactly bounded by -90/90 (lats) and -180/180 (lons); for GMAO and MERRA2, need to adjust the longitude higher end with an extra buffer; for other models, the exact bounds are close to -90/90 (lats) and -180/180 (lons) and thus can be rounded to the above regions (either in the downloading-file API or subsetting- data API) without problems. ''' ex_buffer_lon_max = 0.0 if self . _Name == 'GMAO' or self . _Name == 'MERRA2' : ex_buffer_lon_max = self . _lon_res elif self . _Name == 'HRRR' : Nextra = 6 # have a bigger buffer # At boundary lats and lons, need to modify Nextra buffer so that the lats and lons do not exceed the boundary S , N , W , E = ll_bounds # Adjust bounds if they get near the poles or IDL S = np . max ([ S - Nextra * self . _lat_res , - 90.0 + Nextra * self . _lat_res ]) N = np . min ([ N + Nextra * self . _lat_res , 90.0 - Nextra * self . _lat_res ]) W = np . max ([ W - Nextra * self . _lon_res , - 180.0 + Nextra * self . _lon_res ]) E = np . min ([ E + Nextra * self . _lon_res + ex_buffer_lon_max , 180.0 - Nextra * self . _lon_res - ex_buffer_lon_max ]) self . _ll_bounds = np . array ([ S , N , W , E ]) def load ( self , outLoc , * args , ll_bounds = None , _zlevels = None , zref = _ZREF , ** kwargs ): ''' Calls the load_weather method. Each model class should define a load_weather method appropriate for that class. 'args' should be one or more filenames. ''' self . set_latlon_bounds ( ll_bounds ) # If the weather file has already been processed, do nothing self . _out_name = self . out_file ( outLoc ) if os . path . exists ( self . _out_name ): return self . _out_name else : # Load the weather just for the query points self . load_weather ( * args , ** kwargs ) # Process the weather model data self . _find_e () self . _uniform_in_z ( _zlevels = _zlevels ) self . _checkForNans () self . _get_wet_refractivity () self . _get_hydro_refractivity () self . _adjust_grid ( ll_bounds ) # Compute Zenith delays at the weather model grid nodes self . _getZTD ( zref ) return None @abstractmethod def load_weather ( self , * args , ** kwargs ): ''' Placeholder method. Should be implemented in each weather model type class ''' pass def _get_time ( self , filename = None ): if filename is None : filename = self . files [ 0 ] with netCDF4 . Dataset ( filename , mode = 'r' ) as f : time = f . attrs [ 'datetime' ] . copy () self . time = datetime . datetime . strptime ( time , \"%Y_%m_ %d T%H_%M_%S\" ) def plot ( self , plotType = 'pqt' , savefig = True ): ''' Plotting method. Valid plot types are 'pqt' ''' if plotType == 'pqt' : plot = plots . plot_pqt ( self , savefig ) elif plotType == 'wh' : plot = plots . plot_wh ( self , savefig ) else : raise RuntimeError ( 'WeatherModel.plot: No plotType named {} ' . format ( plotType )) return plot def checkTime ( self , time ): ''' Checks the time against the lag time and valid date range for the given model type ''' logger . info ( 'Weather model %s is available from %s - %s ' , self . Model (), self . _valid_range [ 0 ], self . _valid_range [ 1 ] ) if time < self . _valid_range [ 0 ]: raise RuntimeError ( \"Weather model {} is not available at {} \" . format ( self . Model (), time )) if self . _valid_range [ 1 ] is not None : if self . _valid_range [ 1 ] == 'Present' : pass elif self . _valid_range [ 1 ] < time : raise RuntimeError ( \"Weather model {} is not available at {} \" . format ( self . Model (), time )) if time > datetime . datetime . utcnow () - self . _lag_time : raise RuntimeError ( \"Weather model {} is not available at {} \" . format ( self . Model (), time )) def _convertmb2Pa ( self , pres ): ''' Convert pressure in millibars to Pascals ''' return 100 * pres def _get_heights ( self , lats , geo_hgt , geo_ht_fill = np . nan ): ''' Transform geo heights to actual heights ''' geo_ht_fix = np . where ( geo_hgt != geo_ht_fill , geo_hgt , np . nan ) self . _zs = util . _geo_to_ht ( lats , geo_ht_fix ) def _find_e ( self ): \"\"\"Check the type of e-calculation needed\"\"\" if self . _humidityType == 'rh' : self . _find_e_from_rh () elif self . _humidityType == 'q' : self . _find_e_from_q () else : raise RuntimeError ( 'Not a valid humidity type' ) self . _rh = None self . _q = None def _find_e_from_q ( self ): \"\"\"Calculate e, partial pressure of water vapor.\"\"\" svp = find_svp ( self . _t ) # We have q = w/(w + 1), so w = q/(1 - q) w = self . _q / ( 1 - self . _q ) self . _e = w * self . _R_v * ( self . _p - svp ) / self . _R_d def _find_e_from_rh ( self ): \"\"\"Calculate partial pressure of water vapor.\"\"\" svp = find_svp ( self . _t ) self . _e = self . _rh / 100 * svp def _get_wet_refractivity ( self ): ''' Calculate the wet delay from pressure, temperature, and e ''' self . _wet_refractivity = self . _k2 * self . _e / self . _t + self . _k3 * self . _e / self . _t ** 2 def _get_hydro_refractivity ( self ): ''' Calculate the hydrostatic delay from pressure and temperature ''' self . _hydrostatic_refractivity = self . _k1 * self . _p / self . _t def getWetRefractivity ( self ): return self . _wet_refractivity def getHydroRefractivity ( self ): return self . _hydrostatic_refractivity def _adjust_grid ( self , ll_bounds = None ): ''' This function pads the weather grid with a level at self._zmin, if it does not already go that low. <<The functionality below has been removed.>> <<It also removes levels that are above self._zmax, since they are not needed.>> ''' if self . _zmin < np . nanmin ( self . _zs ): # first add in a new layer at zmin self . _zs = np . insert ( self . _zs , 0 , self . _zmin ) self . _p = util . padLower ( self . _p ) self . _t = util . padLower ( self . _t ) self . _e = util . padLower ( self . _e ) self . _wet_refractivity = util . padLower ( self . _wet_refractivity ) self . _hydrostatic_refractivity = util . padLower ( self . _hydrostatic_refractivity ) if ll_bounds is not None : self . _trimExtent ( ll_bounds ) def _getZTD ( self , zref = None ): ''' Compute the full slant tropospheric delay for each weather model grid node, using the reference height zref ''' if zref is None : zref = self . _zmax wet = self . getWetRefractivity () hydro = self . getHydroRefractivity () # Get the integrated ZTD wet_total , hydro_total = np . zeros ( wet . shape ), np . zeros ( hydro . shape ) for level in range ( wet . shape [ 2 ]): wet_total [ ... , level ] = 1e-6 * np . trapz ( wet [ ... , level :], x = self . _zs [ level :], axis = 2 ) hydro_total [ ... , level ] = 1e-6 * np . trapz ( hydro [ ... , level :], x = self . _zs [ level :], axis = 2 ) self . _hydrostatic_ztd = hydro_total self . _wet_ztd = wet_total def _getExtent ( self , lats , lons ): ''' get the bounding box around a set of lats/lons ''' if ( lats . size == 1 ) & ( lons . size == 1 ): return [ lats - self . _lat_res , lats + self . _lat_res , lons - self . _lon_res , lons + self . _lon_res ] elif ( lats . size > 1 ) & ( lons . size > 1 ): return [ np . nanmin ( lats ), np . nanmax ( lats ), np . nanmin ( lons ), np . nanmax ( lons )] elif lats . size == 1 : return [ lats - self . _lat_res , lats + self . _lat_res , np . nanmin ( lons ), np . nanmax ( lons )] elif lons . size == 1 : return [ np . nanmin ( lats ), np . nanmax ( lats ), lons - self . _lon_res , lons + self . _lon_res ] else : raise RuntimeError ( 'Not a valid lat/lon shape' ) @property def bbox ( self ) -> list : \"\"\" Obtains the bounding box of the weather model in lat/lon CRS. Returns: ------- list xmin, ymin, xmax, ymax Raises ------ ValueError When `self.files` is None. \"\"\" if self . _bbox is None : if self . files is None : raise ValueError ( 'Need to save weather model as netcdf' ) weather_model_path = self . files [ 0 ] with xarray . load_dataset ( weather_model_path ) as ds : try : xmin , xmax = ds . x . min (), ds . x . max () ymin , ymax = ds . y . min (), ds . y . max () except : xmin , xmax = ds . longitude . min (), ds . longitude . max () ymin , ymax = ds . latitude . min (), ds . latitude . max () wm_proj = self . _proj lons , lats = transform_coords ( wm_proj , CRS ( 4326 ), [ xmin , xmax ], [ ymin , ymax ]) self . _bbox = [ lons [ 0 ], lats [ 0 ], lons [ 1 ], lats [ 1 ]] return self . _bbox def checkContainment ( self : weatherModel , ll_bounds : np . ndarray , buffer_deg : float = 1e-5 ) -> bool : \"\"\"\" Checks containment of weather model bbox of outLats and outLons provided. Args: ---------- weather_model : weatherModel outLats : np.ndarray An array of latitude points outLons : np.ndarray An array of longitude points buffer_deg : float For x-translates for extents that lie outside of world bounding box, this ensures that translates have some overlap. The default is 1e-5 or ~11.1 meters. Returns: ------- bool True if weather model contains bounding box of OutLats and outLons and False otherwise. \"\"\" ymin_input , ymax_input , xmin_input , xmax_input = ll_bounds input_box = box ( xmin_input , ymin_input , xmax_input , ymax_input ) xmin , ymin , xmax , ymax = self . bbox weather_model_box = box ( xmin , ymin , xmax , ymax ) world_box = box ( - 180 , - 90 , 180 , 90 ) # Logger input_box_str = [ f ' { x : 1.2f } ' for x in [ xmin_input , ymin_input , xmax_input , ymax_input ]] weath_box_str = [ f ' { x : 1.2f } ' for x in [ xmin , ymin , xmax , ymax ]] weath_box_str = ', ' . join ( weath_box_str ) input_box_str = ', ' . join ( input_box_str ) logger . info ( f 'Extent of the weather model is (xmin, ymin, xmax, ymax):' f ' { weath_box_str } ' ) logger . info ( f 'Extent of the input is (xmin, ymin, xmax, ymax): ' f ' { input_box_str } ' ) # If the bounding box goes beyond the normal world extents # Look at two x-translates, buffer them, and take their union. if not world_box . contains ( weather_model_box ): logger . info ( 'Considering x-translates of weather model +/-360 ' 'as bounding box outside of -180, -90, 180, 90' ) translates = [ weather_model_box . buffer ( buffer_deg ), translate ( weather_model_box , xoff = 360 ) . buffer ( buffer_deg ), translate ( weather_model_box , xoff =- 360 ) . buffer ( buffer_deg ) ] weather_model_box = unary_union ( translates ) return weather_model_box . contains ( input_box ) def _isOutside ( self , extent1 , extent2 ): ''' Determine whether any of extent1 lies outside extent2 extent1/2 should be a list containing [lower_lat, upper_lat, left_lon, right_lon] ''' t1 = extent1 [ 0 ] < extent2 [ 0 ] t2 = extent1 [ 1 ] > extent2 [ 1 ] t3 = extent1 [ 2 ] < extent2 [ 2 ] t4 = extent1 [ 3 ] > extent2 [ 3 ] if np . any ([ t1 , t2 , t3 , t4 ]): return True return False def _trimExtent ( self , extent ): ''' get the bounding box around a set of lats/lons ''' lat = self . _lats [:, :, 0 ] lon = self . _lons [:, :, 0 ] lat [ np . isnan ( lat )] = np . nanmean ( lat ) lon [ np . isnan ( lon )] = np . nanmean ( lon ) mask = ( lat >= extent [ 0 ]) & ( lat <= extent [ 1 ]) & \\ ( lon >= extent [ 2 ]) & ( lon <= extent [ 3 ]) ma1 = np . sum ( mask , axis = 1 ) . astype ( 'bool' ) ma2 = np . sum ( mask , axis = 0 ) . astype ( 'bool' ) if np . sum ( ma1 ) == 0 and np . sum ( ma2 ) == 0 : # Don't need to remove any points return # indices of the part of the grid to keep ny , nx , nz = self . _p . shape index1 = max ( np . arange ( len ( ma1 ))[ ma1 ][ 0 ] - 2 , 0 ) index2 = min ( np . arange ( len ( ma1 ))[ ma1 ][ - 1 ] + 2 , ny ) index3 = max ( np . arange ( len ( ma2 ))[ ma2 ][ 0 ] - 2 , 0 ) index4 = min ( np . arange ( len ( ma2 ))[ ma2 ][ - 1 ] + 2 , nx ) # subset around points of interest self . _lons = self . _lons [ index1 : index2 , index3 : index4 , :] self . _lats = self . _lats [ index1 : index2 , index3 : index4 , ... ] self . _xs = self . _xs [ index3 : index4 ] self . _ys = self . _ys [ index1 : index2 ] self . _p = self . _p [ index1 : index2 , index3 : index4 , ... ] self . _t = self . _t [ index1 : index2 , index3 : index4 , ... ] self . _e = self . _e [ index1 : index2 , index3 : index4 , ... ] self . _wet_refractivity = self . _wet_refractivity [ index1 : index2 , index3 : index4 , ... ] self . _hydrostatic_refractivity = self . _hydrostatic_refractivity [ index1 : index2 , index3 : index4 , :] def _calculategeoh ( self , z , lnsp ): ''' Function to calculate pressure, geopotential, and geopotential height from the surface pressure and model levels provided by a weather model. The model levels are numbered from the highest eleveation to the lowest. Inputs: self - weather model object with parameters a, b defined z - 3-D array of surface heights for the location(s) of interest lnsp - log of the surface pressure Outputs: geopotential - The geopotential in units of height times acceleration pressurelvs - The pressure at each of the model levels for each of the input points geoheight - The geopotential heights ''' return calcgeoh ( lnsp , self . _t , self . _q , z , self . _a , self . _b , self . _R_d , self . _levels ) def getProjection ( self ): ''' Returns: the native weather projection, which should be a pyproj object ''' return self . _proj def getPoints ( self ): return self . _xs . copy (), self . _ys . copy (), self . _zs . copy () def _uniform_in_z ( self , _zlevels = None ): ''' Interpolate all variables to a regular grid in z ''' nx , ny = self . _p . shape [: 2 ] # new regular z-spacing if _zlevels is None : try : _zlevels = self . _zlevels except BaseException : _zlevels = np . nanmean ( self . _zs , axis = ( 0 , 1 )) new_zs = np . tile ( _zlevels , ( nx , ny , 1 )) # re-assign values to the uniform z self . _t = interpolate_along_axis ( self . _zs , self . _t , new_zs , axis = 2 , fill_value = np . nan ) . astype ( np . float32 ) self . _p = interpolate_along_axis ( self . _zs , self . _p , new_zs , axis = 2 , fill_value = np . nan ) . astype ( np . float32 ) self . _e = interpolate_along_axis ( self . _zs , self . _e , new_zs , axis = 2 , fill_value = np . nan ) . astype ( np . float32 ) self . _lats = interpolate_along_axis ( self . _zs , self . _lats , new_zs , axis = 2 , fill_value = np . nan ) . astype ( np . float32 ) self . _lons = interpolate_along_axis ( self . _zs , self . _lons , new_zs , axis = 2 , fill_value = np . nan ) . astype ( np . float32 ) self . _zs = _zlevels self . _xs = np . unique ( self . _xs ) self . _ys = np . unique ( self . _ys ) def _checkForNans ( self ): ''' Fill in NaN-values ''' self . _p = fillna3D ( self . _p ) self . _t = fillna3D ( self . _t ) self . _e = fillna3D ( self . _e ) def out_file ( self , outLoc ): f = make_weather_model_filename ( self . _Name , self . _time , self . _ll_bounds , ) return os . path . join ( outLoc , f ) def filename ( self , time = None , outLoc = 'weather_files' ): ''' Create a filename to store the weather model ''' os . makedirs ( outLoc , exist_ok = True ) if time is None : if self . _time is None : raise ValueError ( 'Time must be specified before the file can be written' ) else : time = self . _time f = make_raw_weather_data_filename ( outLoc , self . _Name , time , ) self . files = [ f ] def write ( self , NoDataValue =- 3.4028234e+38 , chunk = ( 1 , 128 , 128 ), ): ''' By calling the abstract/modular netcdf writer (RAiDER.utilFcns.write2NETCDF4core), write the weather model data and refractivity to an NETCDF4 file that can be accessed by external programs. ''' # Generate the filename mapping_name = get_mapping ( self . _proj ) f = self . _out_name dimidY , dimidX , dimidZ = self . _t . shape chunk_lines_Y = np . min ([ chunk [ 1 ], dimidY ]) chunk_lines_X = np . min ([ chunk [ 2 ], dimidX ]) ChunkSize = [ 1 , chunk_lines_Y , chunk_lines_X ] nc_outfile = netCDF4 . Dataset ( f , 'w' , clobber = True , format = 'NETCDF4' ) nc_outfile . setncattr ( 'Conventions' , 'CF-1.6' ) nc_outfile . setncattr ( 'datetime' , datetime . datetime . strftime ( self . _time , \"%Y_%m_ %d T%H_%M_%S\" )) nc_outfile . setncattr ( 'date_created' , datetime . datetime . now () . strftime ( \"%Y_%m_ %d T%H_%M_%S\" )) title = 'Weather model data and delay calculations' nc_outfile . setncattr ( 'title' , title ) tran = [ self . _xs [ 0 ], self . _xs [ 1 ] - self . _xs [ 0 ], 0.0 , self . _ys [ 0 ], 0.0 , self . _ys [ 1 ] - self . _ys [ 0 ]] dimension_dict = { 'x' : { 'varname' : 'x' , 'datatype' : np . dtype ( 'float64' ), 'dimensions' : ( 'x' ), 'length' : dimidX , 'FillValue' : None , 'standard_name' : 'projection_x_coordinate' , 'description' : 'weather model native x' , 'dataset' : self . _xs , 'units' : 'degrees_east' }, 'y' : { 'varname' : 'y' , 'datatype' : np . dtype ( 'float64' ), 'dimensions' : ( 'y' ), 'length' : dimidY , 'FillValue' : None , 'standard_name' : 'projection_y_coordinate' , 'description' : 'weather model native y' , 'dataset' : self . _ys , 'units' : 'degrees_north' }, 'z' : { 'varname' : 'z' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' ), 'length' : dimidZ , 'FillValue' : None , 'standard_name' : 'projection_z_coordinate' , 'description' : 'vertical coordinate' , 'dataset' : self . _zs , 'units' : 'm' } } dataset_dict = { 'latitude' : { 'varname' : 'latitude' , 'datatype' : np . dtype ( 'float64' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'latitude' , 'description' : 'latitude' , 'dataset' : self . _lats . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 ), 'units' : 'degrees_north' }, 'longitude' : { 'varname' : 'longitude' , 'datatype' : np . dtype ( 'float64' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'longitude' , 'description' : 'longitude' , 'dataset' : self . _lons . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 ), 'units' : 'degrees_east' }, 't' : { 'varname' : 't' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'temperature' , 'description' : 'temperature' , 'dataset' : self . _t . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 ), 'units' : 'K' }, 'p' : { 'varname' : 'p' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'pressure' , 'description' : 'pressure' , 'dataset' : self . _p . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 ), 'units' : 'Pa' }, 'e' : { 'varname' : 'e' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'humidity' , 'description' : 'humidity' , 'dataset' : self . _e . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 ), 'units' : 'Pa' }, 'wet' : { 'varname' : 'wet' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'wet_refractivity' , 'description' : 'wet_refractivity' , 'dataset' : self . _wet_refractivity . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 )}, 'hydro' : { 'varname' : 'hydro' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'hydrostatic_refractivity' , 'description' : 'hydrostatic_refractivity' , 'dataset' : self . _hydrostatic_refractivity . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 )}, 'wet_total' : { 'varname' : 'wet_total' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'total_wet_refractivity' , 'description' : 'total_wet_refractivity' , 'dataset' : self . _wet_ztd . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 )}, 'hydro_total' : { 'varname' : 'hydro_total' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'total_hydrostatic_refractivity' , 'description' : 'total_hydrostatic_refractivity' , 'dataset' : self . _hydrostatic_ztd . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 )} } nc_outfile = write2NETCDF4core ( nc_outfile , dimension_dict , dataset_dict , tran , mapping_name = mapping_name ) nc_outfile . sync () # flush data to disk nc_outfile . close () return f bbox : list property Obtains the bounding box of the weather model in lat/lon CRS. list xmin, ymin, xmax, ymax Raises ValueError When self.files is None. checkContainment ( ll_bounds , buffer_deg = 1e-05 ) \" Checks containment of weather model bbox of outLats and outLons provided. weather_model : weatherModel np.ndarray An array of latitude points np.ndarray An array of longitude points float For x-translates for extents that lie outside of world bounding box, this ensures that translates have some overlap. The default is 1e-5 or ~11.1 meters. bool True if weather model contains bounding box of OutLats and outLons and False otherwise. Source code in RAiDER/models/weatherModel.py 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 def checkContainment ( self : weatherModel , ll_bounds : np . ndarray , buffer_deg : float = 1e-5 ) -> bool : \"\"\"\" Checks containment of weather model bbox of outLats and outLons provided. Args: ---------- weather_model : weatherModel outLats : np.ndarray An array of latitude points outLons : np.ndarray An array of longitude points buffer_deg : float For x-translates for extents that lie outside of world bounding box, this ensures that translates have some overlap. The default is 1e-5 or ~11.1 meters. Returns: ------- bool True if weather model contains bounding box of OutLats and outLons and False otherwise. \"\"\" ymin_input , ymax_input , xmin_input , xmax_input = ll_bounds input_box = box ( xmin_input , ymin_input , xmax_input , ymax_input ) xmin , ymin , xmax , ymax = self . bbox weather_model_box = box ( xmin , ymin , xmax , ymax ) world_box = box ( - 180 , - 90 , 180 , 90 ) # Logger input_box_str = [ f ' { x : 1.2f } ' for x in [ xmin_input , ymin_input , xmax_input , ymax_input ]] weath_box_str = [ f ' { x : 1.2f } ' for x in [ xmin , ymin , xmax , ymax ]] weath_box_str = ', ' . join ( weath_box_str ) input_box_str = ', ' . join ( input_box_str ) logger . info ( f 'Extent of the weather model is (xmin, ymin, xmax, ymax):' f ' { weath_box_str } ' ) logger . info ( f 'Extent of the input is (xmin, ymin, xmax, ymax): ' f ' { input_box_str } ' ) # If the bounding box goes beyond the normal world extents # Look at two x-translates, buffer them, and take their union. if not world_box . contains ( weather_model_box ): logger . info ( 'Considering x-translates of weather model +/-360 ' 'as bounding box outside of -180, -90, 180, 90' ) translates = [ weather_model_box . buffer ( buffer_deg ), translate ( weather_model_box , xoff = 360 ) . buffer ( buffer_deg ), translate ( weather_model_box , xoff =- 360 ) . buffer ( buffer_deg ) ] weather_model_box = unary_union ( translates ) return weather_model_box . contains ( input_box ) checkTime ( time ) Checks the time against the lag time and valid date range for the given model type Source code in RAiDER/models/weatherModel.py 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 def checkTime ( self , time ): ''' Checks the time against the lag time and valid date range for the given model type ''' logger . info ( 'Weather model %s is available from %s - %s ' , self . Model (), self . _valid_range [ 0 ], self . _valid_range [ 1 ] ) if time < self . _valid_range [ 0 ]: raise RuntimeError ( \"Weather model {} is not available at {} \" . format ( self . Model (), time )) if self . _valid_range [ 1 ] is not None : if self . _valid_range [ 1 ] == 'Present' : pass elif self . _valid_range [ 1 ] < time : raise RuntimeError ( \"Weather model {} is not available at {} \" . format ( self . Model (), time )) if time > datetime . datetime . utcnow () - self . _lag_time : raise RuntimeError ( \"Weather model {} is not available at {} \" . format ( self . Model (), time )) fetch ( out , ll_bounds , time ) Checks the input datetime against the valid date range for the model and then calls the model _fetch routine out - ll_bounds - 4 x 1 array, SNWE time = UTC datetime Source code in RAiDER/models/weatherModel.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def fetch ( self , out , ll_bounds , time ): ''' Checks the input datetime against the valid date range for the model and then calls the model _fetch routine Args: ---------- out - ll_bounds - 4 x 1 array, SNWE time = UTC datetime ''' self . checkTime ( time ) self . set_latlon_bounds ( ll_bounds ) self . setTime ( time ) self . _fetch ( out ) filename ( time = None , outLoc = 'weather_files' ) Create a filename to store the weather model Source code in RAiDER/models/weatherModel.py 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 def filename ( self , time = None , outLoc = 'weather_files' ): ''' Create a filename to store the weather model ''' os . makedirs ( outLoc , exist_ok = True ) if time is None : if self . _time is None : raise ValueError ( 'Time must be specified before the file can be written' ) else : time = self . _time f = make_raw_weather_data_filename ( outLoc , self . _Name , time , ) self . files = [ f ] getProjection () Source code in RAiDER/models/weatherModel.py 539 540 541 542 543 def getProjection ( self ): ''' Returns: the native weather projection, which should be a pyproj object ''' return self . _proj load ( outLoc , * args , ll_bounds = None , _zlevels = None , zref = _ZREF , ** kwargs ) Calls the load_weather method. Each model class should define a load_weather method appropriate for that class. 'args' should be one or more filenames. Source code in RAiDER/models/weatherModel.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 def load ( self , outLoc , * args , ll_bounds = None , _zlevels = None , zref = _ZREF , ** kwargs ): ''' Calls the load_weather method. Each model class should define a load_weather method appropriate for that class. 'args' should be one or more filenames. ''' self . set_latlon_bounds ( ll_bounds ) # If the weather file has already been processed, do nothing self . _out_name = self . out_file ( outLoc ) if os . path . exists ( self . _out_name ): return self . _out_name else : # Load the weather just for the query points self . load_weather ( * args , ** kwargs ) # Process the weather model data self . _find_e () self . _uniform_in_z ( _zlevels = _zlevels ) self . _checkForNans () self . _get_wet_refractivity () self . _get_hydro_refractivity () self . _adjust_grid ( ll_bounds ) # Compute Zenith delays at the weather model grid nodes self . _getZTD ( zref ) return None load_weather ( * args , ** kwargs ) abstractmethod Placeholder method. Should be implemented in each weather model type class Source code in RAiDER/models/weatherModel.py 223 224 225 226 227 228 @abstractmethod def load_weather ( self , * args , ** kwargs ): ''' Placeholder method. Should be implemented in each weather model type class ''' pass plot ( plotType = 'pqt' , savefig = True ) Plotting method. Valid plot types are 'pqt' Source code in RAiDER/models/weatherModel.py 237 238 239 240 241 242 243 244 245 246 247 def plot ( self , plotType = 'pqt' , savefig = True ): ''' Plotting method. Valid plot types are 'pqt' ''' if plotType == 'pqt' : plot = plots . plot_pqt ( self , savefig ) elif plotType == 'wh' : plot = plots . plot_wh ( self , savefig ) else : raise RuntimeError ( 'WeatherModel.plot: No plotType named {} ' . format ( plotType )) return plot setTime ( time , fmt = '%Y-%m- %d T%H:%M:%S' ) Set the time for a weather model Source code in RAiDER/models/weatherModel.py 148 149 150 151 152 153 154 155 def setTime ( self , time , fmt = '%Y-%m- %d T%H:%M:%S' ): ''' Set the time for a weather model ''' if isinstance ( time , str ): self . _time = datetime . datetime . strptime ( time , fmt ) elif isinstance ( time , datetime . datetime ): self . _time = time else : raise ValueError ( '\"time\" must be a string or a datetime object' ) set_latlon_bounds ( ll_bounds , Nextra = 2 ) Need to correct lat/lon bounds because not all of the weather models have valid data exactly bounded by -90/90 (lats) and -180/180 (lons); for GMAO and MERRA2, need to adjust the longitude higher end with an extra buffer; for other models, the exact bounds are close to -90/90 (lats) and -180/180 (lons) and thus can be rounded to the above regions (either in the downloading-file API or subsetting- data API) without problems. Source code in RAiDER/models/weatherModel.py 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def set_latlon_bounds ( self , ll_bounds , Nextra = 2 ): ''' Need to correct lat/lon bounds because not all of the weather models have valid data exactly bounded by -90/90 (lats) and -180/180 (lons); for GMAO and MERRA2, need to adjust the longitude higher end with an extra buffer; for other models, the exact bounds are close to -90/90 (lats) and -180/180 (lons) and thus can be rounded to the above regions (either in the downloading-file API or subsetting- data API) without problems. ''' ex_buffer_lon_max = 0.0 if self . _Name == 'GMAO' or self . _Name == 'MERRA2' : ex_buffer_lon_max = self . _lon_res elif self . _Name == 'HRRR' : Nextra = 6 # have a bigger buffer # At boundary lats and lons, need to modify Nextra buffer so that the lats and lons do not exceed the boundary S , N , W , E = ll_bounds # Adjust bounds if they get near the poles or IDL S = np . max ([ S - Nextra * self . _lat_res , - 90.0 + Nextra * self . _lat_res ]) N = np . min ([ N + Nextra * self . _lat_res , 90.0 - Nextra * self . _lat_res ]) W = np . max ([ W - Nextra * self . _lon_res , - 180.0 + Nextra * self . _lon_res ]) E = np . min ([ E + Nextra * self . _lon_res + ex_buffer_lon_max , 180.0 - Nextra * self . _lon_res - ex_buffer_lon_max ]) self . _ll_bounds = np . array ([ S , N , W , E ]) write ( NoDataValue =- 3.4028234e+38 , chunk = ( 1 , 128 , 128 )) By calling the abstract/modular netcdf writer (RAiDER.utilFcns.write2NETCDF4core), write the weather model data and refractivity to an NETCDF4 file that can be accessed by external programs. Source code in RAiDER/models/weatherModel.py 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 def write ( self , NoDataValue =- 3.4028234e+38 , chunk = ( 1 , 128 , 128 ), ): ''' By calling the abstract/modular netcdf writer (RAiDER.utilFcns.write2NETCDF4core), write the weather model data and refractivity to an NETCDF4 file that can be accessed by external programs. ''' # Generate the filename mapping_name = get_mapping ( self . _proj ) f = self . _out_name dimidY , dimidX , dimidZ = self . _t . shape chunk_lines_Y = np . min ([ chunk [ 1 ], dimidY ]) chunk_lines_X = np . min ([ chunk [ 2 ], dimidX ]) ChunkSize = [ 1 , chunk_lines_Y , chunk_lines_X ] nc_outfile = netCDF4 . Dataset ( f , 'w' , clobber = True , format = 'NETCDF4' ) nc_outfile . setncattr ( 'Conventions' , 'CF-1.6' ) nc_outfile . setncattr ( 'datetime' , datetime . datetime . strftime ( self . _time , \"%Y_%m_ %d T%H_%M_%S\" )) nc_outfile . setncattr ( 'date_created' , datetime . datetime . now () . strftime ( \"%Y_%m_ %d T%H_%M_%S\" )) title = 'Weather model data and delay calculations' nc_outfile . setncattr ( 'title' , title ) tran = [ self . _xs [ 0 ], self . _xs [ 1 ] - self . _xs [ 0 ], 0.0 , self . _ys [ 0 ], 0.0 , self . _ys [ 1 ] - self . _ys [ 0 ]] dimension_dict = { 'x' : { 'varname' : 'x' , 'datatype' : np . dtype ( 'float64' ), 'dimensions' : ( 'x' ), 'length' : dimidX , 'FillValue' : None , 'standard_name' : 'projection_x_coordinate' , 'description' : 'weather model native x' , 'dataset' : self . _xs , 'units' : 'degrees_east' }, 'y' : { 'varname' : 'y' , 'datatype' : np . dtype ( 'float64' ), 'dimensions' : ( 'y' ), 'length' : dimidY , 'FillValue' : None , 'standard_name' : 'projection_y_coordinate' , 'description' : 'weather model native y' , 'dataset' : self . _ys , 'units' : 'degrees_north' }, 'z' : { 'varname' : 'z' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' ), 'length' : dimidZ , 'FillValue' : None , 'standard_name' : 'projection_z_coordinate' , 'description' : 'vertical coordinate' , 'dataset' : self . _zs , 'units' : 'm' } } dataset_dict = { 'latitude' : { 'varname' : 'latitude' , 'datatype' : np . dtype ( 'float64' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'latitude' , 'description' : 'latitude' , 'dataset' : self . _lats . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 ), 'units' : 'degrees_north' }, 'longitude' : { 'varname' : 'longitude' , 'datatype' : np . dtype ( 'float64' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'longitude' , 'description' : 'longitude' , 'dataset' : self . _lons . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 ), 'units' : 'degrees_east' }, 't' : { 'varname' : 't' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'temperature' , 'description' : 'temperature' , 'dataset' : self . _t . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 ), 'units' : 'K' }, 'p' : { 'varname' : 'p' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'pressure' , 'description' : 'pressure' , 'dataset' : self . _p . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 ), 'units' : 'Pa' }, 'e' : { 'varname' : 'e' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'humidity' , 'description' : 'humidity' , 'dataset' : self . _e . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 ), 'units' : 'Pa' }, 'wet' : { 'varname' : 'wet' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'wet_refractivity' , 'description' : 'wet_refractivity' , 'dataset' : self . _wet_refractivity . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 )}, 'hydro' : { 'varname' : 'hydro' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'hydrostatic_refractivity' , 'description' : 'hydrostatic_refractivity' , 'dataset' : self . _hydrostatic_refractivity . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 )}, 'wet_total' : { 'varname' : 'wet_total' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'total_wet_refractivity' , 'description' : 'total_wet_refractivity' , 'dataset' : self . _wet_ztd . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 )}, 'hydro_total' : { 'varname' : 'hydro_total' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'total_hydrostatic_refractivity' , 'description' : 'total_hydrostatic_refractivity' , 'dataset' : self . _hydrostatic_ztd . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 )} } nc_outfile = write2NETCDF4core ( nc_outfile , dimension_dict , dataset_dict , tran , mapping_name = mapping_name ) nc_outfile . sync () # flush data to disk nc_outfile . close () return f find_svp ( t ) Calculate standard vapor presure. Should be model-specific Source code in RAiDER/models/weatherModel.py 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 def find_svp ( t ): \"\"\" Calculate standard vapor presure. Should be model-specific \"\"\" # From TRAIN: # Could not find the wrf used equation as they appear to be # mixed with latent heat etc. Istead I used the equations used # in ERA-I (see IFS documentation part 2: Data assimilation # (CY25R1)). Calculate saturated water vapour pressure (svp) for # water (svpw) using Buck 1881 and for ice (swpi) from Alduchow # and Eskridge (1996) euation AERKi # TODO: figure out the sources of all these magic numbers and move # them somewhere more visible. # TODO: (Jeremy) - Need to fix/get the equation for the other # weather model types. Right now this will be used for all models, # except WRF, which is yet to be implemented in my new structure. t1 = 273.15 # O Celsius t2 = 250.15 # -23 Celsius tref = t - t1 wgt = ( t - t2 ) / ( t1 - t2 ) svpw = ( 6.1121 * np . exp (( 17.502 * tref ) / ( 240.97 + tref ))) svpi = ( 6.1121 * np . exp (( 22.587 * tref ) / ( 273.86 + tref ))) svp = svpi + ( svpw - svpi ) * wgt ** 2 ix_bound1 = t > t1 svp [ ix_bound1 ] = svpw [ ix_bound1 ] ix_bound2 = t < t2 svp [ ix_bound2 ] = svpi [ ix_bound2 ] svp = svp * 100 return svp . astype ( np . float32 ) get_mapping ( proj ) Get CF-complient projection information from a proj Source code in RAiDER/models/weatherModel.py 858 859 860 861 862 863 864 def get_mapping ( proj ): '''Get CF-complient projection information from a proj''' # In case of WGS-84 lat/lon, keep it simple if proj . to_epsg () == 4326 : return 'WGS84' else : return proj . to_wkt () make_raw_weather_data_filename ( outLoc , name , time ) Filename generator for the raw downloaded weather model data Source code in RAiDER/models/weatherModel.py 810 811 812 813 814 815 816 817 818 819 820 def make_raw_weather_data_filename ( outLoc , name , time ): ''' Filename generator for the raw downloaded weather model data ''' f = os . path . join ( outLoc , ' {} _ {} . {} ' . format ( name , datetime . datetime . strftime ( time , '%Y_%m_ %d _T%H_%M_%S' ), 'nc' ) ) return f wrf UnitTypeError Bases: Exception Define a unit type exception for easily formatting error messages for units Source code in RAiDER/models/wrf.py 162 163 164 165 166 167 168 169 170 class UnitTypeError ( Exception ): ''' Define a unit type exception for easily formatting error messages for units ''' def __init___ ( self , varName , unittype ): msg = \"Unknown units for {} : ' {} '\" . format ( varName , unittype ) Exception . __init__ ( self , msg ) WRF Bases: WeatherModel WRF class definition, based on the WeatherModel base class. Source code in RAiDER/models/wrf.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 class WRF ( WeatherModel ): ''' WRF class definition, based on the WeatherModel base class. ''' # TODO: finish implementing def __init__ ( self ): WeatherModel . __init__ ( self ) self . _k1 = 0.776 # K/Pa self . _k2 = 0.233 # K/Pa self . _k3 = 3.75e3 # K^2/Pa # Currently WRF is using RH instead of Q to get E self . _humidityType = 'rh' self . _Name = 'WRF' def _fetch ( self ): pass def load_weather ( self , file1 , file2 , * args , ** kwargs ): ''' Consistent class method to be implemented across all weather model types ''' try : lons , lats = self . _get_wm_nodes ( file1 ) self . _read_netcdf ( file2 ) except KeyError : self . _get_wm_nodes ( file2 ) self . _read_netcdf ( file1 ) # WRF doesn't give us the coordinates of the points in the native projection, # only the coordinates in lat/long. Ray transformed these to the native # projection, then used an average to enforce a regular grid. It does matter # for the interpolation whether the grid is regular. lla = CRS . from_epsg ( 4326 ) t = Transformer . from_proj ( lla , self . _proj ) xs , ys = t . transform ( lons . flatten (), lats . flatten ()) xs = xs . reshape ( lons . shape ) ys = ys . reshape ( lats . shape ) # Expected accuracy here is to two decimal places (five significant digits) xs = np . mean ( xs , axis = 0 ) ys = np . mean ( ys , axis = 1 ) _xs = np . broadcast_to ( xs [ np . newaxis , np . newaxis , :], self . _p . shape ) _ys = np . broadcast_to ( ys [ np . newaxis , :, np . newaxis ], self . _p . shape ) # Re-structure everything from (heights, lats, lons) to (lons, lats, heights) self . _p = np . transpose ( self . _p ) self . _t = np . transpose ( self . _t ) self . _rh = np . transpose ( self . _rh ) self . _ys = np . transpose ( _ys ) self . _xs = np . transpose ( _xs ) self . _zs = np . transpose ( self . _zs ) # TODO: Not sure if WRF provides this self . _levels = list ( range ( self . _zs . shape [ 2 ])) def _get_wm_nodes ( self , nodeFile ): with netcdf . netcdf_file ( nodeFile , 'r' , maskandscale = True ) as outf : lats = outf . variables [ 'XLAT' ][ 0 ] . copy () # Takes only the first date! lons = outf . variables [ 'XLONG' ][ 0 ] . copy () lons [ lons > 180 ] -= 360 return lons , lats def _read_netcdf ( self , weatherFile , defNul = None ): \"\"\" Read weather variables from a netCDF file \"\"\" if defNul is None : defNul = np . nan # TODO: it'd be cool to use some kind of units package # TODO: extract partial pressure directly (q?) with netcdf . netcdf_file ( weatherFile , 'r' , maskandscale = True ) as f : spvar = f . variables [ 'P_PL' ] temp = f . variables [ 'T_PL' ] humid = f . variables [ 'RH_PL' ] geohvar = f . variables [ 'GHT_PL' ] lon0 = f . STAND_LON . copy () lat0 = f . MOAD_CEN_LAT . copy () lat1 = f . TRUELAT1 . copy () lat2 = f . TRUELAT2 . copy () checkUnits ( spvar . units . decode ( 'utf-8' ), 'pressure' ) checkUnits ( temp . units . decode ( 'utf-8' ), 'temperature' ) checkUnits ( humid . units . decode ( 'utf-8' ), 'relative humidity' ) checkUnits ( geohvar . units . decode ( 'utf-8' ), 'geopotential' ) # _FillValue is not always set, but when it is we want to read it tNull = getNullValue ( temp ) hNull = getNullValue ( humid ) gNull = getNullValue ( geohvar ) pNull = getNullValue ( spvar ) sp = spvar [ 0 ] . copy () temps = temp [ 0 ] . copy () humids = humid [ 0 ] . copy () geoh = geohvar [ 0 ] . copy () spvar = None temp = None humid = None geohvar = None # Projection # See http://www.pkrc.net/wrf-lambert.html earthRadius = 6370e3 # <- note Ray had a bug here p1 = CRS ( proj = 'lcc' , lat_1 = lat1 , lat_2 = lat2 , lat_0 = lat0 , lon_0 = lon0 , a = earthRadius , b = earthRadius , towgs84 = ( 0 , 0 , 0 ), no_defs = True ) self . _proj = p1 temps [ temps == tNull ] = np . nan sp [ sp == pNull ] = np . nan humids [ humids == hNull ] = np . nan geoh [ geoh == gNull ] = np . nan self . _t = temps self . _rh = humids # Zs are problematic because any z below the topography is nan. # For a temporary fix, I will assign any nan value to equal the # nanmean of that level. zmeans = np . nanmean ( geoh , axis = ( 1 , 2 )) nz , ny , nx = geoh . shape Zmeans = np . tile ( zmeans , ( nx , ny , 1 )) Zmeans = Zmeans . T ix = np . isnan ( geoh ) geoh [ ix ] = Zmeans [ ix ] self . _zs = geoh if len ( sp . shape ) == 1 : self . _p = np . broadcast_to ( sp [:, np . newaxis , np . newaxis ], self . _zs . shape ) else : self . _p = sp load_weather ( file1 , file2 , * args , ** kwargs ) Consistent class method to be implemented across all weather model types Source code in RAiDER/models/wrf.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def load_weather ( self , file1 , file2 , * args , ** kwargs ): ''' Consistent class method to be implemented across all weather model types ''' try : lons , lats = self . _get_wm_nodes ( file1 ) self . _read_netcdf ( file2 ) except KeyError : self . _get_wm_nodes ( file2 ) self . _read_netcdf ( file1 ) # WRF doesn't give us the coordinates of the points in the native projection, # only the coordinates in lat/long. Ray transformed these to the native # projection, then used an average to enforce a regular grid. It does matter # for the interpolation whether the grid is regular. lla = CRS . from_epsg ( 4326 ) t = Transformer . from_proj ( lla , self . _proj ) xs , ys = t . transform ( lons . flatten (), lats . flatten ()) xs = xs . reshape ( lons . shape ) ys = ys . reshape ( lats . shape ) # Expected accuracy here is to two decimal places (five significant digits) xs = np . mean ( xs , axis = 0 ) ys = np . mean ( ys , axis = 1 ) _xs = np . broadcast_to ( xs [ np . newaxis , np . newaxis , :], self . _p . shape ) _ys = np . broadcast_to ( ys [ np . newaxis , :, np . newaxis ], self . _p . shape ) # Re-structure everything from (heights, lats, lons) to (lons, lats, heights) self . _p = np . transpose ( self . _p ) self . _t = np . transpose ( self . _t ) self . _rh = np . transpose ( self . _rh ) self . _ys = np . transpose ( _ys ) self . _xs = np . transpose ( _xs ) self . _zs = np . transpose ( self . _zs ) # TODO: Not sure if WRF provides this self . _levels = list ( range ( self . _zs . shape [ 2 ])) checkUnits ( unitCheck , varName ) Implement a check that the units are as expected Source code in RAiDER/models/wrf.py 173 174 175 176 177 178 179 def checkUnits ( unitCheck , varName ): ''' Implement a check that the units are as expected ''' unitDict = { 'pressure' : 'Pa' , 'temperature' : 'K' , 'relative humidity' : '%' , 'geopotential' : 'm' } if unitCheck != unitDict [ varName ]: raise UnitTypeError ( varName , unitCheck ) getNullValue ( var ) Get the null (or fill) value if it exists, otherwise set the null value to defNullValue Source code in RAiDER/models/wrf.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def getNullValue ( var ): ''' Get the null (or fill) value if it exists, otherwise set the null value to defNullValue ''' # NetCDF files have the ability to record their nodata value, but in the # particular NetCDF files that I'm reading, this field is left # unspecified and a nodata value of -999 is used. The solution I'm using # is to check if nodata is specified, and otherwise assume it's -999. _default_fill_value = - 999 try : var_fill = var . _FillValue except AttributeError : var_fill = _default_fill_value return var_fill processWM prepareWeatherModel ( weather_model , time = None , wmLoc = None , ll_bounds = None , download_only = False , makePlots = False , force_download = False ) Parse inputs to download and prepare a weather model grid for interpolation Parameters: Name Type Description Default weather_model WeatherModel - instantiated weather model object required time datetime - Python datetime to request. Will be rounded to nearest available time None wmLoc str str - file path to which to write weather model file(s) None ll_bounds List [ float ] list of float - bounding box to download in [S, N, W, E] format None download_only bool bool - False if preprocessing weather model data False makePlots bool bool - whether to write debug plots False force_download bool bool - True if you want to download even when the weather model exists False Returns: Name Type Description str str filename of the netcdf file to which the weather model has been written Source code in RAiDER/processWM.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def prepareWeatherModel ( weather_model , time = None , wmLoc : str = None , ll_bounds : List [ float ] = None , download_only : bool = False , makePlots : bool = False , force_download : bool = False , ) -> str : \"\"\"Parse inputs to download and prepare a weather model grid for interpolation Args: weather_model: WeatherModel - instantiated weather model object time: datetime - Python datetime to request. Will be rounded to nearest available time wmLoc: str - file path to which to write weather model file(s) ll_bounds: list of float - bounding box to download in [S, N, W, E] format download_only: bool - False if preprocessing weather model data makePlots: bool - whether to write debug plots force_download: bool - True if you want to download even when the weather model exists Returns: str: filename of the netcdf file to which the weather model has been written \"\"\" # Ensure the file output location exists if wmLoc is None : wmLoc = os . path . join ( os . getcwd (), 'weather_files' ) os . makedirs ( wmLoc , exist_ok = True ) # check whether weather model files are supplied or should be downloaded download_flag = True if weather_model . files is None : if time is None : raise RuntimeError ( 'prepareWeatherModel: Either a file or a time must be specified' ) weather_model . filename ( time , wmLoc ) if os . path . exists ( weather_model . files [ 0 ]): if not force_download : logger . warning ( 'Weather model already exists, please remove it (\" %s \") if you want ' 'to download a new one.' , weather_model . files ) download_flag = False else : download_flag = False # if no weather model files supplied, check the standard location if download_flag : weather_model . fetch ( * weather_model . files , ll_bounds , time ) else : time = getTimeFromFile ( weather_model . files [ 0 ]) weather_model . setTime ( time ) containment = weather_model . checkContainment ( ll_bounds ) if not containment : logger . warning ( 'The weather model passed does not cover all of the input ' 'points; you may need to download a larger area.' ) # If only downloading, exit now if download_only : logger . warning ( 'download_only flag selected. No further processing will happen.' ) return None # Otherwise, load the weather model data f = weather_model . load ( wmLoc , ll_bounds = ll_bounds , ) if f is not None : logger . warning ( 'The processed weather model file already exists,' ' so I will use that.' ) return f # Logging some basic info logger . debug ( 'Number of weather model nodes: {} ' . format ( np . prod ( weather_model . getWetRefractivity () . shape ) ) ) shape = weather_model . getWetRefractivity () . shape logger . debug ( f 'Shape of weather model: { shape } ' ) logger . debug ( 'Bounds of the weather model: %.2f / %.2f / %.2f / %.2f (SNWE)' , np . nanmin ( weather_model . _ys ), np . nanmax ( weather_model . _ys ), np . nanmin ( weather_model . _xs ), np . nanmax ( weather_model . _xs ) ) logger . debug ( 'Weather model: %s ' , weather_model . Model ()) logger . debug ( 'Mean value of the wet refractivity: %f ' , np . nanmean ( weather_model . getWetRefractivity ()) ) logger . debug ( 'Mean value of the hydrostatic refractivity: %f ' , np . nanmean ( weather_model . getHydroRefractivity ()) ) logger . debug ( weather_model ) if makePlots : weather_model . plot ( 'wh' , True ) weather_model . plot ( 'pqt' , True ) plt . close ( 'all' ) try : f = weather_model . write () return f except Exception as e : logger . exception ( \"Unable to save weathermodel to file\" ) logger . exception ( e ) raise RuntimeError ( \"Unable to save weathermodel to file\" ) finally : del weather_model utilFcns Geodesy-related utility functions. calcgeoh ( lnsp , t , q , z , a , b , R_d , num_levels ) Calculate pressure, geopotential, and geopotential height from the surface pressure and model levels provided by a weather model. The model levels are numbered from the highest eleveation to the lowest. lnsp: ndarray - [y, x] array of log surface pressure t: ndarray - [z, y, x] cube of temperatures q: ndarray - [z, y, x] cube of specific humidity geopotential: ndarray - [z, y, x] cube of geopotential values a: ndarray - [z] vector of a values b: ndarray - [z] vector of b values num_levels: int - integer number of model levels geopotential - The geopotential in units of height times acceleration pressurelvs - The pressure at each of the model levels for each of the input points geoheight - The geopotential heights Source code in RAiDER/utilFcns.py 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 def calcgeoh ( lnsp , t , q , z , a , b , R_d , num_levels ): ''' Calculate pressure, geopotential, and geopotential height from the surface pressure and model levels provided by a weather model. The model levels are numbered from the highest eleveation to the lowest. Args: ---------- lnsp: ndarray - [y, x] array of log surface pressure t: ndarray - [z, y, x] cube of temperatures q: ndarray - [z, y, x] cube of specific humidity geopotential: ndarray - [z, y, x] cube of geopotential values a: ndarray - [z] vector of a values b: ndarray - [z] vector of b values num_levels: int - integer number of model levels Returns: ------- geopotential - The geopotential in units of height times acceleration pressurelvs - The pressure at each of the model levels for each of the input points geoheight - The geopotential heights ''' geopotential = np . zeros_like ( t ) pressurelvs = np . zeros_like ( geopotential ) geoheight = np . zeros_like ( geopotential ) # log surface pressure # Note that we integrate from the ground up, so from the largest model level to 0 sp = np . exp ( lnsp ) if len ( a ) != num_levels + 1 or len ( b ) != num_levels + 1 : raise ValueError ( 'I have here a model with {} levels, but parameters a ' . format ( num_levels ) + 'and b have lengths {} and {} respectively. Of ' . format ( len ( a ), len ( b )) + 'course, these three numbers should be equal.' ) # Integrate up into the atmosphere from *lowest level* z_h = 0 # initial value for lev , t_level , q_level in zip ( range ( num_levels , 0 , - 1 ), t [:: - 1 ], q [:: - 1 ]): # lev is the level number 1-60, we need a corresponding index # into ts and qs # ilevel = num_levels - lev # << this was Ray's original, but is a typo # because indexing like that results in pressure and height arrays that # are in the opposite orientation to the t/q arrays. ilevel = lev - 1 # compute moist temperature t_level = t_level * ( 1 + 0.609133 * q_level ) # compute the pressures (on half-levels) Ph_lev = a [ lev - 1 ] + ( b [ lev - 1 ] * sp ) Ph_levplusone = a [ lev ] + ( b [ lev ] * sp ) pressurelvs [ ilevel ] = Ph_lev # + Ph_levplusone) / 2 # average pressure at half-levels above and below if lev == 1 : dlogP = np . log ( Ph_levplusone / 0.1 ) alpha = np . log ( 2 ) else : dlogP = np . log ( Ph_levplusone ) - np . log ( Ph_lev ) alpha = 1 - (( Ph_lev / ( Ph_levplusone - Ph_lev )) * dlogP ) TRd = t_level * R_d # z_f is the geopotential of this full level # integrate from previous (lower) half-level z_h to the full level z_f = z_h + TRd * alpha + z # Geopotential (add in surface geopotential) geopotential [ ilevel ] = z_f geoheight [ ilevel ] = geopotential [ ilevel ] / g0 # z_h is the geopotential of 'half-levels' # integrate z_h to next half level z_h += TRd * dlogP return geopotential , pressurelvs , geoheight checkLOS ( los , Npts ) Check that los is either (1) Zenith, (2) a set of scalar values of the same size as the number of points, which represent the projection value), or (3) a set of vectors, same number as the number of points. Source code in RAiDER/utilFcns.py 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def checkLOS ( los , Npts ): ''' Check that los is either: (1) Zenith, (2) a set of scalar values of the same size as the number of points, which represent the projection value), or (3) a set of vectors, same number as the number of points. ''' from RAiDER.losreader import Zenith # los is a bunch of vectors or Zenith if los is not Zenith : los = los . reshape ( - 1 , 3 ) if los is not Zenith and los . shape [ 0 ] != Npts : raise RuntimeError ( 'Found {} line-of-sight values and only {} points' . format ( los . shape [ 0 ], Npts )) return los checkShapes ( los , lats , lons , hts ) Make sure that by the time the code reaches here, we have a consistent set of line-of-sight and position data. Source code in RAiDER/utilFcns.py 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 def checkShapes ( los , lats , lons , hts ): ''' Make sure that by the time the code reaches here, we have a consistent set of line-of-sight and position data. ''' from RAiDER.losreader import Zenith test1 = hts . shape == lats . shape == lons . shape try : test2 = los . shape [: - 1 ] == hts . shape except AttributeError : test2 = los is Zenith if not test1 and test2 : raise ValueError ( 'I need lats, lons, heights, and los to all be the same shape. ' + 'lats had shape {} , lons had shape {} , ' . format ( lats . shape , lons . shape ) + 'heights had shape {} , and los was not Zenith' . format ( hts . shape )) clip_bbox ( bbox , spacing ) Clip box to multiple of spacing Source code in RAiDER/utilFcns.py 714 715 716 717 718 719 720 721 def clip_bbox ( bbox , spacing ): \"\"\" Clip box to multiple of spacing \"\"\" return [ np . floor ( bbox [ 0 ] / spacing ) * spacing , np . ceil ( bbox [ 1 ] / spacing ) * spacing , np . floor ( bbox [ 2 ] / spacing ) * spacing , np . ceil ( bbox [ 3 ] / spacing ) * spacing ] convertLons ( inLons ) Convert lons from 0-360 to -180-180 Source code in RAiDER/utilFcns.py 933 934 935 936 937 938 def convertLons ( inLons ): '''Convert lons from 0-360 to -180-180''' mask = inLons > 180 outLons = inLons outLons [ mask ] = outLons [ mask ] - 360 return outLons cosd ( x ) Return the cosine of x when x is in degrees. Source code in RAiDER/utilFcns.py 44 45 46 def cosd ( x ): \"\"\"Return the cosine of x when x is in degrees.\"\"\" return np . cos ( np . radians ( x )) ecef2enu ( xyz , lat , lon , height ) Convert ECEF xyz to ENU Source code in RAiDER/utilFcns.py 93 94 95 96 97 98 99 100 101 102 def ecef2enu ( xyz , lat , lon , height ): '''Convert ECEF xyz to ENU''' x , y , z = xyz [ ... , 0 ], xyz [ ... , 1 ], xyz [ ... , 2 ] t = cosd ( lon ) * x + sind ( lon ) * y e = - sind ( lon ) * x + cosd ( lon ) * y n = - sind ( lat ) * t + cosd ( lat ) * z u = cosd ( lat ) * t + sind ( lat ) * z return np . stack (( e , n , u ), axis =- 1 ) enu2ecef ( east , north , up , lat0 , lon0 , h0 ) float target east ENU coordinate (meters) float target north ENU coordinate (meters) float target up ENU coordinate (meters) Results u : float v : float w : float Source code in RAiDER/utilFcns.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def enu2ecef ( east : ndarray , north : ndarray , up : ndarray , lat0 : ndarray , lon0 : ndarray , h0 : ndarray , ): \"\"\" Args: ---------- e1 : float target east ENU coordinate (meters) n1 : float target north ENU coordinate (meters) u1 : float target up ENU coordinate (meters) Results ------- u : float v : float w : float \"\"\" t = cosd ( lat0 ) * up - sind ( lat0 ) * north w = sind ( lat0 ) * up + cosd ( lat0 ) * north u = cosd ( lon0 ) * t - sind ( lon0 ) * east v = sind ( lon0 ) * t + cosd ( lon0 ) * east return np . stack (( u , v , w ), axis =- 1 ) floorish ( val , frac ) Round a value to the lower fractional part Source code in RAiDER/utilFcns.py 34 35 36 def floorish ( val , frac ): '''Round a value to the lower fractional part''' return val - ( val % frac ) getChunkSize ( in_shape ) Create a reasonable chunk size Source code in RAiDER/utilFcns.py 979 980 981 982 983 984 985 986 987 988 989 990 def getChunkSize ( in_shape ): '''Create a reasonable chunk size''' minChunkSize = 100 maxChunkSize = 1000 cpu_count = mp . cpu_count () chunkSize = tuple ( max ( min ( maxChunkSize , s // cpu_count ), min ( s , minChunkSize ) ) for s in in_shape ) return chunkSize getTimeFromFile ( filename ) Parse a filename to get a date-time Source code in RAiDER/utilFcns.py 480 481 482 483 484 485 486 487 488 489 490 def getTimeFromFile ( filename ): ''' Parse a filename to get a date-time ''' fmt = '%Y_%m_ %d _T%H_%M_%S' p = re . compile ( r '\\d {4} _\\d {2} _\\d {2} _T\\d {2} _\\d {2} _\\d {2} ' ) try : out = p . search ( filename ) . group () return datetime . strptime ( out , fmt ) except BaseException : # TODO: Which error(s)? raise RuntimeError ( 'The filename for {} does not include a datetime in the correct format' . format ( filename )) get_file_and_band ( filestr ) Support file;bandnum as input for filename strings Source code in RAiDER/utilFcns.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 def get_file_and_band ( filestr ): \"\"\" Support file;bandnum as input for filename strings \"\"\" parts = filestr . split ( \";\" ) # Defaults to first band if no bandnum is provided if len ( parts ) == 1 : return filestr . strip (), 1 elif len ( parts ) == 2 : return parts [ 0 ] . strip (), int ( parts [ 1 ] . strip ()) else : raise ValueError ( f \"Cannot interpret { filestr } as valid filename\" ) nodataToNan ( inarr , listofvals ) Setting values to nan as needed Source code in RAiDER/utilFcns.py 181 182 183 184 185 186 187 188 def nodataToNan ( inarr , listofvals ): \"\"\" Setting values to nan as needed \"\"\" inarr = inarr . astype ( float ) # nans cannot be integers (i.e. in DEM) for val in listofvals : if val is not None : inarr [ inarr == val ] = np . nan padLower ( invar ) add a layer of data below the lowest current z-level at height zmin Source code in RAiDER/utilFcns.py 370 371 372 373 374 375 def padLower ( invar ): ''' add a layer of data below the lowest current z-level at height zmin ''' new_var = _least_nonzero ( invar ) return np . concatenate (( new_var [:, :, np . newaxis ], invar ), axis = 2 ) projectDelays ( delay , inc ) Project zenith delays to LOS Source code in RAiDER/utilFcns.py 29 30 31 def projectDelays ( delay , inc ): '''Project zenith delays to LOS''' return delay / cosd ( inc ) read_hgt_file ( filename ) Read height data from a comma-delimited file Source code in RAiDER/utilFcns.py 417 418 419 420 421 422 423 def read_hgt_file ( filename ): ''' Read height data from a comma-delimited file ''' data = pd . read_csv ( filename ) hgts = data [ 'Hgt_m' ] . values return hgts requests_retry_session ( retries = 10 , session = None ) https://www.peterbe.com/plog/best-practice-with-retries-with-requests Source code in RAiDER/utilFcns.py 724 725 726 727 728 729 730 731 732 733 734 735 736 def requests_retry_session ( retries = 10 , session = None ): \"\"\" https://www.peterbe.com/plog/best-practice-with-retries-with-requests \"\"\" import requests from requests.adapters import HTTPAdapter from requests.packages.urllib3.util.retry import Retry # add a retry strategy; https://findwork.dev/blog/advanced-usage-python-requests-timeouts-retries-hooks/ session = session or requests . Session () retry = Retry ( total = retries , read = retries , connect = retries , backoff_factor = 0.3 , status_forcelist = list ( range ( 429 , 505 ))) adapter = HTTPAdapter ( max_retries = retry ) session . mount ( 'http://' , adapter ) session . mount ( 'https://' , adapter ) return session rio_extents ( profile ) Get a bounding box in SNWE from a rasterio profile Source code in RAiDER/utilFcns.py 129 130 131 132 133 134 135 136 137 138 139 def rio_extents ( profile ): \"\"\" Get a bounding box in SNWE from a rasterio profile \"\"\" gt = profile [ \"transform\" ] . to_gdal () xSize = profile [ \"width\" ] ySize = profile [ \"height\" ] if profile [ \"crs\" ] is None or not gt : raise AttributeError ( 'Profile does not contain geotransform information' ) W , E = gt [ 0 ], gt [ 0 ] + ( xSize - 1 ) * gt [ 1 ] + ( ySize - 1 ) * gt [ 2 ] N , S = gt [ 3 ], gt [ 3 ] + ( xSize - 1 ) * gt [ 4 ] + ( ySize - 1 ) * gt [ 5 ] return S , N , W , E robmax ( a ) Get the minimum of an array, accounting for empty lists Source code in RAiDER/utilFcns.py 328 329 330 331 332 333 334 335 def robmax ( a ): ''' Get the minimum of an array, accounting for empty lists ''' try : return np . nanmax ( a ) except ValueError : return 'N/A' robmin ( a ) Get the minimum of an array, accounting for empty lists Source code in RAiDER/utilFcns.py 318 319 320 321 322 323 324 325 def robmin ( a ): ''' Get the minimum of an array, accounting for empty lists ''' try : return np . nanmin ( a ) except ValueError : return 'N/A' round_time ( dt , roundTo = 60 ) Round a datetime object to any time lapse in seconds dt: datetime.datetime object roundTo: Closest number of seconds to round to, default 1 minute. Source: https://stackoverflow.com/questions/3463930/how-to-round-the-minute-of-a-datetime-object/10854034#10854034 Source code in RAiDER/utilFcns.py 426 427 428 429 430 431 432 433 434 435 def round_time ( dt , roundTo = 60 ): ''' Round a datetime object to any time lapse in seconds dt: datetime.datetime object roundTo: Closest number of seconds to round to, default 1 minute. Source: https://stackoverflow.com/questions/3463930/how-to-round-the-minute-of-a-datetime-object/10854034#10854034 ''' seconds = ( dt . replace ( tzinfo = None ) - dt . min ) . seconds rounding = ( seconds + roundTo / 2 ) // roundTo * roundTo return dt + timedelta ( 0 , rounding - seconds , - dt . microsecond ) sind ( x ) Return the sine of x when x is in degrees. Source code in RAiDER/utilFcns.py 39 40 41 def sind ( x ): \"\"\"Return the sine of x when x is in degrees.\"\"\" return np . sin ( np . radians ( x )) transform_bbox ( wesn , dest_crs = 4326 , src_crs = 4326 , margin = 100.0 ) Transform bbox to lat/lon or another CRS for use with rest of workflow Source code in RAiDER/utilFcns.py 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 def transform_bbox ( wesn , dest_crs = 4326 , src_crs = 4326 , margin = 100. ): \"\"\" Transform bbox to lat/lon or another CRS for use with rest of workflow Returns: SNWE \"\"\" # TODO - Handle dateline crossing if isinstance ( src_crs , int ): src_crs = pyproj . CRS . from_epsg ( src_crs ) elif isinstance ( src_crs , str ): src_crs = pyproj . CRS ( src_crs ) # Handle margin for input bbox in degrees if src_crs . axis_info [ 0 ] . unit_name == \"degree\" : margin = margin / 1.0e5 if isinstance ( dest_crs , int ): dest_crs = pyproj . CRS . from_epsg ( dest_crs ) elif isinstance ( dest_crs , str ): dest_crs = pyproj . CRS ( dest_crs ) # If dest_crs is same as src_crs if dest_crs == src_crs : snwe = [ wesn [ 2 ], wesn [ 3 ], wesn [ 0 ], wesn [ 1 ]] return snwe T = Transformer . from_crs ( src_crs , dest_crs , always_xy = True ) xs = np . linspace ( wesn [ 0 ] - margin , wesn [ 1 ] + margin , num = 11 ) ys = np . linspace ( wesn [ 2 ] - margin , wesn [ 3 ] + margin , num = 11 ) X , Y = np . meshgrid ( xs , ys ) # Transform to lat/lon xx , yy = T . transform ( X , Y ) # query_area convention snwe = [ np . nanmin ( yy ), np . nanmax ( yy ), np . nanmin ( xx ), np . nanmax ( xx )] return snwe transform_coords ( proj1 , proj2 , x , y ) Transform coordinates from proj1 to proj2 (can be EPSG or crs from proj). e.g. x, y = transform_coords(4326, 4087, lon, lat) Source code in RAiDER/utilFcns.py 1073 1074 1075 1076 1077 1078 1079 def transform_coords ( proj1 , proj2 , x , y ): \"\"\" Transform coordinates from proj1 to proj2 (can be EPSG or crs from proj). e.g. x, y = transform_coords(4326, 4087, lon, lat) \"\"\" transformer = Transformer . from_crs ( proj1 , proj2 , always_xy = True ) return transformer . transform ( x , y ) write2NETCDF4core ( nc_outfile , dimension_dict , dataset_dict , tran , mapping_name = 'WGS84' ) The abstract/modular netcdf writer that can be called by a wrapper function to write data to a NETCDF4 file that can be accessed by external programs. The point of doing this is to alleviate some of the memory load of keeping the full model in memory and make it easier to scale up the program. Source code in RAiDER/utilFcns.py 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 def write2NETCDF4core ( nc_outfile , dimension_dict , dataset_dict , tran , mapping_name = 'WGS84' ): ''' The abstract/modular netcdf writer that can be called by a wrapper function to write data to a NETCDF4 file that can be accessed by external programs. The point of doing this is to alleviate some of the memory load of keeping the full model in memory and make it easier to scale up the program. ''' from osgeo import osr if mapping_name == 'WGS84' : epsg = 4326 crs = pyproj . CRS . from_epsg ( epsg ) grid_mapping = 'WGS84' # need to set this as an attribute for the image variables else : crs = pyproj . CRS . from_wkt ( mapping_name ) grid_mapping = 'CRS' datatype = np . dtype ( 'S1' ) dimensions = () var = nc_outfile . createVariable ( grid_mapping , datatype , dimensions , fill_value = None ) # variable made, now add attributes for k , v in crs . to_cf () . items (): var . setncattr ( k , v ) var . setncattr ( 'GeoTransform' , ' ' . join ( str ( x ) for x in tran )) # note this has pixel size in it - set explicitly above for dim in dimension_dict : nc_outfile . createDimension ( dim , dimension_dict [ dim ][ 'length' ]) varname = dimension_dict [ dim ][ 'varname' ] datatype = dimension_dict [ dim ][ 'datatype' ] dimensions = dimension_dict [ dim ][ 'dimensions' ] FillValue = dimension_dict [ dim ][ 'FillValue' ] var = nc_outfile . createVariable ( varname , datatype , dimensions , fill_value = FillValue ) var . setncattr ( 'standard_name' , dimension_dict [ dim ][ 'standard_name' ]) var . setncattr ( 'description' , dimension_dict [ dim ][ 'description' ]) var . setncattr ( 'units' , dimension_dict [ dim ][ 'units' ]) var [:] = dimension_dict [ dim ][ 'dataset' ] . astype ( dimension_dict [ dim ][ 'datatype' ]) for data in dataset_dict : varname = dataset_dict [ data ][ 'varname' ] datatype = dataset_dict [ data ][ 'datatype' ] dimensions = dataset_dict [ data ][ 'dimensions' ] FillValue = dataset_dict [ data ][ 'FillValue' ] ChunkSize = dataset_dict [ data ][ 'ChunkSize' ] var = nc_outfile . createVariable ( varname , datatype , dimensions , fill_value = FillValue , zlib = True , complevel = 2 , shuffle = True , chunksizes = ChunkSize ) # Override with correct name here var . setncattr ( 'grid_mapping' , grid_mapping ) # dataset_dict[data]['grid_mapping']) var . setncattr ( 'standard_name' , dataset_dict [ data ][ 'standard_name' ]) var . setncattr ( 'description' , dataset_dict [ data ][ 'description' ]) if 'units' in dataset_dict [ data ]: var . setncattr ( 'units' , dataset_dict [ data ][ 'units' ]) ndmask = np . isnan ( dataset_dict [ data ][ 'dataset' ]) dataset_dict [ data ][ 'dataset' ][ ndmask ] = FillValue var [:] = dataset_dict [ data ][ 'dataset' ] . astype ( datatype ) return nc_outfile writeArrayToFile ( lats , lons , array , filename , noDataValue =- 9999 ) Write a single-dim array of values to a file Source code in RAiDER/utilFcns.py 276 277 278 279 280 281 282 283 284 def writeArrayToFile ( lats , lons , array , filename , noDataValue =- 9999 ): ''' Write a single-dim array of values to a file ''' array [ np . isnan ( array )] = noDataValue with open ( filename , 'w' ) as f : f . write ( 'Lat,Lon,Hgt_m \\n ' ) for lat , lon , height in zip ( lats , lons , array ): f . write ( ' {} , {} , {} \\n ' . format ( lat , lon , height )) writeArrayToRaster ( array , filename , noDataValue = 0.0 , fmt = 'ENVI' , proj = None , gt = None ) write a numpy array to a GDAL-readable raster Source code in RAiDER/utilFcns.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 def writeArrayToRaster ( array , filename , noDataValue = 0. , fmt = 'ENVI' , proj = None , gt = None ): ''' write a numpy array to a GDAL-readable raster ''' array_shp = np . shape ( array ) if array . ndim != 2 : raise RuntimeError ( 'writeArrayToRaster: cannot write an array of shape {} to a raster image' . format ( array_shp )) # Data type if \"complex\" in str ( array . dtype ): dtype = np . complex64 elif \"float\" in str ( array . dtype ): dtype = np . float32 else : dtype = np . uint8 # Geotransform trans = None if gt is not None : trans = rasterio . Affine . from_gdal ( * gt ) ## cant write netcdfs with rasterio in a simple way driver = fmt if not fmt == 'nc' else 'GTiff' with rasterio . open ( filename , mode = \"w\" , count = 1 , width = array_shp [ 1 ], height = array_shp [ 0 ], dtype = dtype , crs = proj , nodata = noDataValue , driver = driver , transform = trans ) as dst : dst . write ( array , 1 ) logger . info ( 'Wrote: %s ' , filename ) return writeDelays ( aoi , wetDelay , hydroDelay , wetFilename , hydroFilename = None , outformat = None , proj = None , gt = None , ndv = 0.0 ) Write the delay numpy arrays to files in the format specified Source code in RAiDER/utilFcns.py 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 def writeDelays ( aoi , wetDelay , hydroDelay , wetFilename , hydroFilename = None , outformat = None , proj = None , gt = None , ndv = 0. ): ''' Write the delay numpy arrays to files in the format specified ''' # Need to consistently handle noDataValues wetDelay [ np . isnan ( wetDelay )] = ndv hydroDelay [ np . isnan ( hydroDelay )] = ndv # Do different things, depending on the type of input if aoi . type () == 'station_file' : try : df = pd . read_csv ( aoi . _filename ) except ValueError : df = pd . read_csv ( aoi . _filename ) df [ 'wetDelay' ] = wetDelay df [ 'hydroDelay' ] = hydroDelay df [ 'totalDelay' ] = wetDelay + hydroDelay df . to_csv ( wetFilename , index = False ) else : writeArrayToRaster ( wetDelay , wetFilename , noDataValue = ndv , fmt = outformat , proj = proj , gt = gt ) writeArrayToRaster ( hydroDelay , hydroFilename , noDataValue = ndv , fmt = outformat , proj = proj , gt = gt ) writePnts2HDF5 ( lats , lons , hgts , los , lengths , outName = 'testx.h5' , chunkSize = None , noDataValue = 0.0 , epsg = 4326 ) Write query points to an HDF5 file for storage and access Source code in RAiDER/utilFcns.py 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 def writePnts2HDF5 ( lats , lons , hgts , los , lengths , outName = 'testx.h5' , chunkSize = None , noDataValue = 0. , epsg = 4326 ): ''' Write query points to an HDF5 file for storage and access ''' projname = 'projection' # converts from WGS84 geodetic to WGS84 geocentric t = Transformer . from_crs ( epsg , 4978 , always_xy = True ) checkLOS ( los , np . prod ( lats . shape )) in_shape = lats . shape # create directory if needed os . makedirs ( os . path . abspath ( os . path . dirname ( outName )), exist_ok = True ) # Set up the chunking if chunkSize is None : chunkSize = getChunkSize ( in_shape ) with h5py . File ( outName , 'w' ) as f : f . attrs [ 'Conventions' ] = np . string_ ( \"CF-1.8\" ) x = f . create_dataset ( 'lon' , data = lons , chunks = chunkSize , fillvalue = noDataValue ) y = f . create_dataset ( 'lat' , data = lats , chunks = chunkSize , fillvalue = noDataValue ) z = f . create_dataset ( 'hgt' , data = hgts , chunks = chunkSize , fillvalue = noDataValue ) los = f . create_dataset ( 'LOS' , data = los , chunks = chunkSize + ( 3 ,), fillvalue = noDataValue ) lengths = f . create_dataset ( 'Rays_len' , data = lengths , chunks = x . chunks , fillvalue = noDataValue ) sp_data = np . stack ( t . transform ( lons , lats , hgts ), axis =- 1 ) . astype ( np . float64 ) sp = f . create_dataset ( 'Rays_SP' , data = sp_data , chunks = chunkSize + ( 3 ,), fillvalue = noDataValue ) x . attrs [ 'Shape' ] = in_shape y . attrs [ 'Shape' ] = in_shape z . attrs [ 'Shape' ] = in_shape los . attrs [ 'Shape' ] = in_shape + ( 3 ,) lengths . attrs [ 'Shape' ] = in_shape lengths . attrs [ 'Units' ] = 'm' sp . attrs [ 'Shape' ] = in_shape + ( 3 ,) f . attrs [ 'ChunkSize' ] = chunkSize f . attrs [ 'NoDataValue' ] = noDataValue # CF 1.8 Convention stuff crs = pyproj . CRS . from_epsg ( epsg ) projds = f . create_dataset ( projname , (), dtype = 'i' ) projds [()] = epsg # WGS84 ellipsoid projds . attrs [ 'semi_major_axis' ] = 6378137.0 projds . attrs [ 'inverse_flattening' ] = 298.257223563 projds . attrs [ 'ellipsoid' ] = np . string_ ( \"WGS84\" ) projds . attrs [ 'epsg_code' ] = epsg # TODO - Remove the wkt version after verification projds . attrs [ 'spatial_ref' ] = np . string_ ( crs . to_wkt ( \"WKT1_GDAL\" )) # Geodetic latitude / longitude if epsg == 4326 : # Set up grid mapping projds . attrs [ 'grid_mapping_name' ] = np . string_ ( 'latitude_longitude' ) projds . attrs [ 'longitude_of_prime_meridian' ] = 0.0 x . attrs [ 'standard_name' ] = np . string_ ( \"longitude\" ) x . attrs [ 'units' ] = np . string_ ( \"degrees_east\" ) y . attrs [ 'standard_name' ] = np . string_ ( \"latitude\" ) y . attrs [ 'units' ] = np . string_ ( \"degrees_north\" ) z . attrs [ 'standard_name' ] = np . string_ ( \"height\" ) z . attrs [ 'units' ] = np . string_ ( \"m\" ) else : raise NotImplementedError los . attrs [ 'grid_mapping' ] = np . string_ ( projname ) sp . attrs [ 'grid_mapping' ] = np . string_ ( projname ) lengths . attrs [ 'grid_mapping' ] = np . string_ ( projname ) f . attrs [ 'NumRays' ] = len ( x ) f [ 'Rays_len' ] . attrs [ 'MaxLen' ] = np . nanmax ( lengths ) writeResultsToHDF5 ( lats , lons , hgts , wet , hydro , filename , delayType = None ) write a 1-D array to a NETCDF5 file Source code in RAiDER/utilFcns.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 def writeResultsToHDF5 ( lats , lons , hgts , wet , hydro , filename , delayType = None ): ''' write a 1-D array to a NETCDF5 file ''' if delayType is None : delayType = \"Zenith\" with h5py . File ( filename , 'w' ) as f : f [ 'lat' ] = lats f [ 'lon' ] = lons f [ 'hgts' ] = hgts f [ 'wetDelay' ] = wet f [ 'hydroDelay' ] = hydro f [ 'wetDelayUnit' ] = \"m\" f [ 'hydroDelayUnit' ] = \"m\" f [ 'hgtsUnit' ] = \"m\" f . attrs [ 'DelayType' ] = delayType writeWeatherVars2NETCDF4 ( self , lat , lon , h , q , p , t , outName = None , NoDataValue = None , chunk = ( 1 , 91 , 144 ), mapping_name = 'WGS84' ) By calling the abstract/modular netcdf writer (RAiDER.utilFcns.write2NETCDF4core), write the OpenDAP/PyDAP-retrieved weather model data (GMAO and MERRA-2) to a NETCDF4 file that can be accessed by external programs. The point of doing this is to alleviate some of the memory load of keeping the full model in memory and make it easier to scale up the program. Source code in RAiDER/utilFcns.py 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 def writeWeatherVars2NETCDF4 ( self , lat , lon , h , q , p , t , outName = None , NoDataValue = None , chunk = ( 1 , 91 , 144 ), mapping_name = 'WGS84' ): ''' By calling the abstract/modular netcdf writer (RAiDER.utilFcns.write2NETCDF4core), write the OpenDAP/PyDAP-retrieved weather model data (GMAO and MERRA-2) to a NETCDF4 file that can be accessed by external programs. The point of doing this is to alleviate some of the memory load of keeping the full model in memory and make it easier to scale up the program. ''' import netCDF4 if outName is None : outName = os . path . join ( os . getcwd () + '/weather_files' , self . _Name + datetime . strftime ( self . _time , '_%Y_%m_ %d _T%H_%M_%S' ) + '.nc' ) if NoDataValue is None : NoDataValue = - 9999. self . _time = getTimeFromFile ( outName ) dimidZ , dimidY , dimidX = t . shape chunk_lines_Y = np . min ([ chunk [ 1 ], dimidY ]) chunk_lines_X = np . min ([ chunk [ 2 ], dimidX ]) ChunkSize = [ 1 , chunk_lines_Y , chunk_lines_X ] nc_outfile = netCDF4 . Dataset ( outName , 'w' , clobber = True , format = 'NETCDF4' ) nc_outfile . setncattr ( 'Conventions' , 'CF-1.6' ) nc_outfile . setncattr ( 'datetime' , datetime . strftime ( self . _time , \"%Y_%m_ %d T%H_%M_%S\" )) nc_outfile . setncattr ( 'date_created' , datetime . now () . strftime ( \"%Y_%m_ %d T%H_%M_%S\" )) title = self . _Name + ' weather model data' nc_outfile . setncattr ( 'title' , title ) tran = [ lon [ 0 ], lon [ 1 ] - lon [ 0 ], 0.0 , lat [ 0 ], 0.0 , lat [ 1 ] - lat [ 0 ]] dimension_dict = { 'x' : { 'varname' : 'x' , 'datatype' : np . dtype ( 'float64' ), 'dimensions' : ( 'x' ), 'length' : dimidX , 'FillValue' : None , 'standard_name' : 'longitude' , 'description' : 'longitude' , 'dataset' : lon , 'units' : 'degrees_east' }, 'y' : { 'varname' : 'y' , 'datatype' : np . dtype ( 'float64' ), 'dimensions' : ( 'y' ), 'length' : dimidY , 'FillValue' : None , 'standard_name' : 'latitude' , 'description' : 'latitude' , 'dataset' : lat , 'units' : 'degrees_north' }, 'z' : { 'varname' : 'z' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' ), 'length' : dimidZ , 'FillValue' : None , 'standard_name' : 'model_layers' , 'description' : 'model layers' , 'dataset' : np . arange ( dimidZ ), 'units' : 'layer' } } dataset_dict = { 'h' : { 'varname' : 'H' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'mid_layer_heights' , 'description' : 'mid layer heights' , 'dataset' : h , 'units' : 'm' }, 'q' : { 'varname' : 'QV' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'specific_humidity' , 'description' : 'specific humidity' , 'dataset' : q , 'units' : 'kg kg-1' }, 'p' : { 'varname' : 'PL' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'mid_level_pressure' , 'description' : 'mid level pressure' , 'dataset' : p , 'units' : 'Pa' }, 't' : { 'varname' : 'T' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'air_temperature' , 'description' : 'air temperature' , 'dataset' : t , 'units' : 'K' } } nc_outfile = write2NETCDF4core ( nc_outfile , dimension_dict , dataset_dict , tran , mapping_name = 'WGS84' ) nc_outfile . sync () # flush data to disk nc_outfile . close ()","title":"API Reference"},{"location":"reference/#raider-v030-api-reference","text":"Raytracing Atmospheric Delay Estimation for RADAR Copyright (c) 2019-2022, California Institute of Technology (\"Caltech\"). All rights reserved.","title":"RAiDER v0.3.0 API Reference"},{"location":"reference/#RAiDER.checkArgs","text":"","title":"checkArgs"},{"location":"reference/#RAiDER.checkArgs.checkArgs","text":"Helper fcn for checking argument compatibility and returns the correct variables Source code in RAiDER/checkArgs.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def checkArgs ( args ): ''' Helper fcn for checking argument compatibility and returns the correct variables ''' ######################################################################################################################### # Directories if not os . path . exists ( args . weather_model_directory ): os . mkdir ( args . weather_model_directory ) ######################################################################################################################### # Date and Time parsing args . date_list = [ datetime . combine ( d , args . time ) for d in args . date_list ] ######################################################################################################################### # filenames wetNames , hydroNames = [], [] for d in args . date_list : if ( args . aoi . type () != 'bounding_box' ): # Handle the GNSS station file if ( args . aoi . type () == 'station_file' ): wetFilename = os . path . join ( args . output_directory , ' {} _Delay_ {} .csv' . format ( args . weather_model . Model (), d . strftime ( '%Y%m %d T%H%M%S' ), ) ) hydroFilename = None # only the 'wetFilename' is used for the station_file # copy the input station file to the output location for editing indf = pd . read_csv ( args . aoi . _filename ) . drop_duplicates ( subset = [ \"Lat\" , \"Lon\" ]) indf . to_csv ( wetFilename , index = False ) else : # This implies rasters fmt = get_raster_ext ( args . file_format ) wetFilename , hydroFilename = makeDelayFileNames ( d , args . los , fmt , args . weather_model . _dataset . upper (), args . output_directory , ) else : # In this case a cube file format is needed if args . file_format not in '.nc .h5 h5 hdf5 .hdf5 nc' . split (): fmt = 'nc' logger . debug ( 'Invalid extension %s for cube. Defaulting to .nc' , args . file_format ) else : fmt = args . file_format . strip ( '.' ) . replace ( 'df' , '' ) wetFilename , hydroFilename = makeDelayFileNames ( d , args . los , fmt , args . weather_model . _dataset . upper (), args . output_directory , ) wetNames . append ( wetFilename ) hydroNames . append ( hydroFilename ) args . wetFilenames = wetNames args . hydroFilenames = hydroNames return args","title":"checkArgs()"},{"location":"reference/#RAiDER.checkArgs.makeDelayFileNames","text":"return names for the wet and hydrostatic delays.","title":"makeDelayFileNames()"},{"location":"reference/#RAiDER.checkArgs.makeDelayFileNames--examples","text":"makeDelayFileNames(datetime(2020, 1, 1, 0, 0, 0), None, \"h5\", \"model_name\", \"some_dir\") ('some_dir/model_name_wet_00_00_00_ztd.h5', 'some_dir/model_name_hydro_00_00_00_ztd.h5') makeDelayFileNames(None, None, \"h5\", \"model_name\", \"some_dir\") ('some_dir/model_name_wet_ztd.h5', 'some_dir/model_name_hydro_ztd.h5') Source code in RAiDER/checkArgs.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def makeDelayFileNames ( time , los , outformat , weather_model_name , out ): ''' return names for the wet and hydrostatic delays. # Examples: >>> makeDelayFileNames(datetime(2020, 1, 1, 0, 0, 0), None, \"h5\", \"model_name\", \"some_dir\") ('some_dir/model_name_wet_00_00_00_ztd.h5', 'some_dir/model_name_hydro_00_00_00_ztd.h5') >>> makeDelayFileNames(None, None, \"h5\", \"model_name\", \"some_dir\") ('some_dir/model_name_wet_ztd.h5', 'some_dir/model_name_hydro_ztd.h5') ''' format_string = \" {model_name} _{{}}_ {time}{los} . {ext} \" . format ( model_name = weather_model_name , time = time . strftime ( \"%Y%m %d T%H%M%S_\" ) if time is not None else \"\" , los = \"ztd\" if ( isinstance ( los , Zenith ) or los is None ) else \"std\" , ext = outformat ) hydroname , wetname = ( format_string . format ( dtyp ) for dtyp in ( 'hydro' , 'wet' ) ) hydro_file_name = os . path . join ( out , hydroname ) wet_file_name = os . path . join ( out , wetname ) return wet_file_name , hydro_file_name","title":"Examples:"},{"location":"reference/#RAiDER.cli","text":"","title":"cli"},{"location":"reference/#RAiDER.cli.raider","text":"","title":"raider"},{"location":"reference/#RAiDER.cli.raider.calcDelays","text":"Parse command line arguments using argparse. Source code in RAiDER/cli/raider.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 def calcDelays ( iargs = None ): \"\"\" Parse command line arguments using argparse. \"\"\" import RAiDER from RAiDER.delay import tropo_delay from RAiDER.checkArgs import checkArgs from RAiDER.processWM import prepareWeatherModel from RAiDER.utilFcns import writeDelays examples = 'Examples of use:' \\ ' \\n\\t raider.py customTemplatefile.cfg' \\ ' \\n\\t raider.py -g' p = argparse . ArgumentParser ( description = 'Command line interface for RAiDER processing with a configure file.' 'Default options can be found by running: raider.py --generate_config' , epilog = examples , formatter_class = argparse . RawDescriptionHelpFormatter ) p . add_argument ( 'customTemplateFile' , nargs = '?' , help = 'custom template with option settings. \\n ' + \"ignored if the default smallbaselineApp.cfg is input.\" ) p . add_argument ( '-g' , '--generate_template' , action = 'store_true' , help = 'generate default template (if it does not exist) and exit.' ) p . add_argument ( '--download_only' , action = 'store_true' , help = 'only download a weather model.' ) ## if not None, will replace first argument (customTemplateFile) args = p . parse_args ( args = iargs ) # default input file template_file = os . path . join ( os . path . dirname ( RAiDER . __file__ ), 'cli' , 'raider.yaml' ) if args . generate_template : dst = os . path . join ( os . getcwd (), 'raider.yaml' ) shutil . copyfile ( template_file , dst ) logger . info ( 'Wrote %s ' , dst ) os . sys . exit () # check: existence of input template files if ( not args . customTemplateFile and not os . path . isfile ( os . path . basename ( template_file )) and not args . generate_template ): msg = \"No template file found! It requires that either:\" msg += \" \\n a custom template file, OR the default template \" msg += \" \\n file 'raider.yaml' exists in current directory.\" p . print_usage () print ( examples ) raise SystemExit ( f 'ERROR: { msg } ' ) if args . customTemplateFile : # check the existence if not os . path . isfile ( args . customTemplateFile ): raise FileNotFoundError ( args . customTemplateFile ) args . customTemplateFile = os . path . abspath ( args . customTemplateFile ) else : args . customTemplateFile = template_file # Read the template file params = read_template_file ( args . customTemplateFile ) # Argument checking params = checkArgs ( params ) if not params . verbose : logger . setLevel ( logging . INFO ) delay_dct = {} for t , w , f in zip ( params [ 'date_list' ], params [ 'wetFilenames' ], params [ 'hydroFilenames' ] ): los = params [ 'los' ] aoi = params [ 'aoi' ] model = params [ 'weather_model' ] if los . ray_trace (): ll_bounds = aoi . add_buffer ( buffer = 1 ) # add a buffer for raytracing else : ll_bounds = aoi . bounds () ########################################################### # weather model calculation logger . debug ( 'Starting to run the weather model calculation' ) logger . debug ( 'Time: {} ' . format ( t . strftime ( '%Y%m %d ' ))) logger . debug ( 'Beginning weather model pre-processing' ) try : weather_model_file = prepareWeatherModel ( model , t , ll_bounds = ll_bounds , # SNWE wmLoc = params [ 'weather_model_directory' ], makePlots = params [ 'verbose' ], ) except RuntimeError : logger . exception ( \"Date %s failed\" , t ) continue # Now process the delays try : wet_delay , hydro_delay = tropo_delay ( t , weather_model_file , aoi , los , height_levels = params [ 'height_levels' ], out_proj = params [ 'output_projection' ], look_dir = params [ 'look_dir' ], cube_spacing_m = params [ 'cube_spacing_in_m' ], ) except RuntimeError : logger . exception ( \"Date %s failed\" , t ) continue ########################################################### # Write the delays to file # Different options depending on the inputs if los . is_Projected (): out_filename = w . replace ( \"_ztd\" , \"_std\" ) f = f . replace ( \"_ztd\" , \"_std\" ) elif los . ray_trace (): out_filename = w . replace ( \"_std\" , \"_ray\" ) f = f . replace ( \"_std\" , \"_ray\" ) else : out_filename = w if hydro_delay is None : # means that a dataset was returned ds = wet_delay ext = os . path . splitext ( out_filename )[ 1 ] if ext not in [ '.nc' , '.h5' ]: out_filename = f ' { os . path . splitext ( out_filename )[ 0 ] } .nc' out_filename = out_filename . replace ( \"wet\" , \"tropo\" ) if out_filename . endswith ( \".nc\" ): ds . to_netcdf ( out_filename , mode = \"w\" ) elif out_filename . endswith ( \".h5\" ): ds . to_netcdf ( out_filename , engine = \"h5netcdf\" , invalid_netcdf = True ) else : if aoi . type () == 'station_file' : out_filename = f ' { os . path . splitext ( out_filename )[ 0 ] } .csv' if aoi . type () in [ 'station_file' , 'radar_rasters' , 'geocoded_file' ]: writeDelays ( aoi , wet_delay , hydro_delay , out_filename , f , outformat = params [ 'raster_format' ]) logger . info ( 'Wrote hydro delays to: %s ' , f ) logger . info ( 'Wrote wet delays to: %s ' , out_filename ) # delay_dct[t] = wet_delay, hydro_delay delay_dct [ t ] = out_filename , f return delay_dct","title":"calcDelays()"},{"location":"reference/#RAiDER.cli.raider.create_parser","text":"Parse command line arguments using argparse. Source code in RAiDER/cli/raider.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def create_parser (): \"\"\"Parse command line arguments using argparse.\"\"\" p = argparse . ArgumentParser ( formatter_class = argparse . RawDescriptionHelpFormatter , description = HELP_MESSAGE , epilog = EXAMPLES , ) p . add_argument ( 'customTemplateFile' , nargs = '?' , help = 'custom template with option settings. \\n ' + \"ignored if the default smallbaselineApp.cfg is input.\" ) p . add_argument ( '-g' , '--generate_template' , dest = 'generate_template' , action = 'store_true' , help = 'generate default template (if it does not exist) and exit.' ) p . add_argument ( '--download-only' , action = 'store_true' , help = 'only download a weather model.' ) return p","title":"create_parser()"},{"location":"reference/#RAiDER.cli.raider.downloadGNSS","text":"Parse command line arguments using argparse. Source code in RAiDER/cli/raider.py 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 def downloadGNSS (): \"\"\"Parse command line arguments using argparse.\"\"\" from RAiDER.gnss.downloadGNSSDelays import main as dlGNSS p = argparse . ArgumentParser ( formatter_class = argparse . RawDescriptionHelpFormatter , description = \"\"\" \\ Check for and download tropospheric zenith delays for a set of GNSS stations from UNR Example call to virtually access and append zenith delay information to a CSV table in specified output directory, across specified range of time (in YYMMDD YYMMDD) and all available times of day, and confined to specified geographic bounding box : downloadGNSSdelay.py --out products -y 20100101 20141231 -b '39 40 -79 -78' Example call to virtually access and append zenith delay information to a CSV table in specified output directory, across specified range of time (in YYMMDD YYMMDD) and specified time of day, and distributed globally : downloadGNSSdelay.py --out products -y 20100101 20141231 --returntime '00:00:00' Example call to virtually access and append zenith delay information to a CSV table in specified output directory, across specified range of time in 12 day steps (in YYMMDD YYMMDD days) and specified time of day, and distributed globally : downloadGNSSdelay.py --out products -y 20100101 20141231 12 --returntime '00:00:00' Example call to virtually access and append zenith delay information to a CSV table in specified output directory, across specified range of time (in YYMMDD YYMMDD) and specified time of day, and distributed globally but restricted to list of stations specified in input textfile : downloadGNSSdelay.py --out products -y 20100101 20141231 --returntime '00:00:00' -f station_list.txt NOTE, following example call to physically download zenith delay information not recommended as it is not necessary for most applications. Example call to physically download and append zenith delay information to a CSV table in specified output directory, across specified range of time (in YYMMDD YYMMDD) and specified time of day, and confined to specified geographic bounding box : downloadGNSSdelay.py --download --out products -y 20100101 20141231 --returntime '00:00:00' -b '39 40 -79 -78' \"\"\" ) # Stations to check/download area = p . add_argument_group ( 'Stations to check/download. Can be a lat/lon bounding box or file, or will run the whole world if not specified' ) area . add_argument ( '--station_file' , '-f' , default = None , dest = 'station_file' , help = ( 'Text file containing a list of 4-char station IDs separated by newlines' )) area . add_argument ( '-b' , '--bounding_box' , dest = 'bounding_box' , type = str , default = None , help = \"Provide either valid shapefile or Lat/Lon Bounding SNWE. -- Example : '19 20 -99.5 -98.5'\" ) area . add_argument ( '--gpsrepo' , '-gr' , default = 'UNR' , dest = 'gps_repo' , help = ( 'Specify GPS repository you wish to query. Currently supported archives: UNR.' )) misc = p . add_argument_group ( \"Run parameters\" ) add_out ( misc ) misc . add_argument ( '--date' , dest = 'dateList' , help = dedent ( \"\"\" \\ Date to calculate delay. Can be a single date, a list of two dates (earlier, later) with 1-day interval, or a list of two dates and interval in days (earlier, later, interval). Example accepted formats: YYYYMMDD or YYYYMMDD YYYYMMDD YYYYMMDD YYYYMMDD N \"\"\" ), nargs = \"+\" , action = DateListAction , type = date_type , required = True ) misc . add_argument ( '--returntime' , dest = 'returnTime' , help = \"Return delays closest to this specified time. If not specified, the GPS delays for all times will be returned. Input in 'HH:MM:SS', e.g. '16:00:00'\" , default = None ) misc . add_argument ( '--download' , help = 'Physically download data. Note this option is not necessary to proceed with statistical analyses, as data can be handled virtually in the program.' , action = 'store_true' , dest = 'download' , default = False ) add_cpus ( misc ) add_verbose ( misc ) args = p . parse_args () dlGNSS ( args ) return","title":"downloadGNSS()"},{"location":"reference/#RAiDER.cli.raider.parseCMD","text":"Parse command-line arguments and pass to delay.py Source code in RAiDER/cli/raider.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def parseCMD ( iargs = None ): \"\"\" Parse command-line arguments and pass to delay.py \"\"\" p = create_parser () args = p . parse_args ( args = iargs ) args . argv = iargs if iargs else sys . argv [ 1 :] # default input file template_file = os . path . join ( os . path . dirname ( RAiDER . __file__ ), 'cli' , 'raider.yaml' ) if '-g' in args . argv : dst = os . path . join ( os . getcwd (), 'raider.yaml' ) shutil . copyfile ( template_file , dst , ) logger . info ( 'Wrote %s ' , dst ) sys . exit ( 0 ) # check: existence of input template files if ( not args . customTemplateFile and not os . path . isfile ( os . path . basename ( template_file )) and not args . generate_template ): p . print_usage () print ( EXAMPLES ) msg = \"No template file found! It requires that either:\" msg += \" \\n a custom template file, OR the default template \" msg += \" \\n file 'raider.yaml' exists in current directory.\" raise SystemExit ( f 'ERROR: { msg } ' ) if args . customTemplateFile : # check the existence if not os . path . isfile ( args . customTemplateFile ): raise FileNotFoundError ( args . customTemplateFile ) args . customTemplateFile = os . path . abspath ( args . customTemplateFile ) return args","title":"parseCMD()"},{"location":"reference/#RAiDER.cli.raider.read_template_file","text":"Read the template file into a dictionary structure. Args: fname (str): full path to the template file Returns: Name Type Description dict arguments to pass to RAiDER functions Examples: template = read_template_file('raider.yaml') Source code in RAiDER/cli/raider.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def read_template_file ( fname ): \"\"\" Read the template file into a dictionary structure. Args: fname (str): full path to the template file Returns: dict: arguments to pass to RAiDER functions Examples: >>> template = read_template_file('raider.yaml') \"\"\" from RAiDER.cli.validators import ( enforce_time , enforce_bbox , parse_dates , get_query_region , get_heights , get_los , enforce_wm ) with open ( fname , 'r' ) as f : try : params = yaml . safe_load ( f ) except yaml . YAMLError as exc : print ( exc ) raise ValueError ( 'Something is wrong with the yaml file {} ' . format ( fname )) # Drop any values not specified params = drop_nans ( params ) # Need to ensure that all the groups exist, even if they are not specified by the user group_keys = [ 'date_group' , 'time_group' , 'aoi_group' , 'height_group' , 'los_group' , 'runtime_group' ] for key in group_keys : if not key in params . keys (): params [ key ] = {} # Parse the user-provided arguments template = DEFAULT_DICT for key , value in params . items (): if key == 'runtime_group' : for k , v in value . items (): if v is not None : template [ k ] = v if key == 'weather_model' : template [ key ] = enforce_wm ( value ) if key == 'time_group' : template . update ( enforce_time ( AttributeDict ( value ))) if key == 'date_group' : template [ 'date_list' ] = parse_dates ( AttributeDict ( value )) if key == 'aoi_group' : ## in case a DEM is passed and should be used dct_temp = { ** AttributeDict ( value ), ** AttributeDict ( params [ 'height_group' ])} template [ 'aoi' ] = get_query_region ( AttributeDict ( dct_temp )) if key == 'los_group' : template [ 'los' ] = get_los ( AttributeDict ( value )) if key == 'look_dir' : if value . lower () not in [ 'right' , 'left' ]: raise ValueError ( f \"Unknown look direction { value } \" ) template [ 'look_dir' ] = value . lower () # Have to guarantee that certain variables exist prior to looking at heights for key , value in params . items (): if key == 'height_group' : template . update ( get_heights ( AttributeDict ( value ), template [ 'output_directory' ], template [ 'station_file' ], template [ 'bounding_box' ], ) ) return AttributeDict ( template )","title":"read_template_file()"},{"location":"reference/#RAiDER.cli.statsPlot","text":"","title":"statsPlot"},{"location":"reference/#RAiDER.cli.statsPlot.RaiderStats","text":"Bases: object Class which loads standard weather model/GPS delay files and generates a series of user-requested statistics and graphics. Source code in RAiDER/cli/statsPlot.py 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 class RaiderStats ( object ): ''' Class which loads standard weather model/GPS delay files and generates a series of user-requested statistics and graphics. ''' # import dependencies import glob def __init__ ( self , filearg , col_name , unit = 'm' , workdir = './' , bbox = None , spacing = 1 , timeinterval = None , seasonalinterval = None , obs_errlimit = 'inf' , time_lines = False , stationsongrids = False , station_seasonal_phase = False , cbounds = None , colorpercentile = [ 25 , 95 ], usr_colormap = 'hot_r' , grid_heatmap = False , grid_delay_mean = False , grid_delay_median = False , grid_delay_stdev = False , grid_seasonal_phase = False , grid_delay_absolute_mean = False , grid_delay_absolute_median = False , grid_delay_absolute_stdev = False , grid_seasonal_absolute_phase = False , grid_to_raster = False , min_span = [ 2 , 0.6 ], period_limit = 0.5 , numCPUs = 8 , phaseamp_per_station = False ): self . fname = filearg self . col_name = col_name self . unit = unit self . workdir = workdir self . bbox = bbox self . spacing = spacing self . timeinterval = timeinterval self . seasonalinterval = seasonalinterval self . obs_errlimit = float ( obs_errlimit ) self . time_lines = time_lines self . stationsongrids = stationsongrids self . station_seasonal_phase = station_seasonal_phase self . cbounds = cbounds self . colorpercentile = colorpercentile self . usr_colormap = usr_colormap self . grid_heatmap = grid_heatmap self . grid_delay_mean = grid_delay_mean self . grid_delay_median = grid_delay_median self . grid_delay_stdev = grid_delay_stdev self . grid_seasonal_phase = grid_seasonal_phase self . grid_seasonal_amplitude = False self . grid_seasonal_period = False self . grid_seasonal_phase_stdev = False self . grid_seasonal_amplitude_stdev = False self . grid_seasonal_period_stdev = False self . grid_seasonal_fit_rmse = False self . grid_delay_absolute_mean = grid_delay_absolute_mean self . grid_delay_absolute_median = grid_delay_absolute_median self . grid_delay_absolute_stdev = grid_delay_absolute_stdev self . grid_seasonal_absolute_phase = grid_seasonal_absolute_phase self . grid_seasonal_absolute_amplitude = False self . grid_seasonal_absolute_period = False self . grid_seasonal_absolute_phase_stdev = False self . grid_seasonal_absolute_amplitude_stdev = False self . grid_seasonal_absolute_period_stdev = False self . grid_seasonal_absolute_fit_rmse = False self . grid_to_raster = grid_to_raster self . min_span = min_span self . period_limit = period_limit self . numCPUs = numCPUs self . phaseamp_per_station = phaseamp_per_station self . grid_range = False self . grid_variance = False self . grid_variogram_rmse = False # create workdir if it doesn't exist if not os . path . exists ( self . workdir ): os . mkdir ( self . workdir ) # get colorbounds if self . cbounds : self . cbounds = [ float ( val ) for val in self . cbounds . split ()] # Pass color percentile and check for input error if self . colorpercentile is None : self . colorpercentile = [ 25 , 95 ] if self . colorpercentile [ 0 ] > self . colorpercentile [ 1 ]: raise Exception ( 'Input colorpercentile lower threshold {} higher than upper threshold {} ' . format ( self . colorpercentile [ 0 ], self . colorpercentile [ 1 ])) # load dataframe directly if previously generated TIF grid-file if self . fname . endswith ( '.tif' ): if 'grid_heatmap' in self . fname : self . grid_heatmap , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_heatmap' )[ 0 ] if 'grid_delay_mean' in self . fname : self . grid_delay_mean , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_delay_mean' )[ 0 ] if 'grid_delay_median' in self . fname : self . grid_delay_median , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_delay_median' )[ 0 ] if 'grid_delay_stdev' in self . fname : self . grid_delay_stdev , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_delay_stdev' )[ 0 ] if 'grid_seasonal_phase' in self . fname : self . grid_seasonal_phase , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_seasonal_phase' )[ 0 ] if 'grid_seasonal_period' in self . fname : self . grid_seasonal_period , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_seasonal_period' )[ 0 ] if 'grid_seasonal_amplitude' in self . fname : self . grid_seasonal_amplitude , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_seasonal_amplitude' )[ 0 ] if 'grid_seasonal_phase_stdev' in self . fname : self . grid_seasonal_phase_stdev , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_seasonal_phase_stdev' )[ 0 ] if 'grid_seasonal_amplitude_stdev' in self . fname : self . grid_seasonal_amplitude_stdev , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_seasonal_amplitude_stdev' )[ 0 ] if 'grid_seasonal_period_stdev' in self . fname : self . grid_seasonal_period_stdev , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_seasonal_period_stdev' )[ 0 ] if 'grid_seasonal_fit_rmse' in self . fname : self . grid_seasonal_fit_rmse , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_seasonal_fit_rmse' )[ 0 ] if 'grid_delay_absolute_mean' in self . fname : self . grid_delay_absolute_mean , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_delay_absolute_mean' )[ 0 ] if 'grid_delay_absolute_median' in self . fname : self . grid_delay_absolute_median , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_delay_absolute_median' )[ 0 ] if 'grid_delay_absolute_stdev' in self . fname : self . grid_delay_absolute_stdev , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_delay_absolute_stdev' )[ 0 ] if 'grid_seasonal_absolute_phase' in self . fname : self . grid_seasonal_absolute_phase , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_seasonal_absolute_phase' )[ 0 ] if 'grid_seasonal_absolute_period' in self . fname : self . grid_seasonal_absolute_period , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_seasonal_absolute_period' )[ 0 ] if 'grid_seasonal_absolute_amplitude' in self . fname : self . grid_seasonal_absolute_amplitude , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_seasonal_absolute_amplitude' )[ 0 ] if 'grid_seasonal_absolute_phase_stdev' in self . fname : self . grid_seasonal_absolute_phase_stdev , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_seasonal_absolute_phase_stdev' )[ 0 ] if 'grid_seasonal_absolute_amplitude_stdev' in self . fname : self . grid_seasonal_absolute_amplitude_stdev , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_seasonal_absolute_amplitude_stdev' )[ 0 ] if 'grid_seasonal_absolute_period_stdev' in self . fname : self . grid_seasonal_absolute_period_stdev , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_seasonal_absolute_period_stdev' )[ 0 ] if 'grid_seasonal_absolute_fit_rmse' in self . fname : self . grid_seasonal_absolute_fit_rmse , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_seasonal_absolute_fit_rmse' )[ 0 ] if 'grid_range' in self . fname : self . grid_range , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_range' )[ 0 ] if 'grid_variance' in self . fname : self . grid_variance , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_variance' )[ 0 ] if 'grid_variogram_rmse' in self . fname : self . grid_variogram_rmse , self . plotbbox , self . spacing , self . colorbarfmt , self . stationsongrids , self . time_lines = load_gridfile ( self . fname , self . unit ) self . col_name = os . path . basename ( self . fname ) . split ( '_' + 'grid_variogram_rmse' )[ 0 ] # setup dataframe for statistical analyses (if CSV) if self . fname . endswith ( '.csv' ): self . create_DF () def _get_extent ( self ): # dataset, spacing=1, userbbox=None \"\"\" Get the bbox, spacing in deg (by default 1deg), optionally pass user-specified bbox. Output array in WESN degrees \"\"\" extent = [ np . floor ( min ( self . df [ 'Lon' ])), np . ceil ( max ( self . df [ 'Lon' ])), np . floor ( min ( self . df [ 'Lat' ])), np . ceil ( max ( self . df [ 'Lat' ]))] if self . bbox is not None : dfextents_poly = Polygon ( np . column_stack (( np . array ([ extent [ 0 ], extent [ 0 ], extent [ 1 ], extent [ 1 ], extent [ 0 ]]), np . array ([ extent [ 2 ], extent [ 3 ], extent [ 3 ], extent [ 2 ], extent [ 2 ]])))) userbbox_poly = Polygon ( np . column_stack (( np . array ([ self . bbox [ 2 ], self . bbox [ 3 ], self . bbox [ 3 ], self . bbox [ 2 ], self . bbox [ 2 ]]), np . array ([ self . bbox [ 0 ], self . bbox [ 0 ], self . bbox [ 1 ], self . bbox [ 1 ], self . bbox [ 0 ]])))) if userbbox_poly . intersects ( dfextents_poly ): extent = [ np . floor ( self . bbox [ 2 ]), np . ceil ( self . bbox [ - 1 ]), np . floor ( self . bbox [ 0 ]), np . ceil ( self . bbox [ 1 ])] else : raise Exception ( \"User-specified bounds do not overlap with dataset bounds, adjust bounds and re-run program.\" ) if extent [ 0 ] < - 180. or extent [ 1 ] > 180. or extent [ 2 ] < - 90. or extent [ 3 ] > 90. : raise Exception ( \"Specified bounds exceed -180/180 lon and/or -90/90 lat, adjust bounds and re-run program.\" ) del dfextents_poly , userbbox_poly # ensure that extents do not exceed -180/180 lon and -90/90 lat if extent [ 0 ] < - 180. : extent [ 0 ] = - 180. if extent [ 1 ] > 180. : extent [ 1 ] = 180. if extent [ 2 ] < - 90. : extent [ 2 ] = - 90. if extent [ 3 ] > 90. : extent [ 3 ] = 90. # ensure even spacing, set spacing to 1 if specified spacing is not even multiple of bounds if ( extent [ 1 ] - extent [ 0 ]) % self . spacing != 0 or ( extent [ - 1 ] - extent [ - 2 ]) % self . spacing : logger . warning ( \"User-specified spacing %s is not even multiple of bounds, resetting spacing to 1 \\N{DEGREE SIGN} \" , self . spacing ) self . spacing = 1 # Create corners of rectangle to be transformed to a grid nw = [ extent [ 0 ] + ( self . spacing / 2 ), extent [ - 1 ] - ( self . spacing / 2 )] se = [ extent [ 1 ] - ( self . spacing / 2 ), extent [ 2 ] + ( self . spacing / 2 )] # Store grid dimension [y,x] grid_dim = [ int (( extent [ 1 ] - extent [ 0 ]) / self . spacing ), int (( extent [ - 1 ] - extent [ - 2 ]) / self . spacing )] # Iterate over 2D area gridpoints = [] y_shape = [] x_shape = [] x = se [ 0 ] while x >= nw [ 0 ]: y = se [ 1 ] while y <= nw [ 1 ]: y_shape . append ( y ) gridpoints . append ([ x , y ]) y += self . spacing x_shape . append ( x ) x -= self . spacing gridpoints . reverse () return extent , grid_dim , gridpoints def _check_stationgrid_intersection ( self , stat_ID ): ''' Return index of grid cell which intersects with station Note: Fast, but assumes station locations don't change ''' coord = Point (( self . unique_points [ 1 ][ self . unique_points [ 0 ] . index ( stat_ID )], self . unique_points [ 2 ][ self . unique_points [ 0 ] . index ( stat_ID )])) # Get grid cell polygon which intersect with station coordinate grid_int = self . polygon_tree . query ( coord ) # Pass corresponding grid cell index if grid_int : return self . polygon_dict [ id ( grid_int [ 0 ])] return 'NaN' def _reader ( self ): ''' Read a input file ''' try : data = pd . read_csv ( self . fname , parse_dates = [ 'Datetime' ]) data [ 'Date' ] = data [ 'Datetime' ] . apply ( lambda x : x . date ()) data [ 'Date' ] = data [ 'Date' ] . apply ( lambda x : dt . datetime . strptime ( x . strftime ( \"%Y-%m- %d \" ), \"%Y-%m- %d \" )) except BaseException : data = pd . read_csv ( self . fname , parse_dates = [ 'Date' ]) # check if user-specified key is valid if self . col_name not in data . keys (): raise Exception ( 'User-specified key {} not found in input file {} . Must specify valid key.' . format ( self . col_name , self . fname )) # if user-specified key is the same as the 'Date' field, rename if self . col_name == 'Date' : logger . warning ( 'Input key {} same as \"Date\" field name, rename the former' . format ( self . col_name )) self . col_name += '_plot' data [ self . col_name ] = data [ 'Date' ] # convert to specified output unit inputunit = 'm' data [ self . col_name ] = convert_SI ( data [ self . col_name ], inputunit , self . unit ) # filter out obs by error if 'sigZTD' in data . keys (): data [ 'sigZTD' ] = convert_SI ( data [ 'sigZTD' ], inputunit , self . unit ) self . obs_errlimit = convert_SI ( self . obs_errlimit , inputunit , self . unit ) data = data [ data [ 'sigZTD' ] <= self . obs_errlimit ] else : logger . warning ( 'Key \"sigZTD\" not found in dataset, cannot filter out obs by error' ) return data def create_DF ( self ): ''' Create dataframe. ''' # Open file self . df = self . _reader () # Filter dataframe # drop all nans self . df . dropna ( how = 'any' , inplace = True ) self . df . reset_index ( drop = True , inplace = True ) # convert to datetime object # time-interval filter if self . timeinterval : self . timeinterval = [ dt . datetime . strptime ( val , '%Y-%m- %d ' ) for val in self . timeinterval . split ()] self . df = self . df [( self . df [ 'Date' ] >= self . timeinterval [ 0 ]) & ( self . df [ 'Date' ] <= self . timeinterval [ - 1 ])] # seasonal filter if self . seasonalinterval : self . seasonalinterval = self . seasonalinterval . split () # get day of year self . seasonalinterval = [ dt . datetime . strptime ( '2001-' + self . seasonalinterval [ 0 ], '%Y-%m- %d ' ) . timetuple ( ) . tm_yday , dt . datetime . strptime ( '2001-' + self . seasonalinterval [ - 1 ], '%Y-%m- %d ' ) . timetuple () . tm_yday ] # track input order and wrap around year if necessary # e.g. month/day: 03/01 to 06/01 if self . seasonalinterval [ 0 ] < self . seasonalinterval [ 1 ]: # non leap-year filtered_self = self . df [( not self . df [ 'Date' ] . dt . is_leap_year ) & ( self . df [ 'Date' ] . dt . dayofyear >= self . seasonalinterval [ 0 ]) & ( self . df [ 'Date' ] . dt . dayofyear <= self . seasonalinterval [ - 1 ])] # leap-year self . seasonalinterval = [ i + 1 if i > 59 else i for i in self . seasonalinterval ] self . df = filtered_self . append ( self . df [( self . df [ 'Date' ] . dt . is_leap_year ) & ( self . df [ 'Date' ] . dt . dayofyear >= self . seasonalinterval [ 0 ]) & ( self . df [ 'Date' ] . dt . dayofyear <= self . seasonalinterval [ - 1 ])], ignore_index = True ) del filtered_self # e.g. month/day: 12/01 to 03/01 if self . seasonalinterval [ 0 ] > self . seasonalinterval [ 1 ]: # non leap-year filtered_self = self . df [( not self . df [ 'Date' ] . dt . is_leap_year ) & ( self . df [ 'Date' ] . dt . dayofyear >= self . seasonalinterval [ - 1 ]) & ( self . df [ 'Date' ] . dt . dayofyear <= self . seasonalinterval [ 0 ])] # leap-year self . seasonalinterval = [ i + 1 if i > 59 else i for i in self . seasonalinterval ] self . df = filtered_self . append ( self . df [( self . df [ 'Date' ] . dt . is_leap_year ) & ( self . df [ 'Date' ] . dt . dayofyear >= self . seasonalinterval [ - 1 ]) & ( self . df [ 'Date' ] . dt . dayofyear <= self . seasonalinterval [ 0 ])], ignore_index = True ) del filtered_self # estimate central longitude lines if '--time_lines' specified if self . time_lines and 'Datetime' in self . df . keys (): self . df [ 'Date_hr' ] = self . df [ 'Datetime' ] . dt . hour . astype ( float ) . astype ( \"Int32\" ) # get list of unique times all_hrs = sorted ( set ( self . df [ 'Date_hr' ])) # get central longitude bands associated with each time central_points = [] # if single time, avoid loop if len ( all_hrs ) == 1 : central_points . append (([ 0 , max ( self . df [ 'Lon' ])], [ 0 , min ( self . df [ 'Lon' ])])) else : for i in enumerate ( all_hrs ): # last entry if i [ 0 ] == len ( all_hrs ) - 1 : lons = self . df [ self . df [ 'Date_hr' ] > all_hrs [ i [ 0 ] - 1 ]] # first entry elif i [ 0 ] == 0 : lons = self . df [ self . df [ 'Date_hr' ] < all_hrs [ i [ 0 ] + 1 ]] else : lons = self . df [( self . df [ 'Date_hr' ] > all_hrs [ i [ 0 ] - 1 ]) & ( self . df [ 'Date_hr' ] < all_hrs [ i [ 0 ] + 1 ])] central_points . append (([ 0 , max ( lons [ 'Lon' ])], [ 0 , min ( lons [ 'Lon' ])])) # get central longitudes self . time_lines = [ midpoint ( i [ 0 ], i [ 1 ]) for i in central_points ] # Get bbox, buffered by grid spacing. # Check if bbox input is valid list. if self . bbox is not None : try : self . bbox = [ float ( val ) for val in self . bbox . split ()] except BaseException : raise Exception ( 'Cannot understand the --bounding_box argument. String input is incorrect or path does not exist.' ) self . plotbbox , self . grid_dim , self . gridpoints = self . _get_extent () # generate list of grid-polygons append_poly = [] for i in self . gridpoints : bbox = [ i [ 1 ] - ( self . spacing / 2 ), i [ 1 ] + ( self . spacing / 2 ), i [ 0 ] - ( self . spacing / 2 ), i [ 0 ] + ( self . spacing / 2 )] append_poly . append ( Polygon ( np . column_stack (( np . array ([ bbox [ 2 ], bbox [ 3 ], bbox [ 3 ], bbox [ 2 ], bbox [ 2 ]]), np . array ([ bbox [ 0 ], bbox [ 0 ], bbox [ 1 ], bbox [ 1 ], bbox [ 0 ]]))))) # Pass lons/lats to create polygon # Check for grid cell intersection with each station idtogrid_dict = {} self . unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' ]) . size () self . unique_points = [ self . unique_points . index . get_level_values ( 'ID' ) . tolist (), self . unique_points . index . get_level_values ( 'Lon' ) . tolist (), self . unique_points . index . get_level_values ( 'Lat' ) . tolist ()] # Initiate R-tree of gridded array domain self . polygon_dict = dict (( id ( pt ), i ) for i , pt in enumerate ( append_poly )) self . polygon_tree = STRtree ( append_poly ) for stat_ID in self . unique_points [ 0 ]: grd_index = self . _check_stationgrid_intersection ( stat_ID ) idtogrid_dict [ stat_ID ] = grd_index # map gridnode dictionary to dataframe self . df [ 'gridnode' ] = self . df [ 'ID' ] . map ( idtogrid_dict ) self . df = self . df [ self . df [ 'gridnode' ] . astype ( str ) != 'NaN' ] del self . unique_points , self . polygon_dict , self . polygon_tree , idtogrid_dict , append_poly # sort by grid and date self . df . sort_values ([ 'gridnode' , 'Date' ]) # If specified, pass station locations to superimpose on gridplots if self . stationsongrids : unique_points = self . df . groupby ([ 'Lon' , 'Lat' ]) . size () self . stationsongrids = [ unique_points . index . get_level_values ( 'Lon' ) . tolist (), unique_points . index . get_level_values ( 'Lat' ) . tolist ()] # If specified, setup gridded array(s) if self . grid_heatmap : self . grid_heatmap = np . array ([ np . nan if i [ 0 ] not in self . df [ 'gridnode' ] . values [:] else int ( len ( np . unique ( self . df [ 'ID' ][ self . df [ 'gridnode' ] == i [ 0 ]]))) for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_heatmap' + '.tif' ) save_gridfile ( self . grid_heatmap , 'grid_heatmap' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %1i ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'int16' , noData = 0 ) if self . grid_delay_mean : # Take mean of station-wise means per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ self . col_name ] . mean () unique_points = unique_points . groupby ([ 'gridnode' ])[ self . col_name ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_delay_mean = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_delay_mean' + '.tif' ) save_gridfile ( self . grid_delay_mean , 'grid_delay_mean' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.2f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) if self . grid_delay_median : # Take mean of station-wise medians per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ self . col_name ] . median () unique_points = unique_points . groupby ([ 'gridnode' ])[ self . col_name ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_delay_median = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_delay_median' + '.tif' ) save_gridfile ( self . grid_delay_median , 'grid_delay_median' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.2f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) if self . grid_delay_stdev : # Take mean of station-wise stdev per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ self . col_name ] . std () unique_points = unique_points . groupby ([ 'gridnode' ])[ self . col_name ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_delay_stdev = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_delay_stdev' + '.tif' ) save_gridfile ( self . grid_delay_stdev , 'grid_delay_stdev' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.2f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) if self . grid_delay_absolute_mean : # Take mean of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ self . col_name ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_delay_absolute_mean = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_delay_absolute_mean' + '.tif' ) save_gridfile ( self . grid_delay_absolute_mean , 'grid_delay_absolute_mean' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.2f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) if self . grid_delay_absolute_median : # Take median of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ self . col_name ] . median () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_delay_absolute_median = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_delay_absolute_median' + '.tif' ) save_gridfile ( self . grid_delay_absolute_median , 'grid_delay_absolute_median' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.2f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) if self . grid_delay_absolute_stdev : # Take stdev of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ self . col_name ] . std () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_delay_absolute_stdev = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_delay_absolute_stdev' + '.tif' ) save_gridfile ( self . grid_delay_absolute_stdev , 'grid_delay_absolute_stdev' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.2f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # If specified, compute phase/amplitude fits if self . station_seasonal_phase or self . grid_seasonal_phase or self . grid_seasonal_absolute_phase : # Sort by coordinates unique_points = self . df . sort_values ([ 'ID' , 'Date' ]) unique_points [ 'Date' ] = [ i . timestamp () for i in unique_points [ 'Date' ]] # Setup variables self . ampfit = [] self . phsfit = [] self . periodfit = [] self . ampfit_c = [] self . phsfit_c = [] self . periodfit_c = [] self . seasonalfit_rmse = [] args = [] for i in sorted ( list ( set ( unique_points [ 'ID' ]))): # pass all values corresponding to station (ID, data = y, time = x) args . append (( i , unique_points [ unique_points [ 'ID' ] == i ][ 'Date' ] . to_list (), unique_points [ unique_points [ 'ID' ] == i ][ self . col_name ] . to_list (), self . min_span [ 0 ], self . min_span [ 1 ], self . period_limit )) # Parallelize iteration through all grid-cells and time slices with multiprocessing . Pool ( self . numCPUs ) as multipool : for i , j , k , l , m , n , o in multipool . starmap ( self . _amplitude_and_phase , args ): self . ampfit . extend ( i ) self . phsfit . extend ( j ) self . periodfit . extend ( k ) self . ampfit_c . extend ( l ) self . phsfit_c . extend ( m ) self . periodfit_c . extend ( n ) self . seasonalfit_rmse . extend ( o ) # map phase/amplitude fits dictionary to dataframe self . phsfit = { k : v for d in self . phsfit for k , v in d . items ()} self . ampfit = { k : v for d in self . ampfit for k , v in d . items ()} self . periodfit = { k : v for d in self . periodfit for k , v in d . items ()} self . df [ 'phsfit' ] = self . df [ 'ID' ] . map ( self . phsfit ) # check if there are any valid data values if self . df [ 'phsfit' ] . isnull () . values . all ( axis = 0 ): raise Exception ( \"No valid data values, adjust --min_span inputs for time span in years {} and/or fractional obs. {} \" . format ( self . min_span [ 0 ], self . min_span [ 1 ])) self . df [ 'ampfit' ] = self . df [ 'ID' ] . map ( self . ampfit ) self . df [ 'periodfit' ] = self . df [ 'ID' ] . map ( self . periodfit ) self . phsfit_c = { k : v for d in self . phsfit_c for k , v in d . items ()} self . ampfit_c = { k : v for d in self . ampfit_c for k , v in d . items ()} self . periodfit_c = { k : v for d in self . periodfit_c for k , v in d . items ()} self . seasonalfit_rmse = { k : v for d in self . seasonalfit_rmse for k , v in d . items ()} self . df [ 'phsfit_c' ] = self . df [ 'ID' ] . map ( self . phsfit_c ) self . df [ 'ampfit_c' ] = self . df [ 'ID' ] . map ( self . ampfit_c ) self . df [ 'periodfit_c' ] = self . df [ 'ID' ] . map ( self . periodfit_c ) self . df [ 'seasonalfit_rmse' ] = self . df [ 'ID' ] . map ( self . seasonalfit_rmse ) # drop nan self . df . dropna ( how = 'any' , inplace = True ) # If grid plots specified if self . grid_seasonal_phase : # Pass mean phase of station-wise means per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ 'phsfit' ] . mean () unique_points = unique_points . groupby ([ 'gridnode' ])[ 'phsfit' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_phase = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_phase' + '.tif' ) save_gridfile ( self . grid_seasonal_phase , 'grid_seasonal_phase' , gridfile_name , self . plotbbox , self . spacing , 'days' , colorbarfmt = ' %.1i ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass mean amplitude of station-wise means per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ 'ampfit' ] . mean () unique_points = unique_points . groupby ([ 'gridnode' ])[ 'ampfit' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_amplitude = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_amplitude' + '.tif' ) save_gridfile ( self . grid_seasonal_amplitude , 'grid_seasonal_amplitude' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.3f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass mean period of station-wise means per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ 'periodfit' ] . mean () unique_points = unique_points . groupby ([ 'gridnode' ])[ 'periodfit' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_period = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_period' + '.tif' ) save_gridfile ( self . grid_seasonal_period , 'grid_seasonal_period' , gridfile_name , self . plotbbox , self . spacing , 'years' , colorbarfmt = ' %.2f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) ######################################################################################################################## # Pass mean phase stdev of station-wise means per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ 'phsfit_c' ] . mean () unique_points = unique_points . groupby ([ 'gridnode' ])[ 'phsfit_c' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_phase_stdev = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_phase_stdev' + '.tif' ) save_gridfile ( self . grid_seasonal_phase_stdev , 'grid_seasonal_phase_stdev' , gridfile_name , self . plotbbox , self . spacing , 'days' , colorbarfmt = ' %.1i ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass mean amplitude stdev of station-wise means per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ 'ampfit_c' ] . mean () unique_points = unique_points . groupby ([ 'gridnode' ])[ 'ampfit_c' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_amplitude_stdev = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_amplitude_stdev' + '.tif' ) save_gridfile ( self . grid_seasonal_amplitude_stdev , 'grid_seasonal_amplitude_stdev' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.3f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass mean period stdev of station-wise means per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ 'periodfit_c' ] . mean () unique_points = unique_points . groupby ([ 'gridnode' ])[ 'periodfit_c' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_period_stdev = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_period_stdev' + '.tif' ) save_gridfile ( self . grid_seasonal_period_stdev , 'grid_seasonal_period_stdev' , gridfile_name , self . plotbbox , self . spacing , 'years' , colorbarfmt = ' %.2e ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass mean seasonal fit RMSE of station-wise means per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ 'seasonalfit_rmse' ] . mean () unique_points = unique_points . groupby ([ 'gridnode' ])[ 'seasonalfit_rmse' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_fit_rmse = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_fit_rmse' + '.tif' ) save_gridfile ( self . grid_seasonal_fit_rmse , 'grid_seasonal_fit_rmse' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.3f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) ######################################################################################################################## if self . grid_seasonal_absolute_phase : # Pass absolute mean phase of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ 'phsfit' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_absolute_phase = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_absolute_phase' + '.tif' ) save_gridfile ( self . grid_seasonal_absolute_phase , 'grid_seasonal_absolute_phase' , gridfile_name , self . plotbbox , self . spacing , 'days' , colorbarfmt = ' %.1i ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass absolute mean amplitude of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ 'ampfit' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_absolute_amplitude = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_absolute_amplitude' + '.tif' ) save_gridfile ( self . grid_seasonal_absolute_amplitude , 'grid_seasonal_absolute_amplitude' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.3f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass absolute mean period of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ 'periodfit' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_absolute_period = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_absolute_period' + '.tif' ) save_gridfile ( self . grid_seasonal_absolute_period , 'grid_seasonal_absolute_period' , gridfile_name , self . plotbbox , self . spacing , 'years' , colorbarfmt = ' %.2f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) ######################################################################################################################## # Pass absolute mean phase stdev of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ 'phsfit_c' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_absolute_phase_stdev = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_absolute_phase_stdev' + '.tif' ) save_gridfile ( self . grid_seasonal_absolute_phase_stdev , 'grid_seasonal_absolute_phase_stdev' , gridfile_name , self . plotbbox , self . spacing , 'days' , colorbarfmt = ' %.1i ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass absolute mean amplitude stdev of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ 'ampfit_c' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_absolute_amplitude_stdev = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_absolute_amplitude_stdev' + '.tif' ) save_gridfile ( self . grid_seasonal_absolute_amplitude_stdev , 'grid_seasonal_absolute_amplitude_stdev' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.3f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass absolute mean period stdev of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ 'periodfit_c' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_absolute_period_stdev = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_absolute_period_stdev' + '.tif' ) save_gridfile ( self . grid_seasonal_absolute_period_stdev , 'grid_seasonal_absolute_period_stdev' , gridfile_name , self . plotbbox , self . spacing , 'years' , colorbarfmt = ' %.2e ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass absolute mean seasonal fit RMSE of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ 'seasonalfit_rmse' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_absolute_fit_rmse = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_absolute_fit_rmse' + '.tif' ) save_gridfile ( self . grid_seasonal_absolute_fit_rmse , 'grid_seasonal_absolute_fit_rmse' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.2e ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) def _amplitude_and_phase ( self , station , tt , yy , min_span = 2 , min_frac = 0.6 , period_limit = 0. ): ''' Fit sin to the input time sequence, and return fitting parameters: \"amp\", \"omega\", \"phase\", \"offset\", \"freq\", \"period\" and \"fitfunc\". Minimum time span in years (min_span), minimum fractional observations in span (min_frac), and period limit (period_limit) enforced for statistical analysis. Source: https://stackoverflow.com/questions/16716302/how-do-i-fit-a-sine-curve-to-my-data-with-pylab-and-numpy ''' ampfit = {} phsfit = {} periodfit = {} ampfit_c = {} phsfit_c = {} periodfit_c = {} seasonalfit_rmse = {} ampfit [ station ] = np . nan phsfit [ station ] = np . nan periodfit [ station ] = np . nan ampfit_c [ station ] = np . nan phsfit_c [ station ] = np . nan periodfit_c [ station ] = np . nan seasonalfit_rmse [ station ] = np . nan # Fit with custom fit function with fixed period, if specified if period_limit != 0 : # convert from years to radians/seconds w = ( 1 / period_limit ) * ( 1 / 31556952 ) * ( 2. * np . pi ) def custom_sine_function_base ( t , A , p , c ): return self . _sine_function_base ( t , A , w , p , c ) else : def custom_sine_function_base ( t , A , w , p , c ): return self . _sine_function_base ( t , A , w , p , c ) # If station TS does not span specified time period, pass NaNs time_span_yrs = ( max ( tt ) - min ( tt )) / 31556952 if time_span_yrs >= min_span and len ( list ( set ( tt ))) / ( time_span_yrs * 365.25 ) >= min_frac : tt = np . array ( tt ) yy = np . array ( yy ) ff = np . fft . fftfreq ( len ( tt ), ( tt [ 1 ] - tt [ 0 ])) # assume uniform spacing Fyy = abs ( np . fft . fft ( yy )) guess_freq = abs ( ff [ np . argmax ( Fyy [ 1 :]) + 1 ]) # excluding the zero period \"peak\", which is related to offset guess_amp = np . std ( yy ) * 2. ** 0.5 guess_offset = np . mean ( yy ) guess = np . array ([ guess_amp , 2. * np . pi * guess_freq , 0. , guess_offset ]) # Adjust frequency guess to reflect fixed period, if specified if period_limit != 0 : guess = np . array ([ guess_amp , 0. , guess_offset ]) # Catch warning where covariance cannot be estimated # I.e. OptimizeWarning: Covariance of the parameters could not be estimated with warnings . catch_warnings (): warnings . simplefilter ( \"error\" , OptimizeWarning ) try : optimize_warning = False try : # Note, may have to adjust max number of iterations (maxfev) higher to avoid crashes popt , pcov = optimize . curve_fit ( custom_sine_function_base , tt , yy , p0 = guess , maxfev = int ( 1e6 )) # If sparse input such that fittitng is not possible, pass NaNs except TypeError : self . ampfit . append ( np . nan ), self . phsfit . append ( np . nan ), self . periodfit . append ( np . nan ), \\ self . ampfit_c . append ( np . nan ), self . phsfit_c . append ( np . nan ), \\ self . periodfit_c . append ( np . nan ), self . seasonalfit_rmse . append ( np . nan ) return self . ampfit , self . phsfit , self . periodfit , self . ampfit_c , \\ self . phsfit_c , self . periodfit_c , self . seasonalfit_rmse except OptimizeWarning : optimize_warning = True warnings . simplefilter ( \"ignore\" , OptimizeWarning ) popt , pcov = optimize . curve_fit ( custom_sine_function_base , tt , yy , p0 = guess , maxfev = int ( 1e6 )) print ( 'OptimizeWarning: Covariance for station {} could not be estimated. Refer to debug figure here {} \\ ' . format ( station , os . path . join ( self . workdir , 'phaseamp_per_station' , 'station {} .png' . format ( station )))) pass # Adjust expected output to reflect fixed period, if specified if period_limit != 0 : A , p , c = popt else : A , w , p , c = popt # convert from radians/seconds to years f = ( w / ( 2. * np . pi )) * ( 31556952 ) f = 1 / f def fitfunc ( t ): return A * np . sin ( w * t + p ) + c # Outputs = \"amp\": A, \"angular frequency\": w, \"phase\": p, \"offset\": c, \"freq\": f, \"period\": 1./f, # \"fitfunc\": fitfunc, \"maxcov\": np.max(pcov), \"rawres\": (guess,popt,pcov) # Pass amplitude (specified units) and phase (days) and stdev ampfit [ station ] = abs ( A ) # Convert phase from rad to days, apply half wavelength shift if Amp is negative if A < 0 : p += 3.14159 phsfit [ station ] = ( 365.25 / 2 ) * np . sin ( p ) periodfit [ station ] = f # Catch warning where output is so small that it gets rounded to 0 # I.e. RuntimeWarning: invalid value encountered in double_scalars with np . errstate ( invalid = 'raise' ): try : # pass covariance for each parameter ampfit_c [ station ] = pcov [ 0 , 0 ] ** 0.5 periodfit_c [ station ] = pcov [ 1 , 1 ] ** 0.5 phsfit_c [ station ] = pcov [ 2 , 2 ] ** 0.5 # pass RMSE of fit seasonalfit_rmse [ station ] = yy - custom_sine_function_base ( tt , * popt ) seasonalfit_rmse [ station ] = ( scipy_sum ( seasonalfit_rmse [ station ] ** 2 ) / ( seasonalfit_rmse [ station ] . size - 2 )) ** 0.5 except FloatingPointError : pass if self . phaseamp_per_station or optimize_warning : # Debug plotting for each station # convert time (datetime seconds) to absolute years for plotting tt_plot = copy . deepcopy ( tt ) tt_plot -= min ( tt_plot ) tt_plot /= 31556952 plt . plot ( tt_plot , yy , \"ok\" , label = \"input\" ) plt . xlabel ( \"time (years)\" ) plt . ylabel ( \"data ( {} )\" . format ( self . unit )) num_testpoints = len ( tt ) * 10 if num_testpoints > 1000 : num_testpoints = 1000 tt2 = np . linspace ( min ( tt ), max ( tt ), num_testpoints ) # convert time to years for plotting tt2_plot = copy . deepcopy ( tt2 ) tt2_plot -= min ( tt2_plot ) tt2_plot /= 31556952 plt . plot ( tt2_plot , fitfunc ( tt2 ), \"r-\" , label = \"fit\" , linewidth = 2 ) plt . legend ( loc = \"best\" ) if not os . path . exists ( os . path . join ( self . workdir , 'phaseamp_per_station' )): os . mkdir ( os . path . join ( self . workdir , 'phaseamp_per_station' )) plt . savefig ( os . path . join ( self . workdir , 'phaseamp_per_station' , 'station {} .png' . format ( station )), format = 'png' , bbox_inches = 'tight' ) plt . close () optimize_warning = False self . ampfit . append ( ampfit ) self . phsfit . append ( phsfit ) self . periodfit . append ( periodfit ) self . ampfit_c . append ( ampfit_c ) self . phsfit_c . append ( phsfit_c ) self . periodfit_c . append ( periodfit_c ) self . seasonalfit_rmse . append ( seasonalfit_rmse ) return self . ampfit , self . phsfit , self . periodfit , self . ampfit_c , \\ self . phsfit_c , self . periodfit_c , self . seasonalfit_rmse def _sine_function_base ( self , t , A , w , p , c ): ''' Base function for modeling sinusoidal amplitude/phase fits. ''' return A * np . sin ( w * t + p ) + c def __call__ ( self , gridarr , plottype , workdir = './' , drawgridlines = False , colorbarfmt = ' %.2e ' , stationsongrids = None , resValue = 5 , plotFormat = 'pdf' , userTitle = None ): ''' Visualize a suite of statistics w.r.t. stations. Pass either a list of points or a gridded array as the first argument. Alternatively, you may superimpose your gridded array with a supplementary list of points by passing the latter through the stationsongrids argument. ''' from cartopy import crs as ccrs from cartopy import feature as cfeature from cartopy.mpl.ticker import LatitudeFormatter , LongitudeFormatter from matplotlib import ticker as mticker from mpl_toolkits.axes_grid1 import make_axes_locatable # If specified workdir doesn't exist, create it if not os . path . exists ( workdir ): os . mkdir ( workdir ) # Pass cbounds cbounds = self . cbounds # Initiate no-data array to mask data nodat_arr = [ 0 , np . nan , np . inf ] if self . unit in [ 'minute' , 'hour' , 'day' , 'year' ]: colorbarfmt = ' %.1i ' nodat_arr = [ np . nan , np . inf ] fig , axes = plt . subplots ( subplot_kw = { 'projection' : ccrs . PlateCarree ()}) # by default set background to white axes . add_feature ( cfeature . NaturalEarthFeature ( 'physical' , 'land' , '50m' , facecolor = 'white' ), zorder = 0 ) axes . set_extent ( self . plotbbox , ccrs . PlateCarree ()) # add coastlines axes . coastlines ( linewidth = 0.2 , color = \"gray\" , zorder = 4 ) cmap = copy . copy ( mpl . cm . get_cmap ( self . usr_colormap )) # cmap.set_bad('black', 0.) # extract all colors from the hot map cmaplist = [ cmap ( i ) for i in range ( cmap . N )] # create the new map cmap = mpl . colors . LinearSegmentedColormap . from_list ( 'Custom cmap' , cmaplist ) axes . set_xlabel ( 'Longitude' , weight = 'bold' , zorder = 2 ) axes . set_ylabel ( 'Latitude' , weight = 'bold' , zorder = 2 ) # set ticks axes . set_xticks ( np . linspace ( self . plotbbox [ 0 ], self . plotbbox [ 1 ], 5 ), crs = ccrs . PlateCarree ()) axes . set_yticks ( np . linspace ( self . plotbbox [ 2 ], self . plotbbox [ 3 ], 5 ), crs = ccrs . PlateCarree ()) lon_formatter = LongitudeFormatter ( number_format = '.0f' , degree_symbol = '' ) lat_formatter = LatitudeFormatter ( number_format = '.0f' , degree_symbol = '' ) axes . xaxis . set_major_formatter ( lon_formatter ) axes . yaxis . set_major_formatter ( lat_formatter ) # draw central longitude lines corresponding to respective datetimes if self . time_lines : tl = axes . grid ( axis = 'x' , linewidth = 1.5 , color = 'blue' , alpha = 0.5 , linestyle = '-' , zorder = 3 ) # If individual stations passed if isinstance ( gridarr , list ): # spatial distribution of stations if plottype == \"station_distribution\" : im = axes . scatter ( gridarr [ 0 ], gridarr [ 1 ], zorder = 1 , s = 0.5 , marker = '.' , color = 'b' , transform = ccrs . PlateCarree ()) # passing 3rd column as z-value if len ( gridarr ) > 2 : # set land/water background to light gray/blue respectively so station point data can be seen axes . add_feature ( cfeature . NaturalEarthFeature ( 'physical' , 'land' , '50m' , facecolor = '#A9A9A9' ), zorder = 0 ) axes . add_feature ( cfeature . NaturalEarthFeature ( 'physical' , 'ocean' , '50m' , facecolor = '#ADD8E6' ), zorder = 0 ) # set masked values as nans zvalues = gridarr [ 2 ] for i in nodat_arr : zvalues = np . ma . masked_where ( zvalues == i , zvalues ) zvalues = np . ma . filled ( zvalues , np . nan ) # define the bins and normalize if cbounds is None : # avoid \"ufunc 'isnan'\" error by casting array as float cbounds = [ np . nanpercentile ( zvalues . astype ( 'float' ), self . colorpercentile [ 0 ]), np . nanpercentile ( zvalues . astype ( 'float' ), self . colorpercentile [ 1 ])] # if upper/lower bounds identical, overwrite lower bound as 75% of upper bound to avoid plotting ValueError if cbounds [ 0 ] == cbounds [ 1 ]: cbounds [ 0 ] *= 0.75 cbounds . sort () # adjust precision for colorbar if necessary if ( abs ( np . nanmax ( zvalues ) - np . nanmin ( zvalues )) < 1 and ( np . nanmean ( zvalues )) < 1 ) \\ or abs ( np . nanmax ( zvalues ) - np . nanmin ( zvalues )) > 500 : colorbarfmt = ' %.2e ' colorbounds = np . linspace ( cbounds [ 0 ], cbounds [ 1 ], 256 ) colorbounds = np . unique ( colorbounds ) norm = mpl . colors . BoundaryNorm ( colorbounds , cmap . N ) colorbounds_ticks = np . linspace ( cbounds [ 0 ], cbounds [ 1 ], 10 ) # plot data and initiate colorbar im = axes . scatter ( gridarr [ 0 ], gridarr [ 1 ], c = zvalues , cmap = cmap , norm = norm , zorder = 1 , s = 0.5 , marker = '.' , transform = ccrs . PlateCarree ()) # initiate colorbar and control height of colorbar divider = make_axes_locatable ( axes ) cax = divider . append_axes ( \"right\" , size = \"5%\" , pad = 0.05 , axes_class = plt . Axes ) cbar_ax = fig . colorbar ( im , spacing = 'proportional' , ticks = colorbounds_ticks , boundaries = colorbounds , format = colorbarfmt , pad = 0.1 , cax = cax ) # If gridded area passed else : # set masked values as nans for i in nodat_arr : gridarr = np . ma . masked_where ( gridarr == i , gridarr ) gridarr = np . ma . filled ( gridarr , np . nan ) # set land/water background to light gray/blue respectively so grid cells can be seen axes . add_feature ( cfeature . NaturalEarthFeature ( 'physical' , 'land' , '50m' , facecolor = '#A9A9A9' ), zorder = 0 ) axes . add_feature ( cfeature . NaturalEarthFeature ( 'physical' , 'ocean' , '50m' , facecolor = '#ADD8E6' ), zorder = 0 ) # define the bins and normalize if cbounds is None : cbounds = [ np . nanpercentile ( gridarr , self . colorpercentile [ 0 ]), np . nanpercentile ( gridarr , self . colorpercentile [ 1 ])] # if upper/lower bounds identical, overwrite lower bound as 75% of upper bound to avoid plotting ValueError if cbounds [ 0 ] == cbounds [ 1 ]: cbounds [ 0 ] *= 0.75 cbounds . sort () # plot data and initiate colorbar if ( abs ( np . nanmax ( gridarr ) - np . nanmin ( gridarr )) < 1 and abs ( np . nanmean ( gridarr )) < 1 ) \\ or abs ( np . nanmax ( gridarr ) - np . nanmin ( gridarr )) > 500 : colorbarfmt = ' %.2e ' colorbounds = np . linspace ( cbounds [ 0 ], cbounds [ 1 ], 256 ) colorbounds = np . unique ( colorbounds ) norm = mpl . colors . BoundaryNorm ( colorbounds , cmap . N ) colorbounds_ticks = np . linspace ( cbounds [ 0 ], cbounds [ 1 ], 10 ) # plot data im = axes . imshow ( gridarr , cmap = cmap , norm = norm , extent = self . plotbbox , zorder = 1 , origin = 'upper' , transform = ccrs . PlateCarree ()) # initiate colorbar and control height of colorbar divider = make_axes_locatable ( axes ) cax = divider . append_axes ( \"right\" , size = \"5%\" , pad = 0.05 , axes_class = plt . Axes ) cbar_ax = fig . colorbar ( im , spacing = 'proportional' , ticks = colorbounds_ticks , boundaries = colorbounds , format = colorbarfmt , pad = 0.1 , cax = cax ) # superimpose your gridded array with a supplementary list of point, if specified if self . stationsongrids : axes . scatter ( self . stationsongrids [ 0 ], self . stationsongrids [ 1 ], zorder = 2 , s = 0.5 , marker = '.' , color = 'b' , transform = ccrs . PlateCarree ()) # draw gridlines, if specified if drawgridlines : gl = axes . gridlines ( crs = ccrs . PlateCarree ( ), linewidth = 0.5 , color = 'black' , alpha = 0.5 , linestyle = '-' , zorder = 3 ) gl . xlocator = mticker . FixedLocator ( np . arange ( self . plotbbox [ 0 ], self . plotbbox [ 1 ] + self . spacing , self . spacing ) . tolist ()) gl . ylocator = mticker . FixedLocator ( np . arange ( self . plotbbox [ 2 ], self . plotbbox [ 3 ] + self . spacing , self . spacing ) . tolist ()) # Add labels to colorbar, if necessary if 'cbar_ax' in locals (): # experimental variogram fit sill heatmap if plottype == \"grid_variance\" : cbar_ax . set_label ( \" \" . join ( plottype . replace ( 'grid_' , '' ) . split ( '_' )) . title () + ' ( {} \\u00b2 )' . format ( self . unit ), rotation =- 90 , labelpad = 10 ) # specify appropriate units for mean/median/std/amplitude/experimental variogram fit heatmap elif plottype == \"grid_delay_mean\" or plottype == \"grid_delay_median\" or plottype == \"grid_delay_stdev\" or \\ plottype == \"grid_seasonal_amplitude\" or plottype == \"grid_range\" or plottype == \"station_delay_mean\" or \\ plottype == \"station_delay_median\" or plottype == \"station_delay_stdev\" or \\ plottype == \"station_seasonal_amplitude\" or plottype == \"grid_delay_absolute_mean\" or \\ plottype == \"grid_delay_absolute_median\" or plottype == \"grid_delay_absolute_stdev\" or \\ plottype == \"grid_seasonal_absolute_amplitude\" or plottype == \"grid_seasonal_amplitude_stdev\" or \\ plottype == \"grid_seasonal_absolute_amplitude_stdev\" or plottype == \"grid_seasonal_fit_rmse\" or \\ plottype == \"grid_seasonal_absolute_fit_rmse\" or plottype == \"grid_variogram_rmse\" : # update label if sigZTD if 'sig' in self . col_name : cbar_ax . set_label ( \"sig ZTD \" + \" \" . join ( plottype . replace ( 'grid_' , '' ) . replace ( 'delay_' , '' ) . split ( '_' )) . title () + ' ( {} )' . format ( self . unit ), rotation =- 90 , labelpad = 10 ) else : cbar_ax . set_label ( \" \" . join ( plottype . replace ( 'grid_' , '' ) . split ( '_' )) . title () + ' ( {} )' . format ( self . unit ), rotation =- 90 , labelpad = 10 ) # specify appropriate units for phase heatmap (days) elif plottype == \"station_seasonal_phase\" or plottype == \"grid_seasonal_phase\" or plottype == \"grid_seasonal_absolute_phase\" or \\ plottype == \"grid_seasonal_absolute_phase_stdev\" or plottype == \"grid_seasonal_phase_stdev\" : cbar_ax . set_label ( \" \" . join ( plottype . replace ( 'grid_' , '' ) . split ( '_' )) . title () + ' ( {} )' . format ( 'days' ), rotation =- 90 , labelpad = 10 ) # specify appropriate units for period heatmap (years) elif plottype == \"station_delay_period\" or plottype == \"grid_seasonal_period\" or plottype == \"grid_seasonal_absolute_period\" or \\ plottype == \"grid_seasonal_absolute_period_stdev\" or plottype == \"grid_seasonal_period_stdev\" : cbar_ax . set_label ( \" \" . join ( plottype . replace ( 'grid_' , '' ) . split ( '_' )) . title () + ' ( {} )' . format ( 'years' ), rotation =- 90 , labelpad = 10 ) # gridmap of station density has no units else : cbar_ax . set_label ( \" \" . join ( plottype . replace ( 'grid_' , '' ) . split ( '_' )) . title (), rotation =- 90 , labelpad = 10 ) # Add title to plots, if specified if userTitle : axes . set_title ( userTitle , zorder = 2 ) # save/close figure # cbar_ax.ax.locator_params(nbins=10) # for label in cbar_ax.ax.xaxis.get_ticklabels()[::25]: # label.set_visible(False) plt . savefig ( os . path . join ( workdir , self . col_name + '_' + plottype + '.' + plotFormat ), format = plotFormat , bbox_inches = 'tight' ) plt . close () return","title":"RaiderStats"},{"location":"reference/#RAiDER.cli.statsPlot.RaiderStats.__call__","text":"Visualize a suite of statistics w.r.t. stations. Pass either a list of points or a gridded array as the first argument. Alternatively, you may superimpose your gridded array with a supplementary list of points by passing the latter through the stationsongrids argument. Source code in RAiDER/cli/statsPlot.py 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 def __call__ ( self , gridarr , plottype , workdir = './' , drawgridlines = False , colorbarfmt = ' %.2e ' , stationsongrids = None , resValue = 5 , plotFormat = 'pdf' , userTitle = None ): ''' Visualize a suite of statistics w.r.t. stations. Pass either a list of points or a gridded array as the first argument. Alternatively, you may superimpose your gridded array with a supplementary list of points by passing the latter through the stationsongrids argument. ''' from cartopy import crs as ccrs from cartopy import feature as cfeature from cartopy.mpl.ticker import LatitudeFormatter , LongitudeFormatter from matplotlib import ticker as mticker from mpl_toolkits.axes_grid1 import make_axes_locatable # If specified workdir doesn't exist, create it if not os . path . exists ( workdir ): os . mkdir ( workdir ) # Pass cbounds cbounds = self . cbounds # Initiate no-data array to mask data nodat_arr = [ 0 , np . nan , np . inf ] if self . unit in [ 'minute' , 'hour' , 'day' , 'year' ]: colorbarfmt = ' %.1i ' nodat_arr = [ np . nan , np . inf ] fig , axes = plt . subplots ( subplot_kw = { 'projection' : ccrs . PlateCarree ()}) # by default set background to white axes . add_feature ( cfeature . NaturalEarthFeature ( 'physical' , 'land' , '50m' , facecolor = 'white' ), zorder = 0 ) axes . set_extent ( self . plotbbox , ccrs . PlateCarree ()) # add coastlines axes . coastlines ( linewidth = 0.2 , color = \"gray\" , zorder = 4 ) cmap = copy . copy ( mpl . cm . get_cmap ( self . usr_colormap )) # cmap.set_bad('black', 0.) # extract all colors from the hot map cmaplist = [ cmap ( i ) for i in range ( cmap . N )] # create the new map cmap = mpl . colors . LinearSegmentedColormap . from_list ( 'Custom cmap' , cmaplist ) axes . set_xlabel ( 'Longitude' , weight = 'bold' , zorder = 2 ) axes . set_ylabel ( 'Latitude' , weight = 'bold' , zorder = 2 ) # set ticks axes . set_xticks ( np . linspace ( self . plotbbox [ 0 ], self . plotbbox [ 1 ], 5 ), crs = ccrs . PlateCarree ()) axes . set_yticks ( np . linspace ( self . plotbbox [ 2 ], self . plotbbox [ 3 ], 5 ), crs = ccrs . PlateCarree ()) lon_formatter = LongitudeFormatter ( number_format = '.0f' , degree_symbol = '' ) lat_formatter = LatitudeFormatter ( number_format = '.0f' , degree_symbol = '' ) axes . xaxis . set_major_formatter ( lon_formatter ) axes . yaxis . set_major_formatter ( lat_formatter ) # draw central longitude lines corresponding to respective datetimes if self . time_lines : tl = axes . grid ( axis = 'x' , linewidth = 1.5 , color = 'blue' , alpha = 0.5 , linestyle = '-' , zorder = 3 ) # If individual stations passed if isinstance ( gridarr , list ): # spatial distribution of stations if plottype == \"station_distribution\" : im = axes . scatter ( gridarr [ 0 ], gridarr [ 1 ], zorder = 1 , s = 0.5 , marker = '.' , color = 'b' , transform = ccrs . PlateCarree ()) # passing 3rd column as z-value if len ( gridarr ) > 2 : # set land/water background to light gray/blue respectively so station point data can be seen axes . add_feature ( cfeature . NaturalEarthFeature ( 'physical' , 'land' , '50m' , facecolor = '#A9A9A9' ), zorder = 0 ) axes . add_feature ( cfeature . NaturalEarthFeature ( 'physical' , 'ocean' , '50m' , facecolor = '#ADD8E6' ), zorder = 0 ) # set masked values as nans zvalues = gridarr [ 2 ] for i in nodat_arr : zvalues = np . ma . masked_where ( zvalues == i , zvalues ) zvalues = np . ma . filled ( zvalues , np . nan ) # define the bins and normalize if cbounds is None : # avoid \"ufunc 'isnan'\" error by casting array as float cbounds = [ np . nanpercentile ( zvalues . astype ( 'float' ), self . colorpercentile [ 0 ]), np . nanpercentile ( zvalues . astype ( 'float' ), self . colorpercentile [ 1 ])] # if upper/lower bounds identical, overwrite lower bound as 75% of upper bound to avoid plotting ValueError if cbounds [ 0 ] == cbounds [ 1 ]: cbounds [ 0 ] *= 0.75 cbounds . sort () # adjust precision for colorbar if necessary if ( abs ( np . nanmax ( zvalues ) - np . nanmin ( zvalues )) < 1 and ( np . nanmean ( zvalues )) < 1 ) \\ or abs ( np . nanmax ( zvalues ) - np . nanmin ( zvalues )) > 500 : colorbarfmt = ' %.2e ' colorbounds = np . linspace ( cbounds [ 0 ], cbounds [ 1 ], 256 ) colorbounds = np . unique ( colorbounds ) norm = mpl . colors . BoundaryNorm ( colorbounds , cmap . N ) colorbounds_ticks = np . linspace ( cbounds [ 0 ], cbounds [ 1 ], 10 ) # plot data and initiate colorbar im = axes . scatter ( gridarr [ 0 ], gridarr [ 1 ], c = zvalues , cmap = cmap , norm = norm , zorder = 1 , s = 0.5 , marker = '.' , transform = ccrs . PlateCarree ()) # initiate colorbar and control height of colorbar divider = make_axes_locatable ( axes ) cax = divider . append_axes ( \"right\" , size = \"5%\" , pad = 0.05 , axes_class = plt . Axes ) cbar_ax = fig . colorbar ( im , spacing = 'proportional' , ticks = colorbounds_ticks , boundaries = colorbounds , format = colorbarfmt , pad = 0.1 , cax = cax ) # If gridded area passed else : # set masked values as nans for i in nodat_arr : gridarr = np . ma . masked_where ( gridarr == i , gridarr ) gridarr = np . ma . filled ( gridarr , np . nan ) # set land/water background to light gray/blue respectively so grid cells can be seen axes . add_feature ( cfeature . NaturalEarthFeature ( 'physical' , 'land' , '50m' , facecolor = '#A9A9A9' ), zorder = 0 ) axes . add_feature ( cfeature . NaturalEarthFeature ( 'physical' , 'ocean' , '50m' , facecolor = '#ADD8E6' ), zorder = 0 ) # define the bins and normalize if cbounds is None : cbounds = [ np . nanpercentile ( gridarr , self . colorpercentile [ 0 ]), np . nanpercentile ( gridarr , self . colorpercentile [ 1 ])] # if upper/lower bounds identical, overwrite lower bound as 75% of upper bound to avoid plotting ValueError if cbounds [ 0 ] == cbounds [ 1 ]: cbounds [ 0 ] *= 0.75 cbounds . sort () # plot data and initiate colorbar if ( abs ( np . nanmax ( gridarr ) - np . nanmin ( gridarr )) < 1 and abs ( np . nanmean ( gridarr )) < 1 ) \\ or abs ( np . nanmax ( gridarr ) - np . nanmin ( gridarr )) > 500 : colorbarfmt = ' %.2e ' colorbounds = np . linspace ( cbounds [ 0 ], cbounds [ 1 ], 256 ) colorbounds = np . unique ( colorbounds ) norm = mpl . colors . BoundaryNorm ( colorbounds , cmap . N ) colorbounds_ticks = np . linspace ( cbounds [ 0 ], cbounds [ 1 ], 10 ) # plot data im = axes . imshow ( gridarr , cmap = cmap , norm = norm , extent = self . plotbbox , zorder = 1 , origin = 'upper' , transform = ccrs . PlateCarree ()) # initiate colorbar and control height of colorbar divider = make_axes_locatable ( axes ) cax = divider . append_axes ( \"right\" , size = \"5%\" , pad = 0.05 , axes_class = plt . Axes ) cbar_ax = fig . colorbar ( im , spacing = 'proportional' , ticks = colorbounds_ticks , boundaries = colorbounds , format = colorbarfmt , pad = 0.1 , cax = cax ) # superimpose your gridded array with a supplementary list of point, if specified if self . stationsongrids : axes . scatter ( self . stationsongrids [ 0 ], self . stationsongrids [ 1 ], zorder = 2 , s = 0.5 , marker = '.' , color = 'b' , transform = ccrs . PlateCarree ()) # draw gridlines, if specified if drawgridlines : gl = axes . gridlines ( crs = ccrs . PlateCarree ( ), linewidth = 0.5 , color = 'black' , alpha = 0.5 , linestyle = '-' , zorder = 3 ) gl . xlocator = mticker . FixedLocator ( np . arange ( self . plotbbox [ 0 ], self . plotbbox [ 1 ] + self . spacing , self . spacing ) . tolist ()) gl . ylocator = mticker . FixedLocator ( np . arange ( self . plotbbox [ 2 ], self . plotbbox [ 3 ] + self . spacing , self . spacing ) . tolist ()) # Add labels to colorbar, if necessary if 'cbar_ax' in locals (): # experimental variogram fit sill heatmap if plottype == \"grid_variance\" : cbar_ax . set_label ( \" \" . join ( plottype . replace ( 'grid_' , '' ) . split ( '_' )) . title () + ' ( {} \\u00b2 )' . format ( self . unit ), rotation =- 90 , labelpad = 10 ) # specify appropriate units for mean/median/std/amplitude/experimental variogram fit heatmap elif plottype == \"grid_delay_mean\" or plottype == \"grid_delay_median\" or plottype == \"grid_delay_stdev\" or \\ plottype == \"grid_seasonal_amplitude\" or plottype == \"grid_range\" or plottype == \"station_delay_mean\" or \\ plottype == \"station_delay_median\" or plottype == \"station_delay_stdev\" or \\ plottype == \"station_seasonal_amplitude\" or plottype == \"grid_delay_absolute_mean\" or \\ plottype == \"grid_delay_absolute_median\" or plottype == \"grid_delay_absolute_stdev\" or \\ plottype == \"grid_seasonal_absolute_amplitude\" or plottype == \"grid_seasonal_amplitude_stdev\" or \\ plottype == \"grid_seasonal_absolute_amplitude_stdev\" or plottype == \"grid_seasonal_fit_rmse\" or \\ plottype == \"grid_seasonal_absolute_fit_rmse\" or plottype == \"grid_variogram_rmse\" : # update label if sigZTD if 'sig' in self . col_name : cbar_ax . set_label ( \"sig ZTD \" + \" \" . join ( plottype . replace ( 'grid_' , '' ) . replace ( 'delay_' , '' ) . split ( '_' )) . title () + ' ( {} )' . format ( self . unit ), rotation =- 90 , labelpad = 10 ) else : cbar_ax . set_label ( \" \" . join ( plottype . replace ( 'grid_' , '' ) . split ( '_' )) . title () + ' ( {} )' . format ( self . unit ), rotation =- 90 , labelpad = 10 ) # specify appropriate units for phase heatmap (days) elif plottype == \"station_seasonal_phase\" or plottype == \"grid_seasonal_phase\" or plottype == \"grid_seasonal_absolute_phase\" or \\ plottype == \"grid_seasonal_absolute_phase_stdev\" or plottype == \"grid_seasonal_phase_stdev\" : cbar_ax . set_label ( \" \" . join ( plottype . replace ( 'grid_' , '' ) . split ( '_' )) . title () + ' ( {} )' . format ( 'days' ), rotation =- 90 , labelpad = 10 ) # specify appropriate units for period heatmap (years) elif plottype == \"station_delay_period\" or plottype == \"grid_seasonal_period\" or plottype == \"grid_seasonal_absolute_period\" or \\ plottype == \"grid_seasonal_absolute_period_stdev\" or plottype == \"grid_seasonal_period_stdev\" : cbar_ax . set_label ( \" \" . join ( plottype . replace ( 'grid_' , '' ) . split ( '_' )) . title () + ' ( {} )' . format ( 'years' ), rotation =- 90 , labelpad = 10 ) # gridmap of station density has no units else : cbar_ax . set_label ( \" \" . join ( plottype . replace ( 'grid_' , '' ) . split ( '_' )) . title (), rotation =- 90 , labelpad = 10 ) # Add title to plots, if specified if userTitle : axes . set_title ( userTitle , zorder = 2 ) # save/close figure # cbar_ax.ax.locator_params(nbins=10) # for label in cbar_ax.ax.xaxis.get_ticklabels()[::25]: # label.set_visible(False) plt . savefig ( os . path . join ( workdir , self . col_name + '_' + plottype + '.' + plotFormat ), format = plotFormat , bbox_inches = 'tight' ) plt . close () return","title":"__call__()"},{"location":"reference/#RAiDER.cli.statsPlot.RaiderStats.create_DF","text":"Create dataframe. Source code in RAiDER/cli/statsPlot.py 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 def create_DF ( self ): ''' Create dataframe. ''' # Open file self . df = self . _reader () # Filter dataframe # drop all nans self . df . dropna ( how = 'any' , inplace = True ) self . df . reset_index ( drop = True , inplace = True ) # convert to datetime object # time-interval filter if self . timeinterval : self . timeinterval = [ dt . datetime . strptime ( val , '%Y-%m- %d ' ) for val in self . timeinterval . split ()] self . df = self . df [( self . df [ 'Date' ] >= self . timeinterval [ 0 ]) & ( self . df [ 'Date' ] <= self . timeinterval [ - 1 ])] # seasonal filter if self . seasonalinterval : self . seasonalinterval = self . seasonalinterval . split () # get day of year self . seasonalinterval = [ dt . datetime . strptime ( '2001-' + self . seasonalinterval [ 0 ], '%Y-%m- %d ' ) . timetuple ( ) . tm_yday , dt . datetime . strptime ( '2001-' + self . seasonalinterval [ - 1 ], '%Y-%m- %d ' ) . timetuple () . tm_yday ] # track input order and wrap around year if necessary # e.g. month/day: 03/01 to 06/01 if self . seasonalinterval [ 0 ] < self . seasonalinterval [ 1 ]: # non leap-year filtered_self = self . df [( not self . df [ 'Date' ] . dt . is_leap_year ) & ( self . df [ 'Date' ] . dt . dayofyear >= self . seasonalinterval [ 0 ]) & ( self . df [ 'Date' ] . dt . dayofyear <= self . seasonalinterval [ - 1 ])] # leap-year self . seasonalinterval = [ i + 1 if i > 59 else i for i in self . seasonalinterval ] self . df = filtered_self . append ( self . df [( self . df [ 'Date' ] . dt . is_leap_year ) & ( self . df [ 'Date' ] . dt . dayofyear >= self . seasonalinterval [ 0 ]) & ( self . df [ 'Date' ] . dt . dayofyear <= self . seasonalinterval [ - 1 ])], ignore_index = True ) del filtered_self # e.g. month/day: 12/01 to 03/01 if self . seasonalinterval [ 0 ] > self . seasonalinterval [ 1 ]: # non leap-year filtered_self = self . df [( not self . df [ 'Date' ] . dt . is_leap_year ) & ( self . df [ 'Date' ] . dt . dayofyear >= self . seasonalinterval [ - 1 ]) & ( self . df [ 'Date' ] . dt . dayofyear <= self . seasonalinterval [ 0 ])] # leap-year self . seasonalinterval = [ i + 1 if i > 59 else i for i in self . seasonalinterval ] self . df = filtered_self . append ( self . df [( self . df [ 'Date' ] . dt . is_leap_year ) & ( self . df [ 'Date' ] . dt . dayofyear >= self . seasonalinterval [ - 1 ]) & ( self . df [ 'Date' ] . dt . dayofyear <= self . seasonalinterval [ 0 ])], ignore_index = True ) del filtered_self # estimate central longitude lines if '--time_lines' specified if self . time_lines and 'Datetime' in self . df . keys (): self . df [ 'Date_hr' ] = self . df [ 'Datetime' ] . dt . hour . astype ( float ) . astype ( \"Int32\" ) # get list of unique times all_hrs = sorted ( set ( self . df [ 'Date_hr' ])) # get central longitude bands associated with each time central_points = [] # if single time, avoid loop if len ( all_hrs ) == 1 : central_points . append (([ 0 , max ( self . df [ 'Lon' ])], [ 0 , min ( self . df [ 'Lon' ])])) else : for i in enumerate ( all_hrs ): # last entry if i [ 0 ] == len ( all_hrs ) - 1 : lons = self . df [ self . df [ 'Date_hr' ] > all_hrs [ i [ 0 ] - 1 ]] # first entry elif i [ 0 ] == 0 : lons = self . df [ self . df [ 'Date_hr' ] < all_hrs [ i [ 0 ] + 1 ]] else : lons = self . df [( self . df [ 'Date_hr' ] > all_hrs [ i [ 0 ] - 1 ]) & ( self . df [ 'Date_hr' ] < all_hrs [ i [ 0 ] + 1 ])] central_points . append (([ 0 , max ( lons [ 'Lon' ])], [ 0 , min ( lons [ 'Lon' ])])) # get central longitudes self . time_lines = [ midpoint ( i [ 0 ], i [ 1 ]) for i in central_points ] # Get bbox, buffered by grid spacing. # Check if bbox input is valid list. if self . bbox is not None : try : self . bbox = [ float ( val ) for val in self . bbox . split ()] except BaseException : raise Exception ( 'Cannot understand the --bounding_box argument. String input is incorrect or path does not exist.' ) self . plotbbox , self . grid_dim , self . gridpoints = self . _get_extent () # generate list of grid-polygons append_poly = [] for i in self . gridpoints : bbox = [ i [ 1 ] - ( self . spacing / 2 ), i [ 1 ] + ( self . spacing / 2 ), i [ 0 ] - ( self . spacing / 2 ), i [ 0 ] + ( self . spacing / 2 )] append_poly . append ( Polygon ( np . column_stack (( np . array ([ bbox [ 2 ], bbox [ 3 ], bbox [ 3 ], bbox [ 2 ], bbox [ 2 ]]), np . array ([ bbox [ 0 ], bbox [ 0 ], bbox [ 1 ], bbox [ 1 ], bbox [ 0 ]]))))) # Pass lons/lats to create polygon # Check for grid cell intersection with each station idtogrid_dict = {} self . unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' ]) . size () self . unique_points = [ self . unique_points . index . get_level_values ( 'ID' ) . tolist (), self . unique_points . index . get_level_values ( 'Lon' ) . tolist (), self . unique_points . index . get_level_values ( 'Lat' ) . tolist ()] # Initiate R-tree of gridded array domain self . polygon_dict = dict (( id ( pt ), i ) for i , pt in enumerate ( append_poly )) self . polygon_tree = STRtree ( append_poly ) for stat_ID in self . unique_points [ 0 ]: grd_index = self . _check_stationgrid_intersection ( stat_ID ) idtogrid_dict [ stat_ID ] = grd_index # map gridnode dictionary to dataframe self . df [ 'gridnode' ] = self . df [ 'ID' ] . map ( idtogrid_dict ) self . df = self . df [ self . df [ 'gridnode' ] . astype ( str ) != 'NaN' ] del self . unique_points , self . polygon_dict , self . polygon_tree , idtogrid_dict , append_poly # sort by grid and date self . df . sort_values ([ 'gridnode' , 'Date' ]) # If specified, pass station locations to superimpose on gridplots if self . stationsongrids : unique_points = self . df . groupby ([ 'Lon' , 'Lat' ]) . size () self . stationsongrids = [ unique_points . index . get_level_values ( 'Lon' ) . tolist (), unique_points . index . get_level_values ( 'Lat' ) . tolist ()] # If specified, setup gridded array(s) if self . grid_heatmap : self . grid_heatmap = np . array ([ np . nan if i [ 0 ] not in self . df [ 'gridnode' ] . values [:] else int ( len ( np . unique ( self . df [ 'ID' ][ self . df [ 'gridnode' ] == i [ 0 ]]))) for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_heatmap' + '.tif' ) save_gridfile ( self . grid_heatmap , 'grid_heatmap' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %1i ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'int16' , noData = 0 ) if self . grid_delay_mean : # Take mean of station-wise means per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ self . col_name ] . mean () unique_points = unique_points . groupby ([ 'gridnode' ])[ self . col_name ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_delay_mean = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_delay_mean' + '.tif' ) save_gridfile ( self . grid_delay_mean , 'grid_delay_mean' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.2f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) if self . grid_delay_median : # Take mean of station-wise medians per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ self . col_name ] . median () unique_points = unique_points . groupby ([ 'gridnode' ])[ self . col_name ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_delay_median = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_delay_median' + '.tif' ) save_gridfile ( self . grid_delay_median , 'grid_delay_median' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.2f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) if self . grid_delay_stdev : # Take mean of station-wise stdev per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ self . col_name ] . std () unique_points = unique_points . groupby ([ 'gridnode' ])[ self . col_name ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_delay_stdev = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_delay_stdev' + '.tif' ) save_gridfile ( self . grid_delay_stdev , 'grid_delay_stdev' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.2f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) if self . grid_delay_absolute_mean : # Take mean of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ self . col_name ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_delay_absolute_mean = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_delay_absolute_mean' + '.tif' ) save_gridfile ( self . grid_delay_absolute_mean , 'grid_delay_absolute_mean' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.2f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) if self . grid_delay_absolute_median : # Take median of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ self . col_name ] . median () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_delay_absolute_median = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_delay_absolute_median' + '.tif' ) save_gridfile ( self . grid_delay_absolute_median , 'grid_delay_absolute_median' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.2f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) if self . grid_delay_absolute_stdev : # Take stdev of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ self . col_name ] . std () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_delay_absolute_stdev = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_delay_absolute_stdev' + '.tif' ) save_gridfile ( self . grid_delay_absolute_stdev , 'grid_delay_absolute_stdev' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.2f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # If specified, compute phase/amplitude fits if self . station_seasonal_phase or self . grid_seasonal_phase or self . grid_seasonal_absolute_phase : # Sort by coordinates unique_points = self . df . sort_values ([ 'ID' , 'Date' ]) unique_points [ 'Date' ] = [ i . timestamp () for i in unique_points [ 'Date' ]] # Setup variables self . ampfit = [] self . phsfit = [] self . periodfit = [] self . ampfit_c = [] self . phsfit_c = [] self . periodfit_c = [] self . seasonalfit_rmse = [] args = [] for i in sorted ( list ( set ( unique_points [ 'ID' ]))): # pass all values corresponding to station (ID, data = y, time = x) args . append (( i , unique_points [ unique_points [ 'ID' ] == i ][ 'Date' ] . to_list (), unique_points [ unique_points [ 'ID' ] == i ][ self . col_name ] . to_list (), self . min_span [ 0 ], self . min_span [ 1 ], self . period_limit )) # Parallelize iteration through all grid-cells and time slices with multiprocessing . Pool ( self . numCPUs ) as multipool : for i , j , k , l , m , n , o in multipool . starmap ( self . _amplitude_and_phase , args ): self . ampfit . extend ( i ) self . phsfit . extend ( j ) self . periodfit . extend ( k ) self . ampfit_c . extend ( l ) self . phsfit_c . extend ( m ) self . periodfit_c . extend ( n ) self . seasonalfit_rmse . extend ( o ) # map phase/amplitude fits dictionary to dataframe self . phsfit = { k : v for d in self . phsfit for k , v in d . items ()} self . ampfit = { k : v for d in self . ampfit for k , v in d . items ()} self . periodfit = { k : v for d in self . periodfit for k , v in d . items ()} self . df [ 'phsfit' ] = self . df [ 'ID' ] . map ( self . phsfit ) # check if there are any valid data values if self . df [ 'phsfit' ] . isnull () . values . all ( axis = 0 ): raise Exception ( \"No valid data values, adjust --min_span inputs for time span in years {} and/or fractional obs. {} \" . format ( self . min_span [ 0 ], self . min_span [ 1 ])) self . df [ 'ampfit' ] = self . df [ 'ID' ] . map ( self . ampfit ) self . df [ 'periodfit' ] = self . df [ 'ID' ] . map ( self . periodfit ) self . phsfit_c = { k : v for d in self . phsfit_c for k , v in d . items ()} self . ampfit_c = { k : v for d in self . ampfit_c for k , v in d . items ()} self . periodfit_c = { k : v for d in self . periodfit_c for k , v in d . items ()} self . seasonalfit_rmse = { k : v for d in self . seasonalfit_rmse for k , v in d . items ()} self . df [ 'phsfit_c' ] = self . df [ 'ID' ] . map ( self . phsfit_c ) self . df [ 'ampfit_c' ] = self . df [ 'ID' ] . map ( self . ampfit_c ) self . df [ 'periodfit_c' ] = self . df [ 'ID' ] . map ( self . periodfit_c ) self . df [ 'seasonalfit_rmse' ] = self . df [ 'ID' ] . map ( self . seasonalfit_rmse ) # drop nan self . df . dropna ( how = 'any' , inplace = True ) # If grid plots specified if self . grid_seasonal_phase : # Pass mean phase of station-wise means per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ 'phsfit' ] . mean () unique_points = unique_points . groupby ([ 'gridnode' ])[ 'phsfit' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_phase = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_phase' + '.tif' ) save_gridfile ( self . grid_seasonal_phase , 'grid_seasonal_phase' , gridfile_name , self . plotbbox , self . spacing , 'days' , colorbarfmt = ' %.1i ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass mean amplitude of station-wise means per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ 'ampfit' ] . mean () unique_points = unique_points . groupby ([ 'gridnode' ])[ 'ampfit' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_amplitude = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_amplitude' + '.tif' ) save_gridfile ( self . grid_seasonal_amplitude , 'grid_seasonal_amplitude' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.3f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass mean period of station-wise means per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ 'periodfit' ] . mean () unique_points = unique_points . groupby ([ 'gridnode' ])[ 'periodfit' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_period = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_period' + '.tif' ) save_gridfile ( self . grid_seasonal_period , 'grid_seasonal_period' , gridfile_name , self . plotbbox , self . spacing , 'years' , colorbarfmt = ' %.2f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) ######################################################################################################################## # Pass mean phase stdev of station-wise means per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ 'phsfit_c' ] . mean () unique_points = unique_points . groupby ([ 'gridnode' ])[ 'phsfit_c' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_phase_stdev = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_phase_stdev' + '.tif' ) save_gridfile ( self . grid_seasonal_phase_stdev , 'grid_seasonal_phase_stdev' , gridfile_name , self . plotbbox , self . spacing , 'days' , colorbarfmt = ' %.1i ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass mean amplitude stdev of station-wise means per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ 'ampfit_c' ] . mean () unique_points = unique_points . groupby ([ 'gridnode' ])[ 'ampfit_c' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_amplitude_stdev = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_amplitude_stdev' + '.tif' ) save_gridfile ( self . grid_seasonal_amplitude_stdev , 'grid_seasonal_amplitude_stdev' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.3f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass mean period stdev of station-wise means per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ 'periodfit_c' ] . mean () unique_points = unique_points . groupby ([ 'gridnode' ])[ 'periodfit_c' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_period_stdev = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_period_stdev' + '.tif' ) save_gridfile ( self . grid_seasonal_period_stdev , 'grid_seasonal_period_stdev' , gridfile_name , self . plotbbox , self . spacing , 'years' , colorbarfmt = ' %.2e ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass mean seasonal fit RMSE of station-wise means per gridcell unique_points = self . df . groupby ([ 'ID' , 'Lon' , 'Lat' , 'gridnode' ], as_index = False )[ 'seasonalfit_rmse' ] . mean () unique_points = unique_points . groupby ([ 'gridnode' ])[ 'seasonalfit_rmse' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_fit_rmse = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_fit_rmse' + '.tif' ) save_gridfile ( self . grid_seasonal_fit_rmse , 'grid_seasonal_fit_rmse' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.3f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) ######################################################################################################################## if self . grid_seasonal_absolute_phase : # Pass absolute mean phase of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ 'phsfit' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_absolute_phase = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_absolute_phase' + '.tif' ) save_gridfile ( self . grid_seasonal_absolute_phase , 'grid_seasonal_absolute_phase' , gridfile_name , self . plotbbox , self . spacing , 'days' , colorbarfmt = ' %.1i ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass absolute mean amplitude of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ 'ampfit' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_absolute_amplitude = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_absolute_amplitude' + '.tif' ) save_gridfile ( self . grid_seasonal_absolute_amplitude , 'grid_seasonal_absolute_amplitude' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.3f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass absolute mean period of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ 'periodfit' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_absolute_period = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_absolute_period' + '.tif' ) save_gridfile ( self . grid_seasonal_absolute_period , 'grid_seasonal_absolute_period' , gridfile_name , self . plotbbox , self . spacing , 'years' , colorbarfmt = ' %.2f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) ######################################################################################################################## # Pass absolute mean phase stdev of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ 'phsfit_c' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_absolute_phase_stdev = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_absolute_phase_stdev' + '.tif' ) save_gridfile ( self . grid_seasonal_absolute_phase_stdev , 'grid_seasonal_absolute_phase_stdev' , gridfile_name , self . plotbbox , self . spacing , 'days' , colorbarfmt = ' %.1i ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass absolute mean amplitude stdev of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ 'ampfit_c' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_absolute_amplitude_stdev = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_absolute_amplitude_stdev' + '.tif' ) save_gridfile ( self . grid_seasonal_absolute_amplitude_stdev , 'grid_seasonal_absolute_amplitude_stdev' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.3f ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass absolute mean period stdev of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ 'periodfit_c' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_absolute_period_stdev = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_absolute_period_stdev' + '.tif' ) save_gridfile ( self . grid_seasonal_absolute_period_stdev , 'grid_seasonal_absolute_period_stdev' , gridfile_name , self . plotbbox , self . spacing , 'years' , colorbarfmt = ' %.2e ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' ) # Pass absolute mean seasonal fit RMSE of all data per gridcell unique_points = self . df . groupby ([ 'gridnode' ])[ 'seasonalfit_rmse' ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) self . grid_seasonal_absolute_fit_rmse = np . array ([ np . nan if i [ 0 ] not in unique_points . index . get_level_values ( 'gridnode' ) . tolist ( ) else unique_points [ i [ 0 ]] for i in enumerate ( self . gridpoints )]) . reshape ( self . grid_dim ) . T # If specified, save gridded array(s) if self . grid_to_raster : gridfile_name = os . path . join ( self . workdir , self . col_name + '_' + 'grid_seasonal_absolute_fit_rmse' + '.tif' ) save_gridfile ( self . grid_seasonal_absolute_fit_rmse , 'grid_seasonal_absolute_fit_rmse' , gridfile_name , self . plotbbox , self . spacing , self . unit , colorbarfmt = ' %.2e ' , stationsongrids = self . stationsongrids , time_lines = self . time_lines , dtype = 'float32' )","title":"create_DF()"},{"location":"reference/#RAiDER.cli.statsPlot.VariogramAnalysis","text":"Class which ingests dataframe output from 'RaiderStats' class and performs variogram analysis. Source code in RAiDER/cli/statsPlot.py 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 class VariogramAnalysis (): ''' Class which ingests dataframe output from 'RaiderStats' class and performs variogram analysis. ''' def __init__ ( self , filearg , gridpoints , col_name , unit = 'm' , workdir = './' , seasonalinterval = None , densitythreshold = 10 , binnedvariogram = False , numCPUs = 8 , variogram_per_timeslice = False , variogram_errlimit = 'inf' ): self . df = filearg self . col_name = col_name self . unit = unit self . gridpoints = gridpoints self . workdir = workdir self . seasonalinterval = seasonalinterval self . densitythreshold = densitythreshold self . binnedvariogram = binnedvariogram self . numCPUs = numCPUs self . variogram_per_timeslice = variogram_per_timeslice self . variogram_errlimit = float ( variogram_errlimit ) def _get_samples ( self , data , Nsamp = 1000 ): ''' pull samples from a 2D image for variogram analysis ''' import random if len ( data ) < self . densitythreshold : logger . warning ( 'Less than {} points for this gridcell' , self . densitythreshold ) logger . info ( 'Will pass empty list' ) d = [] indpars = [] else : indpars = list ( itertools . combinations ( range ( len ( data )), 2 )) random . shuffle ( indpars ) # subsample Nvalidsamp = int ( len ( data ) * ( len ( data ) - 1 ) / 2 ) # Only downsample if Nsamps>specified value if Nvalidsamp > Nsamp : indpars = indpars [: Nsamp ] d = np . array ([[ data [ r [ 0 ]], data [ r [ 1 ]]] for r in indpars ]) return d , indpars def _get_XY ( self , x2d , y2d , indpars ): ''' Given a list of indices, return the x,y locations from two matrices ''' x = np . array ([[ x2d [ r [ 0 ]], x2d [ r [ 1 ]]] for r in indpars ]) y = np . array ([[ y2d [ r [ 0 ]], y2d [ r [ 1 ]]] for r in indpars ]) return x , y def _get_distances ( self , XY ): ''' Return the distances between each point in a list of points ''' from scipy.spatial.distance import cdist return np . diag ( cdist ( XY [:, :, 0 ], XY [:, :, 1 ], metric = 'euclidean' )) def _get_variogram ( self , XY , xy = None ): ''' Return variograms ''' return 0.5 * np . square ( XY - xy ) # XY = 1st col xy= 2nd col def _emp_vario ( self , x , y , data , Nsamp = 1000 ): ''' Compute empirical semivariance ''' # remove NaNs if possible mask = ~ np . isnan ( data ) if False in mask : data = data [ mask ] x = x [ mask ] y = y [ mask ] # deramp temp1 , temp2 , x , y = WGS84_to_UTM ( x , y , common_center = True ) A = np . array ([ x , y , np . ones ( len ( x ))]) . T ramp = np . linalg . lstsq ( A , data . T , rcond = None )[ 0 ] data = data - ( np . matmul ( A , ramp )) samples , indpars = self . _get_samples ( data , Nsamp ) x , y = self . _get_XY ( x , y , indpars ) dists = self . _get_distances ( np . array ([[ x [:, 0 ], y [:, 0 ]], [ x [:, 1 ], y [:, 1 ]]]) . T ) vario = self . _get_variogram ( samples [:, 0 ], samples [:, 1 ]) return dists , vario def _binned_vario ( self , hEff , rawVario , xBin = None ): ''' return a binned empirical variogram ''' if xBin is None : with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , message = \"All-NaN slice encountered\" ) xBin = np . linspace ( 0 , np . nanmax ( hEff ) * .67 , 20 ) nBins = len ( xBin ) - 1 hExp , expVario = [], [] for iBin in range ( nBins ): iBinMask = np . logical_and ( xBin [ iBin ] < hEff , hEff <= xBin [ iBin + 1 ]) # circumvent indexing try : with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , message = \"Mean of empty slice\" ) hExp . append ( np . nanmean ( hEff [ iBinMask ])) expVario . append ( np . nanmean ( rawVario [ iBinMask ])) except BaseException : # TODO: Which error(s)? pass if False in ~ np . isnan ( hExp ): # NaNs present in binned histogram hExp = [ x for x in hExp if str ( x ) != 'nan' ] expVario = [ x for x in expVario if str ( x ) != 'nan' ] return np . array ( hExp ), np . array ( expVario ) def _fit_vario ( self , dists , vario , model = None , x0 = None , Nparm = None , ub = None ): ''' Fit a variogram model to data ''' from scipy.optimize import least_squares def resid ( x , d , v , m ): return ( m ( x , d ) - v ) if ub is None : with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , message = \"All-NaN slice encountered\" ) ub = np . array ([ np . nanmax ( dists ) * 0.8 , np . nanmax ( vario ) * 0.8 , np . nanmax ( vario ) * 0.8 ]) if x0 is None and Nparm is None : raise RuntimeError ( 'Must specify either x0 or the number of model parameters' ) if x0 is not None : lb = np . zeros ( len ( x0 )) if Nparm is not None : lb = np . zeros ( Nparm ) x0 = ( ub - lb ) / 2 bounds = ( lb , ub ) mask = np . isnan ( dists ) | np . isnan ( vario ) d = dists [ ~ mask ] . copy () v = vario [ ~ mask ] . copy () res_robust = least_squares ( resid , x0 , bounds = bounds , loss = 'soft_l1' , f_scale = 0.1 , args = ( d , v , model )) with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , message = \"All-NaN slice encountered\" ) d_test = np . linspace ( 0 , np . nanmax ( dists ), 100 ) # v_test is my y., # res_robust.x =a, b, c, where a = range, b = sill, and c = nugget model, d_test=x v_test = model ( res_robust . x , d_test ) return res_robust , d_test , v_test # this would be expontential plus nugget def __exponential__ ( self , parms , h , nugget = False ): ''' returns a variogram model given a set of arguments and key-word arguments ''' # a = range, b = sill, c = nugget model a , b , c = parms with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , message = \"overflow encountered in true_divide\" ) if nugget : return b * ( 1 - np . exp ( - h / a )) + c else : return b * ( 1 - np . exp ( - h / a )) # this would be gaussian plus nugget def __gaussian__ ( self , parms , h ): ''' returns a Gaussian variogram model ''' a , b , c = parms return b * ( 1 - np . exp ( - np . square ( h ) / ( a ** 2 ))) + c def _append_variogram ( self , grid_ind , grid_subset ): ''' For a given grid-cell, iterate through time slices to generate/append empirical variogram(s) ''' # Comprehensive arrays recording data across all time epochs for given station dists_arr = [] vario_arr = [] dists_binned_arr = [] vario_binned_arr = [] res_robust_arr = [] d_test_arr = [] v_test_arr = [] for j in sorted ( list ( set ( grid_subset [ 'Date' ]))): # If insufficient sample size, skip slice and record occurence if len ( np . array ( grid_subset [ grid_subset [ 'Date' ] == j ][ self . col_name ])) < self . densitythreshold : # Record skipped [gridnode, timeslice] self . skipped_slices . append ([ grid_ind , j . strftime ( \"%Y-%m- %d \" )]) else : self . gridcenterlist . append ([ 'grid {} ' . format ( grid_ind ) + 'Lat: {} Lon: {} ' . format ( str ( self . gridpoints [ grid_ind ][ 1 ]), str ( self . gridpoints [ grid_ind ][ 0 ]))]) lonarr = np . array ( grid_subset [ grid_subset [ 'Date' ] == j ][ 'Lon' ]) latarr = np . array ( grid_subset [ grid_subset [ 'Date' ] == j ][ 'Lat' ]) delayarray = np . array ( grid_subset [ grid_subset [ 'Date' ] == j ][ self . col_name ]) # fit empirical variogram for each time AND grid dists , vario = self . _emp_vario ( lonarr , latarr , delayarray ) dists_binned , vario_binned = self . _binned_vario ( dists , vario ) # fit experimental variogram for each time AND grid, model default is exponential res_robust , d_test , v_test = self . _fit_vario ( dists_binned , vario_binned , model = self . __exponential__ , x0 = None , Nparm = 3 ) # Plot empirical + experimental variogram for this gridnode and timeslice if not os . path . exists ( os . path . join ( self . workdir , 'variograms/grid {} ' . format ( grid_ind ))): os . makedirs ( os . path . join ( self . workdir , 'variograms/grid {} ' . format ( grid_ind ))) # Make variogram plots for each time-slice if self . variogram_per_timeslice : # Plot empirical variogram for this gridnode and timeslice self . plot_variogram ( grid_ind , j . strftime ( \"%Y%m %d \" ), [ self . gridpoints [ grid_ind ][ 1 ], self . gridpoints [ grid_ind ][ 0 ]], workdir = os . path . join ( self . workdir , 'variograms/grid {} ' . format ( grid_ind )), dists = dists , vario = vario , dists_binned = dists_binned , vario_binned = vario_binned ) # Plot experimental variogram for this gridnode and timeslice self . plot_variogram ( grid_ind , j . strftime ( \"%Y%m %d \" ), [ self . gridpoints [ grid_ind ][ 1 ], self . gridpoints [ grid_ind ][ 0 ]], workdir = os . path . join ( self . workdir , 'variograms/grid {} ' . format ( grid_ind )), d_test = d_test , v_test = v_test , res_robust = res_robust . x , dists_binned = dists_binned , vario_binned = vario_binned ) # append for plotting self . good_slices . append ([ grid_ind , j . strftime ( \"%Y%m %d \" )]) dists_arr . append ( dists ) vario_arr . append ( vario ) dists_binned_arr . append ( dists_binned ) vario_binned_arr . append ( vario_binned ) res_robust_arr . append ( res_robust . x ) d_test_arr . append ( d_test ) v_test_arr . append ( v_test ) # fit experimental variogram for each grid if dists_binned_arr != []: # TODO: need to change this from accumulating binned data to raw data dists_arr = np . concatenate ( dists_arr ) . ravel () vario_arr = np . concatenate ( vario_arr ) . ravel () # if specified, passed binned empirical variograms if self . binnedvariogram : dists_binned_arr = np . concatenate ( dists_binned_arr ) . ravel () vario_binned_arr = np . concatenate ( vario_binned_arr ) . ravel () else : # dists_binned_arr = dists_arr ; vario_binned_arr = vario_arr dists_binned_arr , vario_binned_arr = self . _binned_vario ( dists_arr , vario_arr ) TOT_res_robust , TOT_d_test , TOT_v_test = self . _fit_vario ( dists_binned_arr , vario_binned_arr , model = self . __exponential__ , x0 = None , Nparm = 3 ) tot_timetag = self . good_slices [ 0 ][ 1 ] + '\u2013' + self . good_slices [ - 1 ][ 1 ] # Append TOT arrays self . TOT_good_slices . append ([ grid_ind , tot_timetag ]) self . TOT_res_robust_arr . append ( TOT_res_robust . x ) self . TOT_tot_timetag . append ( tot_timetag ) var_rmse = np . sqrt ( np . nanmean (( TOT_res_robust . fun ) ** 2 )) if var_rmse <= self . variogram_errlimit : self . TOT_res_robust_rmse . append ( var_rmse ) else : self . TOT_res_robust_rmse . append ( np . array ( np . nan )) # Plot empirical variogram for this gridnode self . plot_variogram ( grid_ind , tot_timetag , [ self . gridpoints [ grid_ind ][ 1 ], self . gridpoints [ grid_ind ][ 0 ]], workdir = os . path . join ( self . workdir , 'variograms/grid {} ' . format ( grid_ind )), dists = dists_arr , vario = vario_arr , dists_binned = dists_binned_arr , vario_binned = vario_binned_arr , seasonalinterval = self . seasonalinterval ) # Plot experimental variogram for this gridnode self . plot_variogram ( grid_ind , tot_timetag , [ self . gridpoints [ grid_ind ][ 1 ], self . gridpoints [ grid_ind ][ 0 ]], workdir = os . path . join ( self . workdir , 'variograms/grid {} ' . format ( grid_ind )), d_test = TOT_d_test , v_test = TOT_v_test , res_robust = TOT_res_robust . x , seasonalinterval = self . seasonalinterval , dists_binned = dists_binned_arr , vario_binned = vario_binned_arr ) # Record sparse grids which didn't have sufficient sample size of data through any of the timeslices else : self . sparse_grids . append ( grid_ind ) return self . TOT_good_slices , self . TOT_res_robust_arr , self . TOT_res_robust_rmse , self . gridcenterlist def create_variograms ( self ): ''' Iterate through grid-cells and time slices to generate empirical variogram(s) ''' # track data for plotting self . TOT_good_slices = [] self . TOT_res_robust_arr = [] self . TOT_res_robust_rmse = [] self . TOT_tot_timetag = [] # track pass/rejected grids self . sparse_grids = [] self . good_slices = [] self . skipped_slices = [] # record grid-centers for lookup-table self . gridcenterlist = [] args = [] for i in sorted ( list ( set ( self . df [ 'gridnode' ]))): # pass subset of all stations corresponding to given grid-cell grid_subset = self . df [ self . df [ 'gridnode' ] == i ] args . append (( i , grid_subset )) # Parallelize iteration through all grid-cells and time slices with multiprocessing . Pool ( self . numCPUs ) as multipool : for i , j , k , l in multipool . starmap ( self . _append_variogram , args ): self . TOT_good_slices . extend ( i ) self . TOT_res_robust_arr . extend ( j ) self . TOT_res_robust_rmse . extend ( k ) self . gridcenterlist . extend ( l ) # save grid-center lookup table self . gridcenterlist = [ list ( i ) for i in set ( tuple ( j ) for j in self . gridcenterlist )] self . gridcenterlist . sort ( key = lambda x : int ( x [ 0 ][ 4 : 6 ])) gridcenter = open ( ( os . path . join ( self . workdir , 'variograms/gridlocation_lookup.txt' )), \"w\" ) for element in self . gridcenterlist : gridcenter . writelines ( \" \\n \" . join ( element )) gridcenter . write ( \" \\n \" ) gridcenter . close () TOT_grids = [ i [ 0 ] for i in self . TOT_good_slices ] return TOT_grids , self . TOT_res_robust_arr , self . TOT_res_robust_rmse def plot_variogram ( self , gridID , timeslice , coords , workdir = './' , d_test = None , v_test = None , res_robust = None , dists = None , vario = None , dists_binned = None , vario_binned = None , seasonalinterval = None ): ''' Make empirical and/or experimental variogram fit plots ''' # If specified workdir doesn't exist, create it if not os . path . exists ( workdir ): os . mkdir ( workdir ) # make plot title title_str = ' \\n Lat: {:.2f} Lon: {:.2f} \\n Time: {} ' . format ( coords [ 1 ], coords [ 0 ], str ( timeslice )) if seasonalinterval : title_str += ' Season(mm/dd): {} / {} \u2013 {} / {} ' . format ( int ( timeslice [ 4 : 6 ]), int ( timeslice [ 6 : 8 ]), int ( timeslice [ - 4 : - 2 ]), int ( timeslice [ - 2 :])) if dists is not None and vario is not None : # scale from m to user-defined units dists = [ convert_SI ( i , 'm' , self . unit ) for i in dists ] plt . scatter ( dists , vario , s = 1 , facecolor = '0.5' , label = 'raw' ) if dists_binned is not None and vario_binned is not None : # scale from m to user-defined units dists_binned = [ convert_SI ( i , 'm' , self . unit ) for i in dists_binned ] plt . plot ( dists_binned , vario_binned , 'bo' , label = 'binned' ) if res_robust is not None : plt . axhline ( y = res_robust [ 1 ], color = 'g' , linestyle = '--' , label = '\u0263 \\u0332\\u00b2 ( {} \\u00b2 )' . format ( self . unit )) # scale from m to user-defined units res_robust [ 0 ] = convert_SI ( res_robust [ 0 ], 'm' , self . unit ) plt . axvline ( x = res_robust [ 0 ], color = 'c' , linestyle = '--' , label = 'h ( {} )' . format ( self . unit )) if d_test is not None and v_test is not None : # scale from m to user-defined units d_test = [ convert_SI ( i , 'm' , self . unit ) for i in d_test ] plt . plot ( d_test , v_test , 'r-' , label = 'experimental fit' ) plt . xlabel ( 'Distance ( {} )' . format ( self . unit )) plt . ylabel ( 'Dissimilarity ( {} \\u00b2 )' . format ( self . unit )) plt . legend ( bbox_to_anchor = ( 1.02 , 1 ), loc = 'upper left' , borderaxespad = 0. , framealpha = 1. ) # Plot empirical variogram if d_test is None and v_test is None : plt . title ( 'Empirical variogram' + title_str ) plt . tight_layout () plt . savefig ( os . path . join ( workdir , 'grid {} _timeslice {} _justEMPvariogram.eps' . format ( gridID , timeslice ))) # Plot just experimental variogram else : plt . title ( 'Experimental variogram' + title_str ) plt . tight_layout () plt . savefig ( os . path . join ( workdir , 'grid {} _timeslice {} _justEXPvariogram.eps' . format ( gridID , timeslice ))) plt . close () return","title":"VariogramAnalysis"},{"location":"reference/#RAiDER.cli.statsPlot.VariogramAnalysis.__exponential__","text":"returns a variogram model given a set of arguments and key-word arguments Source code in RAiDER/cli/statsPlot.py 463 464 465 466 467 468 469 470 471 472 473 474 475 def __exponential__ ( self , parms , h , nugget = False ): ''' returns a variogram model given a set of arguments and key-word arguments ''' # a = range, b = sill, c = nugget model a , b , c = parms with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , message = \"overflow encountered in true_divide\" ) if nugget : return b * ( 1 - np . exp ( - h / a )) + c else : return b * ( 1 - np . exp ( - h / a ))","title":"__exponential__()"},{"location":"reference/#RAiDER.cli.statsPlot.VariogramAnalysis.__gaussian__","text":"returns a Gaussian variogram model Source code in RAiDER/cli/statsPlot.py 478 479 480 481 482 483 def __gaussian__ ( self , parms , h ): ''' returns a Gaussian variogram model ''' a , b , c = parms return b * ( 1 - np . exp ( - np . square ( h ) / ( a ** 2 ))) + c","title":"__gaussian__()"},{"location":"reference/#RAiDER.cli.statsPlot.VariogramAnalysis.create_variograms","text":"Iterate through grid-cells and time slices to generate empirical variogram(s) Source code in RAiDER/cli/statsPlot.py 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 def create_variograms ( self ): ''' Iterate through grid-cells and time slices to generate empirical variogram(s) ''' # track data for plotting self . TOT_good_slices = [] self . TOT_res_robust_arr = [] self . TOT_res_robust_rmse = [] self . TOT_tot_timetag = [] # track pass/rejected grids self . sparse_grids = [] self . good_slices = [] self . skipped_slices = [] # record grid-centers for lookup-table self . gridcenterlist = [] args = [] for i in sorted ( list ( set ( self . df [ 'gridnode' ]))): # pass subset of all stations corresponding to given grid-cell grid_subset = self . df [ self . df [ 'gridnode' ] == i ] args . append (( i , grid_subset )) # Parallelize iteration through all grid-cells and time slices with multiprocessing . Pool ( self . numCPUs ) as multipool : for i , j , k , l in multipool . starmap ( self . _append_variogram , args ): self . TOT_good_slices . extend ( i ) self . TOT_res_robust_arr . extend ( j ) self . TOT_res_robust_rmse . extend ( k ) self . gridcenterlist . extend ( l ) # save grid-center lookup table self . gridcenterlist = [ list ( i ) for i in set ( tuple ( j ) for j in self . gridcenterlist )] self . gridcenterlist . sort ( key = lambda x : int ( x [ 0 ][ 4 : 6 ])) gridcenter = open ( ( os . path . join ( self . workdir , 'variograms/gridlocation_lookup.txt' )), \"w\" ) for element in self . gridcenterlist : gridcenter . writelines ( \" \\n \" . join ( element )) gridcenter . write ( \" \\n \" ) gridcenter . close () TOT_grids = [ i [ 0 ] for i in self . TOT_good_slices ] return TOT_grids , self . TOT_res_robust_arr , self . TOT_res_robust_rmse","title":"create_variograms()"},{"location":"reference/#RAiDER.cli.statsPlot.VariogramAnalysis.plot_variogram","text":"Make empirical and/or experimental variogram fit plots Source code in RAiDER/cli/statsPlot.py 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 def plot_variogram ( self , gridID , timeslice , coords , workdir = './' , d_test = None , v_test = None , res_robust = None , dists = None , vario = None , dists_binned = None , vario_binned = None , seasonalinterval = None ): ''' Make empirical and/or experimental variogram fit plots ''' # If specified workdir doesn't exist, create it if not os . path . exists ( workdir ): os . mkdir ( workdir ) # make plot title title_str = ' \\n Lat: {:.2f} Lon: {:.2f} \\n Time: {} ' . format ( coords [ 1 ], coords [ 0 ], str ( timeslice )) if seasonalinterval : title_str += ' Season(mm/dd): {} / {} \u2013 {} / {} ' . format ( int ( timeslice [ 4 : 6 ]), int ( timeslice [ 6 : 8 ]), int ( timeslice [ - 4 : - 2 ]), int ( timeslice [ - 2 :])) if dists is not None and vario is not None : # scale from m to user-defined units dists = [ convert_SI ( i , 'm' , self . unit ) for i in dists ] plt . scatter ( dists , vario , s = 1 , facecolor = '0.5' , label = 'raw' ) if dists_binned is not None and vario_binned is not None : # scale from m to user-defined units dists_binned = [ convert_SI ( i , 'm' , self . unit ) for i in dists_binned ] plt . plot ( dists_binned , vario_binned , 'bo' , label = 'binned' ) if res_robust is not None : plt . axhline ( y = res_robust [ 1 ], color = 'g' , linestyle = '--' , label = '\u0263 \\u0332\\u00b2 ( {} \\u00b2 )' . format ( self . unit )) # scale from m to user-defined units res_robust [ 0 ] = convert_SI ( res_robust [ 0 ], 'm' , self . unit ) plt . axvline ( x = res_robust [ 0 ], color = 'c' , linestyle = '--' , label = 'h ( {} )' . format ( self . unit )) if d_test is not None and v_test is not None : # scale from m to user-defined units d_test = [ convert_SI ( i , 'm' , self . unit ) for i in d_test ] plt . plot ( d_test , v_test , 'r-' , label = 'experimental fit' ) plt . xlabel ( 'Distance ( {} )' . format ( self . unit )) plt . ylabel ( 'Dissimilarity ( {} \\u00b2 )' . format ( self . unit )) plt . legend ( bbox_to_anchor = ( 1.02 , 1 ), loc = 'upper left' , borderaxespad = 0. , framealpha = 1. ) # Plot empirical variogram if d_test is None and v_test is None : plt . title ( 'Empirical variogram' + title_str ) plt . tight_layout () plt . savefig ( os . path . join ( workdir , 'grid {} _timeslice {} _justEMPvariogram.eps' . format ( gridID , timeslice ))) # Plot just experimental variogram else : plt . title ( 'Experimental variogram' + title_str ) plt . tight_layout () plt . savefig ( os . path . join ( workdir , 'grid {} _timeslice {} _justEXPvariogram.eps' . format ( gridID , timeslice ))) plt . close () return","title":"plot_variogram()"},{"location":"reference/#RAiDER.cli.statsPlot.convert_SI","text":"Convert input to desired units Source code in RAiDER/cli/statsPlot.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def convert_SI ( val , unit_in , unit_out ): ''' Convert input to desired units ''' SI = { 'mm' : 0.001 , 'cm' : 0.01 , 'm' : 1.0 , 'km' : 1000. , 'mm^2' : 1e-6 , 'cm^2' : 1e-4 , 'm^2' : 1.0 , 'km^2' : 1e+6 } # avoid conversion if output unit in time if unit_out in [ 'minute' , 'hour' , 'day' , 'year' ]: # adjust if input isn't datetime, and assume it to be part of workflow # e.g. sigZTD filter, already extracted datetime object try : return eval ( 'val.apply(pd.to_datetime).dt. {} .astype(float).astype(\"Int32\")' . format ( unit_out )) except BaseException : # TODO: Which error(s)? return val # check if output spatial unit is supported if unit_out not in SI : raise Exception ( \"User-specified output unit {} not recognized.\" . format ( unit_out )) return val * SI [ unit_in ] / SI [ unit_out ]","title":"convert_SI()"},{"location":"reference/#RAiDER.cli.statsPlot.create_parser","text":"Parse command line arguments using argparse. Source code in RAiDER/cli/statsPlot.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 def create_parser (): \"\"\"Parse command line arguments using argparse.\"\"\" parser = argparse . ArgumentParser ( formatter_class = argparse . RawDescriptionHelpFormatter , description = \"\"\" Perform basic statistical analyses concerning the spatiotemporal distribution of zenith delays. Specifically, make any of the following specified plot(s): scatterplot of station locations, total empirical and experimental variogram fits for data in each grid cell (and for each valid time-slice if -variogram_per_timeslice specified), and gridded heatmaps of data, station distribution, range and sill values associated with experimental variogram fits. The default is to generate all of these. Example call to plot gridded station mean delay in a specific time interval : raiderStats.py -f <filename> -grid_delay_mean -ti '2016-01-01 2018-01-01' Example call to plot gridded station mean delay in a specific time interval with superimposed gridlines and station scatterplots : raiderStats.py -f <filename> -grid_delay_mean -ti '2016-01-01 2018-01-01' --drawgridlines --stationsongrids Example call to plot gridded station variogram in a specific time interval and through explicitly the summer seasons: raiderStats.py -f <filename> -grid_delay_mean -ti '2016-01-01 2018-01-01' --seasonalinterval '06-21 09-21' -variogramplot \"\"\" ) # User inputs userinps = parser . add_argument_group ( 'User inputs/options for which especially careful review is recommended' ) userinps . add_argument ( '-f' , '--file' , dest = 'fname' , type = str , required = True , help = 'Final output file generated from downloadGNSSDelays.py which contains GPS zenith delays for a specified time period and spatial footprint. ' ) userinps . add_argument ( '-c' , '--column_name' , dest = 'col_name' , type = str , default = 'ZTD' , help = 'Name of the input column to plot. Input assumed to be in units of meters' ) userinps . add_argument ( '-u' , '--unit' , dest = 'unit' , type = str , default = 'm' , help = 'Specified output unit (as distance or time), by default m. Input unit assumed to be m following convention in downloadGNSSDelays.py. Refer to \"convert_SI\" for supported units. Note if you specify time unit here, you must specify input for \"--obs_errlimit\" to be in units of m' ) userinps . add_argument ( '-w' , '--workdir' , dest = 'workdir' , default = './' , help = 'Specify directory to deposit all outputs. Default is local directory where script is launched.' ) add_cpus ( userinps ) userinps . add_argument ( '-verbose' , '--verbose' , action = 'store_true' , dest = 'verbose' , help = \"Run in verbose (debug) mode. Default False\" ) # Spatiotemporal subset options dtsubsets = parser . add_argument_group ( 'Controls for spatiotemporal subsetting.' ) dtsubsets . add_argument ( '-b' , '--bounding_box' , dest = 'bounding_box' , type = str , default = None , help = \"Provide either valid shapefile or Lat/Lon Bounding SNWE. -- Example : '19 20 -99.5 -98.5'\" ) dtsubsets . add_argument ( '-sp' , '--spacing' , dest = 'spacing' , type = float , default = '1' , help = 'Specify spacing of grid-cells for statistical analyses. By default 1 deg.' ) dtsubsets . add_argument ( '-ti' , '--timeinterval' , dest = 'timeinterval' , type = str , default = None , help = \"Subset in time by specifying earliest YYYY-MM-DD date followed by latest date YYYY-MM-DD. -- Example : '2016-01-01 2019-01-01'.\" ) dtsubsets . add_argument ( '-si' , '--seasonalinterval' , dest = 'seasonalinterval' , type = str , default = None , help = \"Subset in by an specific interval for each year by specifying earliest MM-DD time followed by latest MM-DD time. -- Example : '03-21 06-21'.\" ) dtsubsets . add_argument ( '-oe' , '--obs_errlimit' , dest = 'obs_errlimit' , type = float , default = 'inf' , help = \"Observation error threshold to discard observations with large uncertainties.\" ) # Plot formatting/options pltformat = parser . add_argument_group ( 'Optional controls for plot formatting/options.' ) pltformat . add_argument ( '-figdpi' , '--figdpi' , dest = 'figdpi' , type = int , default = 100 , help = 'DPI to use for saving figures' ) pltformat . add_argument ( '-title' , '--user_title' , dest = 'user_title' , type = str , default = None , help = 'Specify custom title for plots.' ) pltformat . add_argument ( '-fmt' , '--plot_format' , dest = 'plot_fmt' , type = str , default = 'png' , help = 'Plot format to use for saving figures' ) pltformat . add_argument ( '-cb' , '--color_bounds' , dest = 'cbounds' , type = str , default = None , help = 'List of two floats to use as color axis bounds' ) pltformat . add_argument ( '-cp' , '--colorpercentile' , dest = 'colorpercentile' , type = float , default = None , nargs = 2 , help = 'Set low and upper percentile for plot colorbars. By default 25 %% and 95 %% , respectively.' ) pltformat . add_argument ( '-cm' , '--colormap' , dest = 'usr_colormap' , type = str , default = 'hot_r' , help = 'Specify matplotlib colorbar.' ) pltformat . add_argument ( '-dt' , '--densitythreshold' , dest = 'densitythreshold' , type = int , default = '10' , help = 'For variogram plots, given grid-cell is only valid if it contains this specified threshold of stations. By default 10 stations.' ) pltformat . add_argument ( '-sg' , '--stationsongrids' , dest = 'stationsongrids' , action = 'store_true' , help = 'In gridded plots, superimpose your gridded array with a scatterplot of station locations.' ) pltformat . add_argument ( '-dg' , '--drawgridlines' , dest = 'drawgridlines' , action = 'store_true' , help = 'Draw gridlines on gridded plots.' ) pltformat . add_argument ( '-tl' , '--time_lines' , dest = 'time_lines' , action = 'store_true' , help = 'Draw central longitudinal lines with respect to datetime. Most useful for local-time analyses.' ) pltformat . add_argument ( '-plotall' , '--plotall' , action = 'store_true' , dest = 'plotall' , help = \"Generate all supported plots, including variogram plots.\" ) pltformat . add_argument ( '-min_span' , '--min_span' , dest = 'min_span' , type = float , default = [ 2 , 0.6 ], nargs = 2 , help = \"Minimum TS span (years) and minimum fractional observations in span (fraction) imposed for seasonal amplitude/phase analyses to be performed for a given station.\" ) pltformat . add_argument ( '-period_limit' , '--period_limit' , dest = 'period_limit' , type = float , default = 0. , help = \"period limit (years) imposed for seasonal amplitude/phase analyses to be performed for a given station.\" ) # All plot types # Station scatter-plots pltscatter = parser . add_argument_group ( 'Supported types of individual station scatter-plots.' ) pltscatter . add_argument ( '-station_distribution' , '--station_distribution' , action = 'store_true' , dest = 'station_distribution' , help = \"Plot station distribution.\" ) pltscatter . add_argument ( '-station_delay_mean' , '--station_delay_mean' , action = 'store_true' , dest = 'station_delay_mean' , help = \"Plot station mean delay.\" ) pltscatter . add_argument ( '-station_delay_median' , '--station_delay_median' , action = 'store_true' , dest = 'station_delay_median' , help = \"Plot station median delay.\" ) pltscatter . add_argument ( '-station_delay_stdev' , '--station_delay_stdev' , action = 'store_true' , dest = 'station_delay_stdev' , help = \"Plot station delay stdev.\" ) pltscatter . add_argument ( '-station_seasonal_phase' , '--station_seasonal_phase' , action = 'store_true' , dest = 'station_seasonal_phase' , help = \"Plot station delay phase/amplitude.\" ) pltscatter . add_argument ( '-phaseamp_per_station' , '--phaseamp_per_station' , action = 'store_true' , dest = 'phaseamp_per_station' , help = \"Save debug figures of curve-fit vs data per station.\" ) # Gridded plots pltgrids = parser . add_argument_group ( 'Supported types of gridded plots.' ) pltgrids . add_argument ( '-grid_heatmap' , '--grid_heatmap' , action = 'store_true' , dest = 'grid_heatmap' , help = \"Plot gridded station heatmap.\" ) pltgrids . add_argument ( '-grid_delay_mean' , '--grid_delay_mean' , action = 'store_true' , dest = 'grid_delay_mean' , help = \"Plot gridded station-wise mean delay.\" ) pltgrids . add_argument ( '-grid_delay_median' , '--grid_delay_median' , action = 'store_true' , dest = 'grid_delay_median' , help = \"Plot gridded station-wise median delay.\" ) pltgrids . add_argument ( '-grid_delay_stdev' , '--grid_delay_stdev' , action = 'store_true' , dest = 'grid_delay_stdev' , help = \"Plot gridded station-wise delay stdev.\" ) pltgrids . add_argument ( '-grid_seasonal_phase' , '--grid_seasonal_phase' , action = 'store_true' , dest = 'grid_seasonal_phase' , help = \"Plot gridded station-wise delay phase/amplitude.\" ) pltgrids . add_argument ( '-grid_delay_absolute_mean' , '--grid_delay_absolute_mean' , action = 'store_true' , dest = 'grid_delay_absolute_mean' , help = \"Plot absolute gridded station mean delay.\" ) pltgrids . add_argument ( '-grid_delay_absolute_median' , '--grid_delay_absolute_median' , action = 'store_true' , dest = 'grid_delay_absolute_median' , help = \"Plot absolute gridded station median delay.\" ) pltgrids . add_argument ( '-grid_delay_absolute_stdev' , '--grid_delay_absolute_stdev' , action = 'store_true' , dest = 'grid_delay_absolute_stdev' , help = \"Plot absolute gridded station delay stdev.\" ) pltgrids . add_argument ( '-grid_seasonal_absolute_phase' , '--grid_seasonal_absolute_phase' , action = 'store_true' , dest = 'grid_seasonal_absolute_phase' , help = \"Plot absolute gridded station delay phase/amplitude.\" ) pltgrids . add_argument ( '-grid_to_raster' , '--grid_to_raster' , action = 'store_true' , dest = 'grid_to_raster' , help = \"Save gridded array as raster. May directly load/plot in successive script call.\" ) # Variogram plots pltvario = parser . add_argument_group ( 'Supported types of variogram plots.' ) pltvario . add_argument ( '-variogramplot' , '--variogramplot' , action = 'store_true' , dest = 'variogramplot' , help = \"Plot gridded station variogram.\" ) pltvario . add_argument ( '-binnedvariogram' , '--binnedvariogram' , action = 'store_true' , dest = 'binnedvariogram' , help = \"Apply experimental variogram fit to total binned empirical variograms for each time slice. Default is to pass total unbinned empiricial variogram.\" ) pltvario . add_argument ( '-variogram_per_timeslice' , '--variogram_per_timeslice' , action = 'store_true' , dest = 'variogram_per_timeslice' , help = \"Generate variogram plots per gridded station AND time-slice.\" ) pltvario . add_argument ( '-variogram_errlimit' , '--variogram_errlimit' , dest = 'variogram_errlimit' , type = float , default = 'inf' , help = \"Variogram RMSE threshold to discard grid-cells with large uncertainties.\" ) return parser","title":"create_parser()"},{"location":"reference/#RAiDER.cli.statsPlot.load_gridfile","text":"Function to load gridded-arrays saved from previous runs. Source code in RAiDER/cli/statsPlot.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 def load_gridfile ( fname , unit ): ''' Function to load gridded-arrays saved from previous runs. ''' with rasterio . open ( fname ) as src : grid_array = src . read () . astype ( float ) # Read metadata variables needed for plotting metadata_dict = src . tags () # Initiate no-data array to mask data nodat_arr = [ 0 , np . nan , np . inf ] if unit in [ 'minute' , 'hour' , 'day' , 'year' ]: nodat_arr = [ np . nan , np . inf ] # set masked values as nans for i in nodat_arr : grid_array = np . ma . masked_where ( grid_array == i , grid_array ) grid_array = np . ma . filled ( grid_array , np . nan ) # Make plotting command a global variable gridfile_type = metadata_dict [ 'gridfile_type' ] globals ()[ gridfile_type ] = True plotbbox = [ float ( i ) for i in metadata_dict [ 'plotbbox' ] . split ()] spacing = float ( metadata_dict [ 'spacing' ]) colorbarfmt = metadata_dict [ 'colorbarfmt' ] inputunit = metadata_dict [ 'unit' ] # adjust conversion if native units are squared if '^2' in inputunit : unit = unit . split ( '^2' )[ 0 ] + '^2' # convert to specified output unit grid_array = convert_SI ( grid_array , inputunit , unit ) # Backwards compatible for cases where this key doesn't exist try : time_lines = metadata_dict [ 'time_lines' ] except KeyError : time_lines = False if metadata_dict [ 'stationsongrids' ] == 'False' : stationsongrids = False else : stationsongrids = [ float ( i ) for i in metadata_dict [ 'stationsongrids' . split ()]] if metadata_dict [ 'time_lines' ] == 'False' : time_lines = False else : time_lines = [ float ( i ) for i in metadata_dict [ 'time_lines' ] . split ()] return grid_array , plotbbox , spacing , colorbarfmt , stationsongrids , time_lines","title":"load_gridfile()"},{"location":"reference/#RAiDER.cli.statsPlot.midpoint","text":"Calculate central longitude for '--time_lines' option Source code in RAiDER/cli/statsPlot.py 195 196 197 198 199 200 201 202 203 204 205 206 207 def midpoint ( p1 , p2 ): ''' Calculate central longitude for '--time_lines' option ''' import math lat1 , lon1 , lat2 , lon2 = map ( math . radians , ( p1 [ 0 ], p1 [ 1 ], p2 [ 0 ], p2 [ 1 ])) dlon = lon2 - lon1 dx = math . cos ( lat2 ) * math . cos ( dlon ) dy = math . cos ( lat2 ) * math . sin ( dlon ) lon3 = lon1 + math . atan2 ( dy , math . cos ( lat1 ) + dx ) return ( int ( math . degrees ( lon3 )))","title":"midpoint()"},{"location":"reference/#RAiDER.cli.statsPlot.save_gridfile","text":"Function to save gridded-arrays as GDAL-readable file. Source code in RAiDER/cli/statsPlot.py 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def save_gridfile ( df , gridfile_type , fname , plotbbox , spacing , unit , colorbarfmt = ' %.2f ' , stationsongrids = False , time_lines = False , dtype = \"float32\" , noData = np . nan ): ''' Function to save gridded-arrays as GDAL-readable file. ''' # Pass metadata metadata_dict = {} metadata_dict [ 'gridfile_type' ] = gridfile_type metadata_dict [ 'plotbbox' ] = ' ' . join ([ str ( i ) for i in plotbbox ]) metadata_dict [ 'spacing' ] = str ( spacing ) metadata_dict [ 'unit' ] = unit if unit in [ 'minute' , 'hour' , 'day' , 'year' ]: colorbarfmt = ' %1i ' metadata_dict [ 'colorbarfmt' ] = colorbarfmt if stationsongrids : metadata_dict [ 'stationsongrids' ] = ' ' . join ([ str ( i ) for i in stationsongrids ]) else : metadata_dict [ 'stationsongrids' ] = 'False' if time_lines : metadata_dict [ 'time_lines' ] = ' ' . join ([ str ( i ) for i in time_lines ]) else : metadata_dict [ 'time_lines' ] = 'False' # Write data to file with rasterio . open ( fname , mode = \"w\" , count = 1 , width = df . shape [ 1 ], height = df . shape [ 0 ], dtype = dtype , nodata = noData ) as dst : dst . write ( df , 1 ) dst . update_tags ( 1 , ** metadata_dict ) # Finalize VRT vrtname = fname + \".vrt\" rasterio . shutil . copy ( fname , fname + \".vrt\" , driver = \"VRT\" ) return","title":"save_gridfile()"},{"location":"reference/#RAiDER.cli.statsPlot.stats_analyses","text":"Main workflow for generating a suite of plots to illustrate spatiotemporal distribution and/or character of zenith delays Source code in RAiDER/cli/statsPlot.py 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 def stats_analyses ( fname , col_name , unit , workdir , numCPUs , verbose , bbox , spacing , timeinterval , seasonalinterval , obs_errlimit , figdpi , user_title , plot_fmt , cbounds , colorpercentile , usr_colormap , densitythreshold , stationsongrids , drawgridlines , time_lines , plotall , station_distribution , station_delay_mean , station_delay_median , station_delay_stdev , station_seasonal_phase , phaseamp_per_station , grid_heatmap , grid_delay_mean , grid_delay_median , grid_delay_stdev , grid_seasonal_phase , grid_delay_absolute_mean , grid_delay_absolute_median , grid_delay_absolute_stdev , grid_seasonal_absolute_phase , grid_to_raster , min_span , period_limit , variogramplot , binnedvariogram , variogram_per_timeslice , variogram_errlimit ): ''' Main workflow for generating a suite of plots to illustrate spatiotemporal distribution and/or character of zenith delays ''' if verbose : logger . setLevel ( logging . DEBUG ) # Control DPI for output figures mpl . rcParams [ 'savefig.dpi' ] = figdpi # If user requests to generate all plots. if plotall : logger . info ( '\"-plotall\" == True. All plots will be made.' ) station_distribution = True station_delay_mean = True station_delay_median = True station_delay_stdev = True station_seasonal_phase = True grid_heatmap = True grid_delay_mean = True grid_delay_median = True grid_delay_stdev = True grid_seasonal_phase = True grid_delay_absolute_mean = True grid_delay_absolute_median = True grid_delay_absolute_stdev = True grid_seasonal_absolute_phase = True variogramplot = True logger . info ( \"***Stats Function:***\" ) # prep dataframe object for plotting/variogram analysis based off of user specifications df_stats = RaiderStats ( fname , col_name , unit , workdir , bbox , spacing , timeinterval , seasonalinterval , obs_errlimit , time_lines , stationsongrids , station_seasonal_phase , cbounds , colorpercentile , usr_colormap , grid_heatmap , grid_delay_mean , grid_delay_median , grid_delay_stdev , grid_seasonal_phase , grid_delay_absolute_mean , grid_delay_absolute_median , grid_delay_absolute_stdev , grid_seasonal_absolute_phase , grid_to_raster , min_span , period_limit , numCPUs , phaseamp_per_station ) # Station plots # Plot each individual station if station_distribution : logger . info ( \"- Plot spatial distribution of stations.\" ) unique_points = df_stats . df . groupby ([ 'Lon' , 'Lat' ]) . size () df_stats ([ unique_points . index . get_level_values ( 'Lon' ) . tolist (), unique_points . index . get_level_values ( 'Lat' ) . tolist ( )], 'station_distribution' , workdir = os . path . join ( workdir , 'figures' ), plotFormat = plot_fmt , userTitle = user_title ) # Plot mean delay per station if station_delay_mean : logger . info ( \"- Plot mean delay for each station.\" ) unique_points = df_stats . df . groupby ( [ 'Lon' , 'Lat' ])[ col_name ] . median () unique_points . dropna ( how = 'any' , inplace = True ) df_stats ([ unique_points . index . get_level_values ( 'Lon' ) . tolist (), unique_points . index . get_level_values ( 'Lat' ) . tolist ( ), unique_points . values ], 'station_delay_mean' , workdir = os . path . join ( workdir , 'figures' ), plotFormat = plot_fmt , userTitle = user_title ) # Plot median delay per station if station_delay_median : logger . info ( \"- Plot median delay for each station.\" ) unique_points = df_stats . df . groupby ( [ 'Lon' , 'Lat' ])[ col_name ] . mean () unique_points . dropna ( how = 'any' , inplace = True ) df_stats ([ unique_points . index . get_level_values ( 'Lon' ) . tolist (), unique_points . index . get_level_values ( 'Lat' ) . tolist ( ), unique_points . values ], 'station_delay_median' , workdir = os . path . join ( workdir , 'figures' ), plotFormat = plot_fmt , userTitle = user_title ) # Plot delay stdev per station if station_delay_stdev : logger . info ( \"- Plot delay stdev for each station.\" ) unique_points = df_stats . df . groupby ( [ 'Lon' , 'Lat' ])[ col_name ] . std () unique_points . dropna ( how = 'any' , inplace = True ) df_stats ([ unique_points . index . get_level_values ( 'Lon' ) . tolist (), unique_points . index . get_level_values ( 'Lat' ) . tolist ( ), unique_points . values ], 'station_delay_stdev' , workdir = os . path . join ( workdir , 'figures' ), plotFormat = plot_fmt , userTitle = user_title ) # Plot delay phase/amplitude per station if station_seasonal_phase : logger . info ( \"- Plot delay phase/amplitude for each station.\" ) # phase unique_points_phase = df_stats . df . groupby ( [ 'Lon' , 'Lat' ])[ 'phsfit' ] . mean () unique_points_phase . dropna ( how = 'any' , inplace = True ) df_stats ([ unique_points_phase . index . get_level_values ( 'Lon' ) . tolist (), unique_points_phase . index . get_level_values ( 'Lat' ) . tolist ( ), unique_points_phase . values ], 'station_seasonal_phase' , workdir = os . path . join ( workdir , 'figures' ), colorbarfmt = ' %.1i ' , plotFormat = plot_fmt , userTitle = user_title ) # amplitude unique_points_amplitude = df_stats . df . groupby ( [ 'Lon' , 'Lat' ])[ 'ampfit' ] . mean () unique_points_amplitude . dropna ( how = 'any' , inplace = True ) df_stats ([ unique_points_amplitude . index . get_level_values ( 'Lon' ) . tolist (), unique_points_amplitude . index . get_level_values ( 'Lat' ) . tolist ( ), unique_points_amplitude . values ], 'station_seasonal_amplitude' , workdir = os . path . join ( workdir , 'figures' ), colorbarfmt = ' %.3f ' , plotFormat = plot_fmt , userTitle = user_title ) # period unique_points_period = df_stats . df . groupby ( [ 'Lon' , 'Lat' ])[ 'periodfit' ] . mean () df_stats ([ unique_points_period . index . get_level_values ( 'Lon' ) . tolist (), unique_points_period . index . get_level_values ( 'Lat' ) . tolist ( ), unique_points_period . values ], 'station_delay_period' , workdir = os . path . join ( workdir , 'figures' ), colorbarfmt = ' %.2f ' , plotFormat = plot_fmt , userTitle = user_title ) # Gridded station plots # Plot density of stations for each gridcell if isinstance ( df_stats . grid_heatmap , np . ndarray ): logger . info ( \"- Plot density of stations per gridcell.\" ) df_stats ( df_stats . grid_heatmap , 'grid_heatmap' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.1i ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot mean of station-wise mean delay across each gridcell if isinstance ( df_stats . grid_delay_mean , np . ndarray ): logger . info ( \"- Plot mean of station-wise mean delay across each gridcell.\" ) df_stats ( df_stats . grid_delay_mean , 'grid_delay_mean' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.2f ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot mean of station-wise median delay across each gridcell if isinstance ( df_stats . grid_delay_median , np . ndarray ): logger . info ( \"- Plot mean of station-wise median delay across each gridcell.\" ) df_stats ( df_stats . grid_delay_median , 'grid_delay_median' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.2f ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot mean of station-wise stdev delay across each gridcell if isinstance ( df_stats . grid_delay_stdev , np . ndarray ): logger . info ( \"- Plot mean of station-wise stdev delay across each gridcell.\" ) df_stats ( df_stats . grid_delay_stdev , 'grid_delay_stdev' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.2f ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot mean of station-wise delay phase across each gridcell if isinstance ( df_stats . grid_seasonal_phase , np . ndarray ): logger . info ( \"- Plot mean of station-wise delay phase across each gridcell.\" ) df_stats ( df_stats . grid_seasonal_phase , 'grid_seasonal_phase' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.1i ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot mean of station-wise delay amplitude across each gridcell if isinstance ( df_stats . grid_seasonal_amplitude , np . ndarray ): logger . info ( \"- Plot mean of station-wise delay amplitude across each gridcell.\" ) df_stats ( df_stats . grid_seasonal_amplitude , 'grid_seasonal_amplitude' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.3f ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot mean of station-wise delay period across each gridcell if isinstance ( df_stats . grid_seasonal_period , np . ndarray ): logger . info ( \"- Plot mean of station-wise delay period across each gridcell.\" ) df_stats ( df_stats . grid_seasonal_period , 'grid_seasonal_period' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.2f ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot mean stdev of station-wise delay phase across each gridcell if isinstance ( df_stats . grid_seasonal_phase_stdev , np . ndarray ): logger . info ( \"- Plot mean stdev of station-wise delay phase across each gridcell.\" ) df_stats ( df_stats . grid_seasonal_phase_stdev , 'grid_seasonal_phase_stdev' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.1i ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot mean stdev of station-wise delay amplitude across each gridcell if isinstance ( df_stats . grid_seasonal_amplitude_stdev , np . ndarray ): logger . info ( \"- Plot mean stdev of station-wise delay amplitude across each gridcell.\" ) df_stats ( df_stats . grid_seasonal_amplitude_stdev , 'grid_seasonal_amplitude_stdev' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.3f ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot mean stdev of station-wise delay period across each gridcell if isinstance ( df_stats . grid_seasonal_period_stdev , np . ndarray ): logger . info ( \"- Plot mean stdev of station-wise delay period across each gridcell.\" ) df_stats ( df_stats . grid_seasonal_period_stdev , 'grid_seasonal_period_stdev' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.2e ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot mean of seasonal fit RMSE across each gridcell if isinstance ( df_stats . grid_seasonal_fit_rmse , np . ndarray ): logger . info ( \"- Plot mean of seasonal fit RMSE across each gridcell.\" ) df_stats ( df_stats . grid_seasonal_fit_rmse , 'grid_seasonal_fit_rmse' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.3f ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot absolute mean delay for each gridcell if isinstance ( df_stats . grid_delay_absolute_mean , np . ndarray ): logger . info ( \"- Plot absolute mean delay per gridcell.\" ) df_stats ( df_stats . grid_delay_absolute_mean , 'grid_delay_absolute_mean' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.2f ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot absolute median delay for each gridcell if isinstance ( df_stats . grid_delay_absolute_median , np . ndarray ): logger . info ( \"- Plot absolute median delay per gridcell.\" ) df_stats ( df_stats . grid_delay_absolute_median , 'grid_delay_absolute_median' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.2f ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot absolute stdev delay for each gridcell if isinstance ( df_stats . grid_delay_absolute_stdev , np . ndarray ): logger . info ( \"- Plot absolute delay stdev per gridcell.\" ) df_stats ( df_stats . grid_delay_absolute_stdev , 'grid_delay_absolute_stdev' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.2f ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot absolute delay phase for each gridcell if isinstance ( df_stats . grid_seasonal_absolute_phase , np . ndarray ): logger . info ( \"- Plot absolute delay phase per gridcell.\" ) df_stats ( df_stats . grid_seasonal_absolute_phase , 'grid_seasonal_absolute_phase' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.1i ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot absolute delay amplitude for each gridcell if isinstance ( df_stats . grid_seasonal_absolute_amplitude , np . ndarray ): logger . info ( \"- Plot absolute delay amplitude per gridcell.\" ) df_stats ( df_stats . grid_seasonal_absolute_amplitude , 'grid_seasonal_absolute_amplitude' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.3f ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot absolute delay period for each gridcell if isinstance ( df_stats . grid_seasonal_absolute_period , np . ndarray ): logger . info ( \"- Plot absolute delay period per gridcell.\" ) df_stats ( df_stats . grid_seasonal_absolute_period , 'grid_seasonal_absolute_period' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.2f ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot absolute delay phase stdev for each gridcell if isinstance ( df_stats . grid_seasonal_absolute_phase_stdev , np . ndarray ): logger . info ( \"- Plot absolute delay phase stdev per gridcell.\" ) df_stats ( df_stats . grid_seasonal_absolute_phase_stdev , 'grid_seasonal_absolute_phase_stdev' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.1i ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot absolute delay amplitude stdev for each gridcell if isinstance ( df_stats . grid_seasonal_absolute_amplitude_stdev , np . ndarray ): logger . info ( \"- Plot absolute delay amplitude stdev per gridcell.\" ) df_stats ( df_stats . grid_seasonal_absolute_amplitude_stdev , 'grid_seasonal_absolute_amplitude_stdev' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.3f ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot absolute delay period stdev for each gridcell if isinstance ( df_stats . grid_seasonal_absolute_period_stdev , np . ndarray ): logger . info ( \"- Plot absolute delay period stdev per gridcell.\" ) df_stats ( df_stats . grid_seasonal_absolute_period_stdev , 'grid_seasonal_absolute_period_stdev' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.2e ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Plot absolute mean seasonal fit RMSE for each gridcell if isinstance ( df_stats . grid_seasonal_absolute_fit_rmse , np . ndarray ): logger . info ( \"- Plot absolute mean seasonal fit RMSE per gridcell.\" ) df_stats ( df_stats . grid_seasonal_absolute_fit_rmse , 'grid_seasonal_absolute_fit_rmse' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.2e ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) # Perform variogram analysis if variogramplot and not isinstance ( df_stats . grid_range , np . ndarray ) \\ and not isinstance ( df_stats . grid_variance , np . ndarray ) \\ and not isinstance ( df_stats . grid_variogram_rmse , np . ndarray ): logger . info ( \"***Variogram Analysis Function:***\" ) if unit in [ 'minute' , 'hour' , 'day' , 'year' ]: unit = 'm' df_stats . unit = 'm' logger . warning ( \"Output unit {} specified for Variogram analysis. Reverted to meters\" . format ( unit )) make_variograms = VariogramAnalysis ( df_stats . df , df_stats . gridpoints , col_name , unit , workdir , df_stats . seasonalinterval , densitythreshold , binnedvariogram , numCPUs , variogram_per_timeslice , variogram_errlimit ) TOT_grids , TOT_res_robust_arr , TOT_res_robust_rmse = make_variograms . create_variograms () # get range df_stats . grid_range = np . array ([ np . nan if i [ 0 ] not in TOT_grids else float ( TOT_res_robust_arr [ TOT_grids . index ( i [ 0 ])][ 0 ]) for i in enumerate ( df_stats . gridpoints )]) . reshape ( df_stats . grid_dim ) . T # convert range to specified output unit df_stats . grid_range = convert_SI ( df_stats . grid_range , 'm' , unit ) # get sill df_stats . grid_variance = np . array ([ np . nan if i [ 0 ] not in TOT_grids else float ( TOT_res_robust_arr [ TOT_grids . index ( i [ 0 ])][ 1 ]) for i in enumerate ( df_stats . gridpoints )]) . reshape ( df_stats . grid_dim ) . T # convert sill to specified output unit df_stats . grid_range = convert_SI ( df_stats . grid_range , 'm^2' , unit . split ( '^2' )[ 0 ] + '^2' ) # get variogram rmse df_stats . grid_variogram_rmse = np . array ([ np . nan if i [ 0 ] not in TOT_grids else float ( TOT_res_robust_rmse [ TOT_grids . index ( i [ 0 ])]) for i in enumerate ( df_stats . gridpoints )]) . reshape ( df_stats . grid_dim ) . T # convert range to specified output unit df_stats . grid_variogram_rmse = convert_SI ( df_stats . grid_variogram_rmse , 'm' , unit ) # If specified, save gridded array(s) if grid_to_raster : # write range gridfile_name = os . path . join ( workdir , col_name + '_' + 'grid_range' + '.tif' ) save_gridfile ( df_stats . grid_range , 'grid_range' , gridfile_name , df_stats . plotbbox , df_stats . spacing , df_stats . unit , colorbarfmt = ' %1i ' , stationsongrids = df_stats . stationsongrids , dtype = 'float32' ) # write sill gridfile_name = os . path . join ( workdir , col_name + '_' + 'grid_variance' + '.tif' ) save_gridfile ( df_stats . grid_variance , 'grid_variance' , gridfile_name , df_stats . plotbbox , df_stats . spacing , df_stats . unit + '^2' , colorbarfmt = ' %.3e ' , stationsongrids = df_stats . stationsongrids , dtype = 'float32' ) # write variogram rmse gridfile_name = os . path . join ( workdir , col_name + '_' + 'grid_variogram_rmse' + '.tif' ) save_gridfile ( df_stats . grid_variogram_rmse , 'grid_variogram_rmse' , gridfile_name , df_stats . plotbbox , df_stats . spacing , df_stats . unit , colorbarfmt = ' %.2e ' , stationsongrids = df_stats . stationsongrids , dtype = 'float32' ) if isinstance ( df_stats . grid_range , np . ndarray ): # plot range heatmap logger . info ( \"- Plot variogram range per gridcell.\" ) df_stats ( df_stats . grid_range , 'grid_range' , workdir = os . path . join ( workdir , 'figures' ), colorbarfmt = ' %1i ' , drawgridlines = drawgridlines , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) if isinstance ( df_stats . grid_variance , np . ndarray ): # plot sill heatmap logger . info ( \"- Plot variogram sill per gridcell.\" ) df_stats ( df_stats . grid_variance , 'grid_variance' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.3e ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title ) if isinstance ( df_stats . grid_variogram_rmse , np . ndarray ): # plot variogram rmse heatmap logger . info ( \"- Plot variogram RMSE per gridcell.\" ) df_stats ( df_stats . grid_variogram_rmse , 'grid_variogram_rmse' , workdir = os . path . join ( workdir , 'figures' ), drawgridlines = drawgridlines , colorbarfmt = ' %.2e ' , stationsongrids = stationsongrids , plotFormat = plot_fmt , userTitle = user_title )","title":"stats_analyses()"},{"location":"reference/#RAiDER.cli.validators","text":"","title":"validators"},{"location":"reference/#RAiDER.cli.validators.BBoxAction","text":"Bases: Action An Action that parses and stores a valid bounding box Source code in RAiDER/cli/validators.py 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 class BBoxAction ( Action ): \"\"\"An Action that parses and stores a valid bounding box\"\"\" def __init__ ( self , option_strings , dest , nargs = None , const = None , default = None , type = None , choices = None , required = False , help = None , metavar = None ): if nargs != 4 : raise ValueError ( \"nargs must be 4!\" ) super () . __init__ ( option_strings = option_strings , dest = dest , nargs = nargs , const = const , default = default , type = type , choices = choices , required = required , help = help , metavar = metavar ) def __call__ ( self , parser , namespace , values , option_string = None ): S , N , W , E = values if N <= S or E <= W : raise ArgumentError ( self , 'Bounding box has no size; make sure you use \"S N W E\"' ) for sn in ( S , N ): if sn < - 90 or sn > 90 : raise ArgumentError ( self , 'Lats are out of S/N bounds (-90 to 90).' ) for we in ( W , E ): if we < - 180 or we > 180 : raise ArgumentError ( self , 'Lons are out of W/E bounds (-180 to 180); Lons in the format of (0 to 360) are not supported.' ) setattr ( namespace , self . dest , values )","title":"BBoxAction"},{"location":"reference/#RAiDER.cli.validators.DateListAction","text":"Bases: Action An Action that parses and stores a list of dates Source code in RAiDER/cli/validators.py 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 class DateListAction ( Action ): \"\"\"An Action that parses and stores a list of dates\"\"\" def __init__ ( self , option_strings , dest , nargs = None , const = None , default = None , type = None , choices = None , required = False , help = None , metavar = None ): if type is not date_type : raise ValueError ( \"type must be `date_type`!\" ) super () . __init__ ( option_strings = option_strings , dest = dest , nargs = nargs , const = const , default = default , type = type , choices = choices , required = required , help = help , metavar = metavar ) def __call__ ( self , parser , namespace , values , option_string = None ): if len ( values ) > 3 or not values : raise ArgumentError ( self , \"Only 1, 2 dates, or 2 dates and interval may be supplied\" ) if len ( values ) == 2 : start , end = values values = [ start + timedelta ( days = k ) for k in range ( 0 , ( end - start ) . days + 1 , 1 )] elif len ( values ) == 3 : start , end , stepsize = values if not isinstance ( stepsize . day , int ): raise ArgumentError ( self , \"The stepsize should be in integer days\" ) new_year = date ( year = stepsize . year , month = 1 , day = 1 ) stepsize = ( stepsize - new_year ) . days + 1 values = [ start + timedelta ( days = k ) for k in range ( 0 , ( end - start ) . days + 1 , stepsize )] setattr ( namespace , self . dest , values )","title":"DateListAction"},{"location":"reference/#RAiDER.cli.validators.IntegerMappingType","text":"Bases: MappingType , IntegerType An integer type that converts non-integer types through a mapping.","title":"IntegerMappingType"},{"location":"reference/#RAiDER.cli.validators.IntegerMappingType--example","text":"integer = IntegerMappingType(0, 100, random=42) assert integer(\"0\") == 0 assert integer(\"100\") == 100 assert integer(\"random\") == 42 Source code in RAiDER/cli/validators.py 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 class IntegerMappingType ( MappingType , IntegerType ): \"\"\" An integer type that converts non-integer types through a mapping. # Example ``` integer = IntegerMappingType(0, 100, random=42) assert integer(\"0\") == 0 assert integer(\"100\") == 100 assert integer(\"random\") == 42 ``` \"\"\" def __init__ ( self , lo = None , hi = None , mapping = {}, ** kwargs ): IntegerType . __init__ ( self , lo , hi ) kwargs . update ( mapping ) MappingType . __init__ ( self , ** kwargs ) def __call__ ( self , arg ): try : return IntegerType . __call__ ( self , arg ) except ValueError : return MappingType . __call__ ( self , arg )","title":"Example"},{"location":"reference/#RAiDER.cli.validators.IntegerType","text":"Bases: object A type that converts arguments to integers.","title":"IntegerType"},{"location":"reference/#RAiDER.cli.validators.IntegerType--example","text":"integer = IntegerType(0, 100) assert integer(\"0\") == 0 assert integer(\"100\") == 100 integer(\"-10\") # Raises exception Source code in RAiDER/cli/validators.py 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 class IntegerType ( object ): \"\"\" A type that converts arguments to integers. # Example ``` integer = IntegerType(0, 100) assert integer(\"0\") == 0 assert integer(\"100\") == 100 integer(\"-10\") # Raises exception ``` \"\"\" def __init__ ( self , lo = None , hi = None ): self . lo = lo self . hi = hi def __call__ ( self , arg ): integer = int ( arg ) if self . lo is not None and integer < self . lo : raise ArgumentTypeError ( \"Must be greater than {} \" . format ( self . lo )) if self . hi is not None and integer > self . hi : raise ArgumentTypeError ( \"Must be less than {} \" . format ( self . hi )) return integer","title":"Example"},{"location":"reference/#RAiDER.cli.validators.MappingType","text":"Bases: object A type that maps arguments to constants.","title":"MappingType"},{"location":"reference/#RAiDER.cli.validators.MappingType--example","text":"mapping = MappingType(foo=42, bar=\"baz\").default(None) assert mapping(\"foo\") == 42 assert mapping(\"bar\") == \"baz\" assert mapping(\"hello\") is None Source code in RAiDER/cli/validators.py 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 class MappingType ( object ): \"\"\" A type that maps arguments to constants. # Example ``` mapping = MappingType(foo=42, bar=\"baz\").default(None) assert mapping(\"foo\") == 42 assert mapping(\"bar\") == \"baz\" assert mapping(\"hello\") is None ``` \"\"\" UNSET = object () def __init__ ( self , ** kwargs ): self . mapping = kwargs self . _default = self . UNSET def default ( self , default ): \"\"\"Set a default value if no mapping is found\"\"\" self . _default = default return self def __call__ ( self , arg ): if arg in self . mapping : return self . mapping [ arg ] if self . _default is self . UNSET : raise KeyError ( \"Invalid choice ' {} ', must be one of {} \" . format ( arg , list ( self . mapping . keys ()) ) ) return self . _default","title":"Example"},{"location":"reference/#RAiDER.cli.validators.MappingType.default","text":"Set a default value if no mapping is found Source code in RAiDER/cli/validators.py 408 409 410 411 def default ( self , default ): \"\"\"Set a default value if no mapping is found\"\"\" self . _default = default return self","title":"default()"},{"location":"reference/#RAiDER.cli.validators.date_type","text":"Parse a date from a string in pseudo-ISO 8601 format. Source code in RAiDER/cli/validators.py 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 def date_type ( arg ): \"\"\" Parse a date from a string in pseudo-ISO 8601 format. \"\"\" year_formats = ( '%Y-%m- %d ' , '%Y%m %d ' , ' %d ' , '%j' , ) for yf in year_formats : try : return date ( * strptime ( arg , yf )[ 0 : 3 ]) except ValueError : pass raise ArgumentTypeError ( 'Unable to coerce {} to a date. Try %Y-%m- %d ' . format ( arg ) )","title":"date_type()"},{"location":"reference/#RAiDER.cli.validators.enforce_bbox","text":"Enforce a valid bounding box Source code in RAiDER/cli/validators.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 def enforce_bbox ( bbox ): \"\"\" Enforce a valid bounding box \"\"\" if isinstance ( bbox , str ): bbox = [ float ( d ) for d in bbox . strip () . split ()] else : bbox = [ float ( d ) for d in bbox ] # Check the bbox if len ( bbox ) != 4 : raise ValueError ( \"bounding box must have 4 elements!\" ) S , N , W , E = bbox if N <= S or E <= W : raise ValueError ( 'Bounding box has no size; make sure you use \"S N W E\"' ) for sn in ( S , N ): if sn < - 90 or sn > 90 : raise ValueError ( 'Lats are out of S/N bounds (-90 to 90).' ) for we in ( W , E ): if we < - 180 or we > 180 : raise ValueError ( 'Lons are out of W/E bounds (-180 to 180); Lons in the format of (0 to 360) are not supported.' ) return bbox","title":"enforce_bbox()"},{"location":"reference/#RAiDER.cli.validators.enforce_time","text":"Parse an input time (required to be ISO 8601) Source code in RAiDER/cli/validators.py 243 244 245 246 247 248 249 250 251 252 253 254 def enforce_time ( arg_dict ): ''' Parse an input time (required to be ISO 8601) ''' try : arg_dict [ 'time' ] = convert_time ( arg_dict [ 'time' ]) except KeyError : raise ValueError ( 'You must specify a \"time\" in the input config file' ) if 'end_time' in arg_dict . keys (): arg_dict [ 'end_time' ] = convert_time ( arg_dict [ 'end_time' ]) return arg_dict","title":"enforce_time()"},{"location":"reference/#RAiDER.cli.validators.enforce_valid_dates","text":"Parse a date from a string in pseudo-ISO 8601 format. Source code in RAiDER/cli/validators.py 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 def enforce_valid_dates ( arg ): \"\"\" Parse a date from a string in pseudo-ISO 8601 format. \"\"\" year_formats = ( '%Y-%m- %d ' , '%Y%m %d ' , ' %d ' , '%j' , ) for yf in year_formats : try : return datetime . strptime ( str ( arg ), yf ) except ValueError : pass raise ValueError ( 'Unable to coerce {} to a date. Try %Y-%m- %d ' . format ( arg ) )","title":"enforce_valid_dates()"},{"location":"reference/#RAiDER.cli.validators.getBufferedExtent","text":"get the bounding box around a set of lats/lons Source code in RAiDER/cli/validators.py 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 def getBufferedExtent ( lats , lons = None , buf = 0. ): ''' get the bounding box around a set of lats/lons ''' if lons is None : lats , lons = lats [ ... , 0 ], lons [ ... , 1 ] try : if ( lats . size == 1 ) & ( lons . size == 1 ): out = [ lats - buf , lats + buf , lons - buf , lons + buf ] elif ( lats . size > 1 ) & ( lons . size > 1 ): out = [ np . nanmin ( lats ), np . nanmax ( lats ), np . nanmin ( lons ), np . nanmax ( lons )] elif lats . size == 1 : out = [ lats - buf , lats + buf , np . nanmin ( lons ), np . nanmax ( lons )] elif lons . size == 1 : out = [ np . nanmin ( lats ), np . nanmax ( lats ), lons - buf , lons + buf ] except AttributeError : if ( isinstance ( lats , tuple ) or isinstance ( lats , list )) and len ( lats ) == 2 : out = [ min ( lats ) - buf , max ( lats ) + buf , min ( lons ) - buf , max ( lons ) + buf ] except Exception as e : raise RuntimeError ( 'Not a valid lat/lon shape or variable' ) return np . array ( out )","title":"getBufferedExtent()"},{"location":"reference/#RAiDER.cli.validators.get_heights","text":"Parse the Height info and download a DEM if needed Source code in RAiDER/cli/validators.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def get_heights ( args , out , station_file , bounding_box = None ): ''' Parse the Height info and download a DEM if needed ''' dem_path = os . path . join ( out , 'geom' ) if not os . path . exists ( dem_path ): os . mkdir ( dem_path ) out = { 'dem' : None , 'height_file_rdr' : None , 'height_levels' : None , } if 'dem' in args . keys (): if ( station_file is not None ): if 'Hgt_m' not in pd . read_csv ( station_file ): out [ 'dem' ] = os . path . join ( dem_path , 'GLO30.dem' ) elif os . path . exists ( args . dem ): out [ 'dem' ] = args [ 'dem' ] if bounding_box is not None : dem_bounds = rio_extents ( rio_profile ( args . dem )) lats = dem_bounds [: 2 ] lons = dem_bounds [ 2 :] if isOutside ( bounding_box , getBufferedExtent ( lats , lons , buf = _BUFFER_SIZE , ) ): raise ValueError ( 'Existing DEM does not cover the area of the input lat/lon ' 'points; either move the DEM, delete it, or change the input ' 'points.' ) else : pass # will download the dem later elif 'height_file_rdr' in args . keys (): out [ 'height_file_rdr' ] = args . height_file_rdr elif 'height_levels' in args . keys (): l = re . findall ( '[0-9]+' , args . height_levels ) out [ 'height_levels' ] = [ float ( ll ) for ll in l ] else : # download the DEM if needed out [ 'dem' ] = os . path . join ( dem_path , 'GLO30.dem' ) return out","title":"get_heights()"},{"location":"reference/#RAiDER.cli.validators.get_query_region","text":"Parse the query region from inputs Source code in RAiDER/cli/validators.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def get_query_region ( args ): ''' Parse the query region from inputs ''' # Get bounds from the inputs # make sure this is first if args . get ( 'use_dem_latlon' ): query = GeocodedFile ( args . dem , is_dem = True ) elif args . get ( 'lat_file' ): hgt_file = args . get ( 'height_file_rdr' ) # only get it if exists dem_file = args . get ( 'dem' ) query = RasterRDR ( args . lat_file , args . lon_file , hgt_file , dem_file ) elif args . get ( 'station_file' ): query = StationFile ( args . station_file ) elif args . get ( 'bounding_box' ): bbox = enforce_bbox ( args . bounding_box ) if ( np . min ( bbox [ 0 ]) < - 90 ) | ( np . max ( bbox [ 1 ]) > 90 ): raise ValueError ( 'Lats are out of N/S bounds; are your lat/lon coordinates switched? Should be SNWE' ) query = BoundingBox ( bbox ) elif args . get ( 'geocoded_file' ): gfile = os . path . basename ( args . geocoded_file ) . upper () if ( gfile . startswith ( 'SRTM' ) or gfile . startswith ( 'GLO' )): logger . debug ( 'Using user DEM: %s ' , gfile ) is_dem = True else : is_dem = False query = GeocodedFile ( args . geocoded_file , is_dem = is_dem ) ## untested elif 'los_cube' in args . keys (): query = Geocube ( args . los_cube ) else : # TODO: Need to incorporate the cube raise ValueError ( 'No valid query points or bounding box found in the configuration file' ) return query","title":"get_query_region()"},{"location":"reference/#RAiDER.cli.validators.isInside","text":"Determine whether all of extent1 lies inside extent2 extent1/2 should be a list containing [lower_lat, upper_lat, left_lon, right_lon]. Equal extents are considered \"inside\" Source code in RAiDER/cli/validators.py 352 353 354 355 356 357 358 359 360 361 362 363 364 def isInside ( extent1 , extent2 ): ''' Determine whether all of extent1 lies inside extent2 extent1/2 should be a list containing [lower_lat, upper_lat, left_lon, right_lon]. Equal extents are considered \"inside\" ''' t1 = extent1 [ 0 ] <= extent2 [ 0 ] t2 = extent1 [ 1 ] >= extent2 [ 1 ] t3 = extent1 [ 2 ] <= extent2 [ 2 ] t4 = extent1 [ 3 ] >= extent2 [ 3 ] if np . all ([ t1 , t2 , t3 , t4 ]): return True return False","title":"isInside()"},{"location":"reference/#RAiDER.cli.validators.isOutside","text":"Determine whether any of extent1 lies outside extent2 extent1/2 should be a list containing [lower_lat, upper_lat, left_lon, right_lon] Equal extents are considered \"inside\" Source code in RAiDER/cli/validators.py 337 338 339 340 341 342 343 344 345 346 347 348 349 def isOutside ( extent1 , extent2 ): ''' Determine whether any of extent1 lies outside extent2 extent1/2 should be a list containing [lower_lat, upper_lat, left_lon, right_lon] Equal extents are considered \"inside\" ''' t1 = extent1 [ 0 ] < extent2 [ 0 ] t2 = extent1 [ 1 ] > extent2 [ 1 ] t3 = extent1 [ 2 ] < extent2 [ 2 ] t4 = extent1 [ 3 ] > extent2 [ 3 ] if np . any ([ t1 , t2 , t3 , t4 ]): return True return False","title":"isOutside()"},{"location":"reference/#RAiDER.cli.validators.modelName2Module","text":"Turn an arbitrary string into a module name. Takes as input a model name, which hopefully looks like ERA-I, and converts it to a module name, which will look like erai. I doesn't always produce a valid module name, but that's not the goal. The goal is just to handle common cases. Inputs model_name - Name of an allowed weather model (e.g., 'era-5') Outputs module_name - Name of the module wmObject - callable, weather model object Source code in RAiDER/cli/validators.py 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 def modelName2Module ( model_name ): \"\"\"Turn an arbitrary string into a module name. Takes as input a model name, which hopefully looks like ERA-I, and converts it to a module name, which will look like erai. I doesn't always produce a valid module name, but that's not the goal. The goal is just to handle common cases. Inputs: model_name - Name of an allowed weather model (e.g., 'era-5') Outputs: module_name - Name of the module wmObject - callable, weather model object \"\"\" module_name = 'RAiDER.models.' + model_name . lower () . replace ( '-' , '' ) model_module = importlib . import_module ( module_name ) wmObject = getattr ( model_module , model_name . upper () . replace ( '-' , '' )) return module_name , wmObject","title":"modelName2Module()"},{"location":"reference/#RAiDER.cli.validators.parse_dates","text":"Determine the requested dates from the input parameters Source code in RAiDER/cli/validators.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 def parse_dates ( arg_dict ): ''' Determine the requested dates from the input parameters ''' if 'date_list' in arg_dict . keys (): l = arg_dict [ 'date_list' ] if isinstance ( l , str ): l = re . findall ( '[0-9]+' , l ) L = [ enforce_valid_dates ( d ) for d in l ] else : try : start = arg_dict [ 'date_start' ] except KeyError : raise ValueError ( 'Inputs must include either date_list or date_start' ) start = enforce_valid_dates ( start ) if 'date_end' in arg_dict . keys (): end = arg_dict [ 'date_end' ] end = enforce_valid_dates ( end ) else : end = start if 'date_step' in arg_dict . keys (): step = int ( arg_dict [ 'date_step' ]) else : step = 1 L = [ start + timedelta ( days = step ) for step in range ( 0 , ( end - start ) . days + 1 , step )] return L","title":"parse_dates()"},{"location":"reference/#RAiDER.delay","text":"RAiDER tropospheric delay calculation This module provides the main RAiDER functionality for calculating tropospheric wet and hydrostatic delays from a weather model. Weather models are accessed as NETCDF files and should have \"wet\" \"hydro\" \"wet_total\" and \"hydro_total\" fields specified.","title":"delay"},{"location":"reference/#RAiDER.delay.transformPoints","text":"Transform lat/lon/hgt data to an array of points in a new projection Parameters: Name Type Description Default lats np . ndarray ndarray - WGS-84 latitude (EPSG: 4326) required lons np . ndarray ndarray - ditto for longitude required hgts np . ndarray ndarray - Ellipsoidal height in meters required old_proj CRS CRS - the original projection of the points required new_proj CRS CRS - the new projection in which to return the points required Returns: Name Type Description ndarray np . ndarray the array of query points in the weather model coordinate system (YX) Source code in RAiDER/delay.py 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 def transformPoints ( lats : np . ndarray , lons : np . ndarray , hgts : np . ndarray , old_proj : CRS , new_proj : CRS ) -> np . ndarray : ''' Transform lat/lon/hgt data to an array of points in a new projection Args: lats: ndarray - WGS-84 latitude (EPSG: 4326) lons: ndarray - ditto for longitude hgts: ndarray - Ellipsoidal height in meters old_proj: CRS - the original projection of the points new_proj: CRS - the new projection in which to return the points Returns: ndarray: the array of query points in the weather model coordinate system (YX) ''' t = Transformer . from_crs ( old_proj , new_proj ) # Flags for flipping inputs or outputs if not isinstance ( new_proj , pyproj . CRS ): new_proj = CRS . from_epsg ( new_proj . lstrip ( 'EPSG:' )) if not isinstance ( old_proj , pyproj . CRS ): old_proj = CRS . from_epsg ( old_proj . lstrip ( 'EPSG:' )) in_flip = old_proj . axis_info [ 0 ] . direction out_flip = new_proj . axis_info [ 0 ] . direction if in_flip == 'east' : res = t . transform ( lons , lats , hgts ) else : res = t . transform ( lats , lons , hgts ) if out_flip == 'east' : return np . stack (( res [ 1 ], res [ 0 ], res [ 2 ]), axis =- 1 ) . T else : return np . stack ( res , axis =- 1 ) . T","title":"transformPoints()"},{"location":"reference/#RAiDER.delay.tropo_delay","text":"Calculate integrated delays on query points. Parameters: Name Type Description Default dt Datetime - Datetime object for determining when to calculate delays required weather_model_File string - Name of the NETCDF file containing a pre-processed weather model required aoi AOI object - AOI object required los LOS object - LOS object required height_levels List [ float ] list - (optional) list of height levels on which to calculate delays. Only needed for cube generation. None out_proj int | str int,str - (optional) EPSG code for output projection 4326 look_dir str str - (optional) Satellite look direction. Only needed for slant delay calculation 'right' cube_spacing_m int int - (optional) Horizontal spacing in meters when generating cubes None Returns: Type Description xarray Dataset or ndarrays: - wet and hydrostatic delays at the grid nodes / query points. Source code in RAiDER/delay.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def tropo_delay ( dt , weather_model_file : str , aoi , los , height_levels : List [ float ] = None , out_proj : int | str = 4326 , cube_spacing_m : int = None , look_dir : str = 'right' , ): \"\"\" Calculate integrated delays on query points. Args: dt: Datetime - Datetime object for determining when to calculate delays weather_model_File: string - Name of the NETCDF file containing a pre-processed weather model aoi: AOI object - AOI object los: LOS object - LOS object height_levels: list - (optional) list of height levels on which to calculate delays. Only needed for cube generation. out_proj: int,str - (optional) EPSG code for output projection look_dir: str - (optional) Satellite look direction. Only needed for slant delay calculation cube_spacing_m: int - (optional) Horizontal spacing in meters when generating cubes Returns: xarray Dataset *or* ndarrays: - wet and hydrostatic delays at the grid nodes / query points. \"\"\" # get heights if height_levels is None : with xarray . load_dataset ( weather_model_file ) as ds : height_levels = ds . z . values #TODO: expose this as library function ds = _get_delays_on_cube ( dt , weather_model_file , aoi . bounds (), height_levels , los , out_proj = out_proj , cube_spacing_m = cube_spacing_m , look_dir = look_dir ) if ( aoi . type () == 'bounding_box' ) or ( aoi . type () == 'Geocube' ): return ds , None else : # CRS can be an int, str, or CRS object try : out_proj = CRS . from_epsg ( out_proj ) except pyproj . exceptions . CRSError : out_proj = out_proj pnt_proj = CRS . from_epsg ( 4326 ) lats , lons = aoi . readLL () hgts = aoi . readZ () pnts = transformPoints ( lats , lons , hgts , pnt_proj , out_proj ) if pnts . ndim == 3 : pnts = pnts . transpose ( 1 , 2 , 0 ) elif pnts . ndim == 2 : pnts = pnts . T ifWet , ifHydro = getInterpolators ( ds , 'ztd' ) # the cube from get_delays_on_cube calls the total delays 'wet' and 'hydro' wetDelay = ifWet ( pnts ) hydroDelay = ifHydro ( pnts ) # return the delays (ZTD or STD) if los . is_Projected (): los . setTime ( dt ) los . setPoints ( lats , lons , hgts ) wetDelay = los ( wetDelay ) hydroDelay = los ( hydroDelay ) return wetDelay , hydroDelay","title":"tropo_delay()"},{"location":"reference/#RAiDER.delayFcns","text":"","title":"delayFcns"},{"location":"reference/#RAiDER.delayFcns.calculate_start_points","text":"wm_file: str - A file containing a regularized weather model. ndarray - a * x 3 array containing the XYZ locations of the pixels in ECEF coordinates. Note the ordering of the array is [Y X Z] Source code in RAiDER/delayFcns.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def calculate_start_points ( x , y , z , ds ): ''' Args: ---------- wm_file: str - A file containing a regularized weather model. Returns: ------- SP: ndarray - a * x 3 array containing the XYZ locations of the pixels in ECEF coordinates. Note the ordering of the array is [Y X Z] ''' [ X , Y , Z ] = np . meshgrid ( x , y , z ) try : t = Transformer . from_crs ( ds [ 'CRS' ], 4978 , always_xy = True ) # converts to WGS84 geocentric except : print ( \"I can't find a CRS in the weather model file, so I will assume you are using WGS84\" ) t = Transformer . from_crs ( 4326 , 4978 , always_xy = True ) # converts to WGS84 geocentric return np . moveaxis ( np . array ( t . transform ( X , Y , Z )), 0 , - 1 ), np . stack ([ X , Y , Z ], axis =- 1 )","title":"calculate_start_points()"},{"location":"reference/#RAiDER.delayFcns.chunk","text":"Create a set of indices to use as chunks Source code in RAiDER/delayFcns.py 138 139 140 141 142 143 144 def chunk ( chunkSize , in_shape ): ''' Create a set of indices to use as chunks ''' startInds = makeChunkStartInds ( chunkSize , in_shape ) chunkInds = makeChunksFromInds ( startInds , chunkSize , in_shape ) return chunkInds","title":"chunk()"},{"location":"reference/#RAiDER.delayFcns.getInterpolators","text":"Read 3D gridded data from a processed weather model file and wrap it with an interpolator Source code in RAiDER/delayFcns.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def getInterpolators ( wm_file , kind = 'pointwise' , shared = False ): ''' Read 3D gridded data from a processed weather model file and wrap it with an interpolator ''' # Get the weather model data try : ds = xarray . load_dataset ( wm_file ) except : ds = wm_file xs_wm = np . array ( ds . variables [ 'x' ][:]) ys_wm = np . array ( ds . variables [ 'y' ][:]) zs_wm = np . array ( ds . variables [ 'z' ][:]) wet = ds . variables [ 'wet_total' if kind == 'total' else 'wet' ][:] hydro = ds . variables [ 'hydro_total' if kind == 'total' else 'hydro' ][:] wet = np . array ( wet ) . transpose ( 1 , 2 , 0 ) hydro = np . array ( hydro ) . transpose ( 1 , 2 , 0 ) # If shared interpolators are requested # The arrays are not modified - so turning off lock for performance if shared : xs_wm = make_shared_raw ( xs_wm ) ys_wm = make_shared_raw ( ys_wm ) zs_wm = make_shared_raw ( zs_wm ) wet = make_shared_raw ( wet ) hydro = make_shared_raw ( hydro ) ifWet = Interpolator (( ys_wm , xs_wm , zs_wm ), wet , fill_value = np . nan , bounds_error = False ) ifHydro = Interpolator (( ys_wm , xs_wm , zs_wm ), hydro , fill_value = np . nan , bounds_error = False ) return ifWet , ifHydro","title":"getInterpolators()"},{"location":"reference/#RAiDER.delayFcns.get_delays","text":"Create the integration points for each ray path. Source code in RAiDER/delayFcns.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def get_delays ( stepSize , SP , LOS , wm_file , cpu_num = 0 ): ''' Create the integration points for each ray path. ''' ifWet , ifHydro = getInterpolators ( wm_file ) with xarray . load_dataset ( wm_file ) as f : try : wm_proj = f . attrs [ 'CRS' ] except : wm_proj = 4326 print ( \"I can't find a CRS in the weather model file, so I will assume you are using WGS84\" ) t = Transformer . from_crs ( 4326 , 4978 , always_xy = True ) # converts to WGS84 geocentric in_shape = SP . shape [: - 1 ] chunkSize = in_shape CHUNKS = chunk ( in_shape , in_shape ) Nchunks = len ( CHUNKS ) max_len = 15000 stepSize = 100 chunk_inputs = [( kk , CHUNKS [ kk ], wm_proj , SP , LOS , chunkSize , stepSize , ifWet , ifHydro , max_len , wm_file ) for kk in range ( Nchunks )] if Nchunks == 1 : delays = process_chunk ( * chunk_inputs [ 0 ]) else : with mp . Pool () as pool : individual_results = pool . starmap ( process_chunk , chunk_inputs ) try : delays = np . concatenate ( individual_results ) except ValueError : delays = np . concatenate ( individual_results , axis =- 1 ) wet_delay = delays [ 0 , ... ] . reshape ( in_shape ) hydro_delay = delays [ 1 , ... ] . reshape ( in_shape ) return wet_delay , hydro_delay","title":"get_delays()"},{"location":"reference/#RAiDER.delayFcns.interpolate2","text":"helper function to make the interpolation step cleaner Source code in RAiDER/delayFcns.py 256 257 258 259 260 261 262 263 def interpolate2 ( fun , x , y , z ): ''' helper function to make the interpolation step cleaner ''' in_shape = x . shape out = fun (( y . ravel (), x . ravel (), z . ravel ())) # note that this re-ordering is on purpose to match the weather model outData = out . reshape ( in_shape ) return outData","title":"interpolate2()"},{"location":"reference/#RAiDER.delayFcns.makeChunkStartInds","text":"Create a list of start indices for chunking a numpy D-dimensional array. Inputs chunkSize - length-D tuple containing chunk sizes in_shape - length-D tuple containing the shape of the array to be chunked Outputs chunkInds - a list of length-D tuples, where each tuple is the starting multi-index of each chunk Example makeChunkStartInds((2,2,16), (4,8,16)) [(0, 0, 0), (0, 2, 0), (0, 4, 0), (0, 6, 0), (2, 0, 0), (2, 2, 0), (2, 4, 0), (2, 6, 0)] Source code in RAiDER/delayFcns.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def makeChunkStartInds ( chunkSize , in_shape ): ''' Create a list of start indices for chunking a numpy D-dimensional array. Inputs: chunkSize - length-D tuple containing chunk sizes in_shape - length-D tuple containing the shape of the array to be chunked Outputs chunkInds - a list of length-D tuples, where each tuple is the starting multi-index of each chunk Example: makeChunkStartInds((2,2,16), (4,8,16)) Output: [(0, 0, 0), (0, 2, 0), (0, 4, 0), (0, 6, 0), (2, 0, 0), (2, 2, 0), (2, 4, 0), (2, 6, 0)] ''' if len ( in_shape ) == 1 : chunkInds = [( i ,) for i in range ( 0 , in_shape [ 0 ], chunkSize [ 0 ])] elif len ( in_shape ) == 2 : chunkInds = [( i , j ) for i , j in itertools . product ( range ( 0 , in_shape [ 0 ], chunkSize [ 0 ]), range ( 0 , in_shape [ 1 ], chunkSize [ 1 ]))] elif len ( in_shape ) == 3 : chunkInds = [( i , j , k ) for i , j , k in itertools . product ( range ( 0 , in_shape [ 0 ], chunkSize [ 0 ]), range ( 0 , in_shape [ 1 ], chunkSize [ 1 ]), range ( 0 , in_shape [ 2 ], chunkSize [ 2 ]))] else : raise NotImplementedError ( 'makeChunkStartInds: ndim > 3 not supported' ) return chunkInds","title":"makeChunkStartInds()"},{"location":"reference/#RAiDER.delayFcns.makeChunksFromInds","text":"From a length-N list of tuples containing starting indices, create a list of indices into chunks of a numpy D-dimensional array. Inputs startInd - A length-N list of D-dimensional tuples containing the starting indices of a set of chunks chunkSize - A D-dimensional tuple containing chunk size in each dimension in_shape - A D-dimensional tuple containing the size of each dimension Outputs chunks - A length-N list of length-D lists, where each element of the length-D list is a numpy array of indices Example makeChunksFromInds([(0, 0), (0, 2), (2, 0), (2, 2)],(4,4),(2,2)) Output [[np.array([0, 0, 1, 1]), np.array([0, 1, 0, 1])], [np.array([0, 0, 1, 1]), np.array([2, 3, 2, 3])], [np.array([2, 2, 3, 3]), np.array([0, 1, 0, 1])], [np.array([2, 2, 3, 3]), np.array([2, 3, 2, 3])]] Source code in RAiDER/delayFcns.py 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 def makeChunksFromInds ( startInd , chunkSize , in_shape ): ''' From a length-N list of tuples containing starting indices, create a list of indices into chunks of a numpy D-dimensional array. Inputs: startInd - A length-N list of D-dimensional tuples containing the starting indices of a set of chunks chunkSize - A D-dimensional tuple containing chunk size in each dimension in_shape - A D-dimensional tuple containing the size of each dimension Outputs: chunks - A length-N list of length-D lists, where each element of the length-D list is a numpy array of indices Example: makeChunksFromInds([(0, 0), (0, 2), (2, 0), (2, 2)],(4,4),(2,2)) Output: [[np.array([0, 0, 1, 1]), np.array([0, 1, 0, 1])], [np.array([0, 0, 1, 1]), np.array([2, 3, 2, 3])], [np.array([2, 2, 3, 3]), np.array([0, 1, 0, 1])], [np.array([2, 2, 3, 3]), np.array([2, 3, 2, 3])]] ''' indices = [] for ci in startInd : index = [] for si , k , dim in zip ( ci , chunkSize , range ( len ( chunkSize ))): if si + k > in_shape [ dim ]: dend = in_shape [ dim ] else : dend = si + k index . append ( np . array ( range ( si , dend ))) indices . append ( index ) # Now create the index mesh (for Ndim > 1) chunks = [] if len ( in_shape ) > 1 : for index in indices : chunks . append ([ np . array ( g ) for g in zip ( * list ( itertools . product ( * index )))]) else : chunks = indices return chunks","title":"makeChunksFromInds()"},{"location":"reference/#RAiDER.delayFcns.make_shared_raw","text":"Make numpy view array of mp.Array Source code in RAiDER/delayFcns.py 123 124 125 126 127 128 129 130 131 132 133 134 135 def make_shared_raw ( inarr ): \"\"\" Make numpy view array of mp.Array \"\"\" # Create flat shared array shared_arr = mp . RawArray ( 'd' , inarr . size ) # Create a numpy view of it shared_arr_np = np . ndarray ( inarr . shape , dtype = np . float64 , buffer = shared_arr ) # Copy data to shared array np . copyto ( shared_arr_np , inarr ) return shared_arr_np","title":"make_shared_raw()"},{"location":"reference/#RAiDER.delayFcns.process_chunk","text":"Perform the interpolation and integration over a single chunk. Source code in RAiDER/delayFcns.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 def process_chunk ( k , chunkInds , proj_wm , SP , SLV , chunkSize , stepSize , ifWet , ifHydro , max_len , wm_file ): \"\"\" Perform the interpolation and integration over a single chunk. \"\"\" # Transformer from ECEF to weather model t = Transformer . from_proj ( 4978 , proj_wm , always_xy = True ) # datatype must be specific for the cython makePoints* function _DTYPE = np . float64 # H5PY does not support fancy indexing with tuples, hence this if/else check if len ( chunkSize ) == 1 : row = chunkInds [ 0 ] ray = makePoints1D ( max_len , SP [ row , :] . astype ( _DTYPE ), SLV [ row , :] . astype ( _DTYPE ), stepSize ) elif len ( chunkSize ) == 2 : row , col = chunkInds ray = makePoints1D ( max_len , SP [ row , col , :] . astype ( _DTYPE ), SLV [ row , col , :] . astype ( _DTYPE ), stepSize ) elif len ( chunkSize ) == 3 : row , col , zind = chunkInds ray = makePoints1D ( max_len , SP [ row , col , zind , :] . astype ( _DTYPE ), SLV [ row , col , zind , :] . astype ( _DTYPE ), stepSize ) else : raise RuntimeError ( 'Data in more than 4 dimensions is not supported' ) ray_x , ray_y , ray_z = t . transform ( ray [ ... , 0 , :], ray [ ... , 1 , :], ray [ ... , 2 , :]) delay_wet = interpolate2 ( ifWet , ray_x , ray_y , ray_z ) delay_hydro = interpolate2 ( ifHydro , ray_x , ray_y , ray_z ) int_delays = _integrateLOS ( stepSize , delay_wet , delay_hydro ) return int_delays","title":"process_chunk()"},{"location":"reference/#RAiDER.dem","text":"","title":"dem"},{"location":"reference/#RAiDER.dem.download_dem","text":"Download a DEM if one is not already present. Source code in RAiDER/dem.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def download_dem ( ll_bounds , writeDEM = False , outName = 'warpedDEM' , buf = 0.02 , overwrite = False , ): \"\"\" Download a DEM if one is not already present. \"\"\" if os . path . exists ( outName ) and not overwrite : logger . info ( 'Using existing DEM: %s ' , outName ) zvals , metadata = rio_open ( outName , returnProj = True ) else : # inExtent is SNWE # dem-stitcher wants WSEN bounds = [ np . floor ( ll_bounds [ 2 ]) - buf , np . floor ( ll_bounds [ 0 ]) - buf , np . ceil ( ll_bounds [ 3 ]) + buf , np . ceil ( ll_bounds [ 1 ]) + buf ] zvals , metadata = stitch_dem ( bounds , dem_name = 'glo_30' , dst_ellipsoidal_height = True , dst_area_or_point = 'Area' , ) if writeDEM : with rasterio . open ( outName , 'w' , ** metadata ) as ds : ds . write ( zvals , 1 ) ds . update_tags ( AREA_OR_POINT = 'Point' ) logger . info ( 'Wrote DEM: %s ' , outName ) return zvals , metadata","title":"download_dem()"},{"location":"reference/#RAiDER.dem.getHeights","text":"Fcn to return heights from a DEM, either one that already exists or will download one if needed. Source code in RAiDER/dem.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def getHeights ( ll_bounds , dem_type , dem_file , lats = None , lons = None ): ''' Fcn to return heights from a DEM, either one that already exists or will download one if needed. ''' # height_type, height_data = heights if dem_type == 'hgt' : htinfo = get_file_and_band ( dem_file ) hts = rio_open ( htinfo [ 0 ], band = htinfo [ 1 ]) elif dem_type == 'csv' : # Heights are in the .csv file hts = pd . read_csv ( dem_file )[ 'Hgt_m' ] . values elif dem_type == 'interpolate' : # heights will be vertically interpolated to the heightlvs hts = None elif ( dem_type == 'download' ) or ( dem_type == 'dem' ): if ~ os . path . exists ( dem_file ): download_dem ( ll_bounds , writeDEM = True , outName = dem_file ) #TODO: interpolate heights to query lats/lons # Interpolate to the query points hts = interpolateDEM ( dem_file , lats , lons , ) return hts","title":"getHeights()"},{"location":"reference/#RAiDER.getStationDelays","text":"","title":"getStationDelays"},{"location":"reference/#RAiDER.getStationDelays.get_date","text":"extract the date from a station delay file Source code in RAiDER/getStationDelays.py 232 233 234 235 236 237 238 239 240 241 242 def get_date ( stationFile ): ''' extract the date from a station delay file ''' # find the date info year = int ( stationFile [ 1 ]) doy = int ( stationFile [ 2 ]) date = dt . datetime ( year , 1 , 1 ) + dt . timedelta ( doy - 1 ) return date , year , doy","title":"get_date()"},{"location":"reference/#RAiDER.getStationDelays.get_delays_UNR","text":"Parses and returns a dictionary containing either (1) all the GPS delays, if returnTime is None, or (2) only the delay at the closest times to to returnTime. Inputs stationFile - a .gz station delay file returnTime - specified time of GPS delay Outputs a dict and CSV file containing the times and delay information (delay in mm, delay uncertainty, delay gradients) *NOTE: Due to a formatting error in the tropo SINEX files, the two tropospheric gradient columns (TGNTOT and TGETOT) are interchanged, as are the formal error columns (_SIG). Source \u2014> http://geodesy.unr.edu/gps_timeseries/README_trop2.txt) Source code in RAiDER/getStationDelays.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def get_delays_UNR ( stationFile , filename , dateList , returnTime = None ): ''' Parses and returns a dictionary containing either (1) all the GPS delays, if returnTime is None, or (2) only the delay at the closest times to to returnTime. Inputs: stationFile - a .gz station delay file returnTime - specified time of GPS delay Outputs: a dict and CSV file containing the times and delay information (delay in mm, delay uncertainty, delay gradients) *NOTE: Due to a formatting error in the tropo SINEX files, the two tropospheric gradient columns (TGNTOT and TGETOT) are interchanged, as are the formal error columns (_SIG). Source \u2014> http://geodesy.unr.edu/gps_timeseries/README_trop2.txt) ''' # Refer to the following sites to interpret stationFile variable names: # ftp://igs.org/pub/data/format/sinex_tropo.txt # http://geodesy.unr.edu/gps_timeseries/README_trop2.txt # Wet and hydrostratic delays were derived as so: # Constants \u2014> k1 = 0.704, k2 = 0.776, k3 = 3739.0, m = 18.0152/28.9644, # k2' = k2-(k1*m) = 0.33812796398337275, Rv = 461.5 J/(kg\u00b7K), \u03c1l = 997 kg/m^3 # Note wet delays passed here may be computed as so # where PMV = precipitable water vapor, P = total atm pressure, Tm = mean temp of the column \u2014> # Wet zenith delay = 10^-6 \u03c1lRv(k2' + k3/Tm) PMV # Hydrostatic zenith delay = Total zenith delay - wet zenith delay = k1*(P/Tm) # Source \u2014> Hanssen, R. F. (2001) eqns. 6.2.7-10 # sort through station zip files allstationTarfiles = [] # if URL if stationFile . startswith ( 'http' ): r = requests . get ( stationFile ) ziprepo = zipfile . ZipFile ( io . BytesIO ( r . content )) # if downloaded file else : ziprepo = zipfile . ZipFile ( stationFile ) # iterate through tarfiles stationTarlist = sorted ( ziprepo . namelist ()) final_stationTarlist = [] for j in stationTarlist : # get the date of the file time , yearFromFile , doyFromFile = get_date ( os . path . basename ( j ) . split ( '.' )) # check if in list of specified input dates if time . strftime ( '%Y-%m- %d ' ) not in dateList : continue final_stationTarlist . append ( j ) f = gzip . open ( ziprepo . open ( j ), 'rb' ) # initialize variables d , Sig , dwet , dhydro , timesList = [], [], [], [], [] flag = False for line in f . readlines (): try : line = line . decode ( 'utf-8' ) except UnicodeDecodeError : line = line . decode ( 'latin-1' ) if flag : # Do not attempt to read header if 'SITE' in line : continue # Attempt to read data try : split_lines = line . split () # units: mm, mm, mm, deg, deg, deg, deg, mm, mm, K trotot , trototSD , trwet , tgetot , tgetotSD , tgntot , tgntotSD , wvapor , wvaporSD , mtemp = \\ [ float ( t ) for t in split_lines [ 2 :]] except BaseException : # TODO: What error(s)? continue site = split_lines [ 0 ] year , doy , seconds = [ int ( n ) for n in split_lines [ 1 ] . split ( ':' )] # Break iteration if time from line in file does not match date reported in filename if doy != doyFromFile : logger . warning ( 'time %s from line in conflict with time %s from file ' ' %s , will continue reading next tarfile(s)' , doy , doyFromFile , j ) continue # convert units from mm to m d . append ( trotot * 0.001 ) Sig . append ( trototSD * 0.001 ) dwet . append ( trwet * 0.001 ) dhydro . append (( trotot - trwet ) * 0.001 ) timesList . append ( seconds ) if 'TROP/SOLUTION' in line : flag = True del f # Break iteration if file contains no data. if d == []: logger . warning ( 'file %s for station %s is empty, will continue reading next ' 'tarfile(s)' , j , j . split ( '.' )[ 0 ] ) continue # check for missing times true_times = list ( range ( 0 , 86400 , 300 )) if len ( timesList ) != len ( true_times ): missing = [ True if t not in timesList else False for t in true_times ] mask = np . array ( missing ) delay , sig , wet_delay , hydro_delay = [ np . full (( 288 ,), np . nan )] * 4 delay [ ~ mask ] = d sig [ ~ mask ] = Sig wet_delay [ ~ mask ] = dwet hydro_delay [ ~ mask ] = dhydro times = true_times . copy () else : delay = np . array ( d ) times = np . array ( timesList ) sig = np . array ( Sig ) wet_delay = np . array ( dwet ) hydro_delay = np . array ( dhydro ) # if time not specified, pass all times if returnTime is None : filtoutput = { 'ID' : [ site ] * len ( wet_delay ), 'Date' : [ time ] * len ( wet_delay ), 'ZTD' : delay , 'wet_delay' : wet_delay , 'hydrostatic_delay' : hydro_delay , 'times' : times , 'sigZTD' : sig } filtoutput = [{ key : value [ k ] for key , value in filtoutput . items ()} for k in range ( len ( filtoutput [ 'ID' ]))] else : index = np . argmin ( np . abs ( np . array ( timesList ) - returnTime )) filtoutput = [{ 'ID' : site , 'Date' : time , 'ZTD' : delay [ index ], 'wet_delay' : wet_delay [ index ], 'hydrostatic_delay' : hydro_delay [ index ], 'times' : times [ index ], 'sigZTD' : sig [ index ]}] # setup pandas array and write output to CSV, making sure to update existing CSV. filtoutput = pd . DataFrame ( filtoutput ) if os . path . exists ( filename ): filtoutput . to_csv ( filename , index = False , mode = 'a' , header = False ) else : filtoutput . to_csv ( filename , index = False ) # record all used tar files allstationTarfiles . extend ([ os . path . join ( stationFile , k ) for k in stationTarlist ]) allstationTarfiles . sort () del ziprepo return","title":"get_delays_UNR()"},{"location":"reference/#RAiDER.getStationDelays.get_station_data","text":"Pull tropospheric delay data for a given station name Source code in RAiDER/getStationDelays.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def get_station_data ( inFile , dateList , gps_repo = None , numCPUs = 8 , outDir = None , returnTime = None ): ''' Pull tropospheric delay data for a given station name ''' if outDir is None : outDir = os . getcwd () pathbase = os . path . join ( outDir , 'GPS_delays' ) if not os . path . exists ( pathbase ): os . mkdir ( pathbase ) returnTime = seconds_of_day ( returnTime ) # print warning if not divisible by 3 seconds if returnTime % 3 != 0 : index = np . argmin ( np . abs ( np . array ( list ( range ( 0 , 86400 , 300 ))) - returnTime )) updatedreturnTime = str ( dt . timedelta ( seconds = list ( range ( 0 , 86400 , 300 ))[ index ])) logger . warning ( 'input time %s not divisible by 3 seconds, so next closest time %s ' 'will be chosen' , returnTime , updatedreturnTime ) returnTime = updatedreturnTime # get list of station zip files inFile_df = pd . read_csv ( inFile ) stationFiles = inFile_df [ 'path' ] . to_list () del inFile_df if len ( stationFiles ) > 0 : outputfiles = [] args = [] # parse delays from UNR if gps_repo == 'UNR' : for sf in stationFiles : StationID = os . path . basename ( sf ) . split ( '.' )[ 0 ] name = os . path . join ( pathbase , StationID + '_ztd.csv' ) args . append (( sf , name , dateList , returnTime )) outputfiles . append ( name ) # Parallelize remote querying of zenith delays with multiprocessing . Pool ( numCPUs ) as multipool : multipool . starmap ( get_delays_UNR , args ) # confirm file exists (i.e. valid delays exists for specified time/region). outputfiles = [ i for i in outputfiles if os . path . exists ( i )] # Consolidate all CSV files into one object if outputfiles == []: raise Exception ( 'No valid delays found for specified time/region.' ) name = os . path . join ( outDir , ' {} combinedGPS_ztd.csv' . format ( gps_repo )) statsFile = pd . concat ([ pd . read_csv ( i ) for i in outputfiles ]) # drop all duplicate lines statsFile . drop_duplicates ( inplace = True ) # Convert the above object into a csv file and export statsFile . to_csv ( name , index = False , encoding = \"utf-8\" ) del statsFile # Add lat/lon/height info origstatsFile = pd . read_csv ( inFile ) statsFile = pd . read_csv ( name ) statsFile = pd . merge ( left = statsFile , right = origstatsFile [[ 'ID' , 'Lat' , 'Lon' , 'Hgt_m' ]], how = 'left' , left_on = 'ID' , right_on = 'ID' ) # drop all lines with nans and sort by station ID and year statsFile . dropna ( how = 'any' , inplace = True ) # drop all duplicate lines statsFile . drop_duplicates ( inplace = True ) statsFile . sort_values ([ 'ID' , 'Date' ]) statsFile . to_csv ( name , index = False ) del origstatsFile , statsFile","title":"get_station_data()"},{"location":"reference/#RAiDER.getStationDelays.seconds_of_day","text":"Convert HH:MM:SS format time-tag to seconds of day. Source code in RAiDER/getStationDelays.py 245 246 247 248 249 250 251 252 253 254 def seconds_of_day ( returnTime ): ''' Convert HH:MM:SS format time-tag to seconds of day. ''' if isinstance ( returnTime , dt . time ): h , m , s = returnTime . hour , returnTime . minute , returnTime . second else : h , m , s = map ( int , returnTime . split ( \":\" )) return h * 3600 + m * 60 + s","title":"seconds_of_day()"},{"location":"reference/#RAiDER.gnss","text":"","title":"gnss"},{"location":"reference/#RAiDER.gnss.downloadGNSSDelays","text":"","title":"downloadGNSSDelays"},{"location":"reference/#RAiDER.gnss.downloadGNSSDelays.check_url","text":"Check whether a file exists at a URL. Modified from https://stackoverflow.com/questions/9419162/download-returned-zip-file-from-url Source code in RAiDER/gnss/downloadGNSSDelays.py 153 154 155 156 157 158 159 160 161 162 def check_url ( url ): ''' Check whether a file exists at a URL. Modified from https://stackoverflow.com/questions/9419162/download-returned-zip-file-from-url ''' session = requests_retry_session () r = session . head ( url ) if r . status_code == 404 : url = '' return url","title":"check_url()"},{"location":"reference/#RAiDER.gnss.downloadGNSSDelays.download_UNR","text":"Download a zip file containing tropospheric delays for a given station and year The URL format is http://geodesy.unr.edu/gps_timeseries/trop/ / . .trop.zip Inputs statID - 4-character station identifier year - 4-numeral year Source code in RAiDER/gnss/downloadGNSSDelays.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def download_UNR ( statID , year , writeDir = '.' , download = False , baseURL = _UNR_URL ): ''' Download a zip file containing tropospheric delays for a given station and year The URL format is http://geodesy.unr.edu/gps_timeseries/trop/<ssss>/<ssss>.<yyyy>.trop.zip Inputs: statID - 4-character station identifier year - 4-numeral year ''' URL = \" {0} gps_timeseries/trop/ {1} / {1} . {2} .trop.zip\" . format ( baseURL , statID . upper (), year ) logger . debug ( 'Currently checking station %s in %s ' , statID , year ) if download : saveLoc = os . path . abspath ( os . path . join ( writeDir , ' {0} . {1} .trop.zip' . format ( statID . upper (), year ))) filepath = download_url ( URL , saveLoc ) else : filepath = check_url ( URL ) return { 'ID' : statID , 'year' : year , 'path' : filepath }","title":"download_UNR()"},{"location":"reference/#RAiDER.gnss.downloadGNSSDelays.download_tropo_delays","text":"Check for and download GNSS tropospheric delays from an archive. If download is True then files will be physically downloaded, which again is not necessary as data can be virtually accessed. Source code in RAiDER/gnss/downloadGNSSDelays.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def download_tropo_delays ( stats , years , gps_repo = None , writeDir = '.' , numCPUs = 8 , download = False ): ''' Check for and download GNSS tropospheric delays from an archive. If download is True then files will be physically downloaded, which again is not necessary as data can be virtually accessed. ''' # argument checking if not isinstance ( stats , ( list , str )): raise TypeError ( 'stats should be a string or a list of strings' ) if not isinstance ( years , ( list , int )): raise TypeError ( 'years should be an int or a list of ints' ) # Iterate over stations and years and check or download data stat_year_tup = itertools . product ( stats , years ) stat_year_tup = (( * tup , writeDir , download ) for tup in stat_year_tup ) # Parallelize remote querying of station locations with multiprocessing . Pool ( numCPUs ) as multipool : # only record valid path if gps_repo == 'UNR' : results = [ fileurl for fileurl in multipool . starmap ( download_UNR , stat_year_tup ) if fileurl [ 'path' ] ] # Write results to file statDF = pd . DataFrame ( results ) . set_index ( 'ID' ) statDF . to_csv ( os . path . join ( writeDir , ' {} gnssStationList_overbbox_withpaths.csv' . format ( gps_repo )))","title":"download_tropo_delays()"},{"location":"reference/#RAiDER.gnss.downloadGNSSDelays.download_url","text":"Download a file from a URL. Modified from https://stackoverflow.com/questions/9419162/download-returned-zip-file-from-url Source code in RAiDER/gnss/downloadGNSSDelays.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def download_url ( url , save_path , chunk_size = 2048 ): ''' Download a file from a URL. Modified from https://stackoverflow.com/questions/9419162/download-returned-zip-file-from-url ''' session = requests_retry_session () r = session . get ( url , stream = True ) if r . status_code == 404 : return '' else : logger . debug ( 'Beginning download of %s to %s ' , url , save_path ) with open ( save_path , 'wb' ) as fd : for chunk in r . iter_content ( chunk_size = chunk_size ): fd . write ( chunk ) logger . debug ( 'Completed download of %s to %s ' , url , save_path ) return save_path","title":"download_url()"},{"location":"reference/#RAiDER.gnss.downloadGNSSDelays.fix_lons","text":"Fix the given longitudes into the range [-180, 180] . Source code in RAiDER/gnss/downloadGNSSDelays.py 180 181 182 183 184 185 186 187 188 def fix_lons ( lon ): \"\"\" Fix the given longitudes into the range ``[-180, 180]``. \"\"\" fixed_lon = (( lon + 180 ) % 360 ) - 180 # Make the positive 180s positive again. if fixed_lon == - 180 and lon > 0 : fixed_lon *= - 1 return fixed_lon","title":"fix_lons()"},{"location":"reference/#RAiDER.gnss.downloadGNSSDelays.get_ID","text":"Pulls the station ID, lat, lon, and height for a given entry in the UNR text file Source code in RAiDER/gnss/downloadGNSSDelays.py 191 192 193 194 195 196 def get_ID ( line ): ''' Pulls the station ID, lat, lon, and height for a given entry in the UNR text file ''' stat_id , lat , lon , height = line . split ()[: 4 ] return stat_id , float ( lat ), float ( lon ), float ( height )","title":"get_ID()"},{"location":"reference/#RAiDER.gnss.downloadGNSSDelays.get_station_list","text":"Creates a list of stations inside a lat/lon bounding box from a source Inputs bbox - length-4 list of floats that describes a bounding box. Format is S N W E Source code in RAiDER/gnss/downloadGNSSDelays.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def get_station_list ( bbox = None , writeLoc = None , userstatList = None , name_appendix = '' ): ''' Creates a list of stations inside a lat/lon bounding box from a source Inputs: bbox - length-4 list of floats that describes a bounding box. Format is S N W E ''' writeLoc = os . path . join ( writeLoc or os . getcwd (), 'gnssStationList_overbbox' + name_appendix + '.csv' ) if userstatList : userstatList = read_text_file ( userstatList ) statList = get_stats_by_llh ( llhBox = bbox , userstatList = userstatList ) # write to file and pass final stations list statList . to_csv ( writeLoc , index = False ) stations = list ( statList [ 'ID' ] . values ) return stations , writeLoc","title":"get_station_list()"},{"location":"reference/#RAiDER.gnss.downloadGNSSDelays.get_stats_by_llh","text":"Function to pull lat, lon, height, beginning date, end date, and number of solutions for stations inside the bounding box llhBox. llhBox should be a tuple with format (lat1, lat2, lon1, lon2), where lat1, lon1 define the lower left-hand corner and lat2, lon2 define the upper right corner. Source code in RAiDER/gnss/downloadGNSSDelays.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def get_stats_by_llh ( llhBox = None , baseURL = _UNR_URL , userstatList = None ): ''' Function to pull lat, lon, height, beginning date, end date, and number of solutions for stations inside the bounding box llhBox. llhBox should be a tuple with format (lat1, lat2, lon1, lon2), where lat1, lon1 define the lower left-hand corner and lat2, lon2 define the upper right corner. ''' if llhBox is None : llhBox = [ - 90 , 90 , 0 , 360 ] stationHoldings = ' {} NGLStationPages/DataHoldings.txt' . format ( baseURL ) # it's a file like object and works just like a file session = requests_retry_session () data = session . get ( stationHoldings ) stations = [] for ind , line in enumerate ( data . text . splitlines ()): # files are iterable if ind == 0 : continue statID , lat , lon , height = get_ID ( line ) # Only pass if in bbox # And if user list of stations specified, only pass info for stations within list if in_box ( lat , lon , llhBox ) and ( not userstatList or statID in userstatList ): # convert lon into range [-180,180] lon = fix_lons ( lon ) stations . append ({ 'ID' : statID , 'Lat' : lat , 'Lon' : lon , 'Hgt_m' : height }) logger . info ( ' %d stations were found in %s ' , len ( stations ), llhBox ) stations = pd . DataFrame ( stations ) # Report stations from user's list that do not cover bbox if userstatList : userstatList = [ i for i in userstatList if i not in stations [ 'ID' ] . to_list ()] if userstatList : logger . warning ( \"The following user-input stations are not covered by the input \" \"bounding box %s : %s \" , str ( llhBox ) . strip ( '[]' ), str ( userstatList ) . strip ( '[]' ) ) return stations","title":"get_stats_by_llh()"},{"location":"reference/#RAiDER.gnss.downloadGNSSDelays.in_box","text":"Checks whether the given lat, lon pair are inside the bounding box llhbox Source code in RAiDER/gnss/downloadGNSSDelays.py 173 174 175 176 177 def in_box ( lat , lon , llhbox ): ''' Checks whether the given lat, lon pair are inside the bounding box llhbox ''' return lat < llhbox [ 1 ] and lat > llhbox [ 0 ] and lon < llhbox [ 3 ] and lon > llhbox [ 2 ]","title":"in_box()"},{"location":"reference/#RAiDER.gnss.downloadGNSSDelays.main","text":"Main workflow for querying supported GPS repositories for zenith delay information. Source code in RAiDER/gnss/downloadGNSSDelays.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 def main ( inps = None ): \"\"\" Main workflow for querying supported GPS repositories for zenith delay information. \"\"\" try : dateList = inps . date_list returnTime = inps . time except : dateList = inps . dateList returnTime = inps . returnTime station_file = inps . station_file bounding_box = inps . bounding_box gps_repo = inps . gps_repo out = inps . out download = inps . download cpus = inps . cpus verbose = inps . verbose if verbose : logger . setLevel ( logging . DEBUG ) # Create specified output directory if it does not exist. if not os . path . exists ( out ): os . mkdir ( out ) # Setup bounding box if bounding_box : if isinstance ( bounding_box , str ) and not os . path . isfile ( bounding_box ): try : bbox = [ float ( val ) for val in bounding_box . split ()] except ValueError : raise Exception ( 'Cannot understand the --bbox argument. String input is incorrect or path does not exist.' ) elif isinstance ( bounding_box , list ): bbox = bounding_box else : raise Exception ( 'Passing a file with a bounding box not yet supported.' ) long_cross_zero = 1 if bbox [ 2 ] * bbox [ 3 ] < 0 else 0 # if necessary, convert negative longitudes to positive if bbox [ 2 ] < 0 : bbox [ 2 ] += 360 if bbox [ 3 ] < 0 : bbox [ 3 ] += 360 # If bbox not specified, query stations across the entire globe else : bbox = [ - 90 , 90 , 0 , 360 ] long_cross_zero = 1 # Handle station query if long_cross_zero == 1 : bbox1 = bbox . copy () bbox2 = bbox . copy () bbox1 [ 3 ] = 360.0 bbox2 [ 2 ] = 0.0 stats1 , origstatsFile1 = get_station_list ( bbox = bbox1 , writeLoc = out , userstatList = station_file , name_appendix = '_a' ) stats2 , origstatsFile2 = get_station_list ( bbox = bbox2 , writeLoc = out , userstatList = station_file , name_appendix = '_b' ) stats = stats1 + stats2 origstatsFile = origstatsFile1 [: - 6 ] + '.csv' file_a = pd . read_csv ( origstatsFile1 ) file_b = pd . read_csv ( origstatsFile2 ) frames = [ file_a , file_b ] result = pd . concat ( frames , ignore_index = True ) result . to_csv ( origstatsFile , index = False ) else : if bbox [ 3 ] < bbox [ 2 ]: bbox [ 3 ] = 360.0 stats , origstatsFile = get_station_list ( bbox = bbox , writeLoc = out , userstatList = station_file ) # iterate over years years = list ( set ([ i . year for i in dateList ])) download_tropo_delays ( stats , years , gps_repo = gps_repo , writeDir = out , download = download ) # Add lat/lon info origstatsFile = pd . read_csv ( origstatsFile ) statsFile = pd . read_csv ( os . path . join ( out , ' {} gnssStationList_overbbox_withpaths.csv' . format ( gps_repo ))) statsFile = pd . merge ( left = statsFile , right = origstatsFile , how = 'left' , left_on = 'ID' , right_on = 'ID' ) statsFile . to_csv ( os . path . join ( out , ' {} gnssStationList_overbbox_withpaths.csv' . format ( gps_repo )), index = False ) del origstatsFile , statsFile # Extract delays for each station dateList = [ k . strftime ( '%Y-%m- %d ' ) for k in dateList ] get_station_data ( os . path . join ( out , ' {} gnssStationList_overbbox_withpaths.csv' . format ( gps_repo )), dateList , gps_repo = gps_repo , numCPUs = cpus , outDir = out , returnTime = returnTime ) logger . debug ( 'Completed processing' )","title":"main()"},{"location":"reference/#RAiDER.gnss.downloadGNSSDelays.read_text_file","text":"Read a list of GNSS station names from a plain text file Source code in RAiDER/gnss/downloadGNSSDelays.py 165 166 167 168 169 170 def read_text_file ( filename ): ''' Read a list of GNSS station names from a plain text file ''' with open ( filename , 'r' ) as f : return [ line . strip () for line in f ]","title":"read_text_file()"},{"location":"reference/#RAiDER.gnss.processDelayFiles","text":"","title":"processDelayFiles"},{"location":"reference/#RAiDER.gnss.processDelayFiles.addDateTimeToFiles","text":"Run through a list of files and add the datetime of each file as a column Source code in RAiDER/gnss/processDelayFiles.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def addDateTimeToFiles ( fileList , force = False , verbose = False ): ''' Run through a list of files and add the datetime of each file as a column ''' print ( 'Adding Datetime to delay files' ) for f in tqdm ( fileList ): data = pd . read_csv ( f ) if 'Datetime' in data . columns and not force : if verbose : print ( 'File {} already has a \"Datetime\" column, pass' '\"force = True\" if you want to override and ' 're-process' . format ( f ) ) else : try : dt = getDateTime ( f ) data [ 'Datetime' ] = dt # drop all lines with nans data . dropna ( how = 'any' , inplace = True ) # drop all duplicate lines data . drop_duplicates ( inplace = True ) data . to_csv ( f , index = False ) except ( AttributeError , ValueError ): print ( 'File {} does not contain datetime info, skipping' . format ( f ) ) del data","title":"addDateTimeToFiles()"},{"location":"reference/#RAiDER.gnss.processDelayFiles.concatDelayFiles","text":"Read a list of .csv files containing the same columns and append them together, sorting by specified columns Source code in RAiDER/gnss/processDelayFiles.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def concatDelayFiles ( fileList , sort_list = [ 'ID' , 'Datetime' ], return_df = False , outName = None , source = 'model' , ref = None , col_name = 'ZTD' ): ''' Read a list of .csv files containing the same columns and append them together, sorting by specified columns ''' dfList = [] print ( 'Concatenating delay files' ) for f in tqdm ( fileList ): if source == 'model' : dfList . append ( pd . read_csv ( f , parse_dates = [ 'Datetime' ])) else : dfList . append ( readZTDFile ( f , col_name = col_name )) # drop lines not found in reference file if ref : dfr = pd . read_csv ( ref , parse_dates = [ 'Datetime' ]) for i in enumerate ( dfList ): dfList [ i [ 0 ]] = pass_common_obs ( dfr , i [ 1 ]) del dfr df_c = pd . concat ( dfList , ignore_index = True ) . drop_duplicates () . reset_index ( drop = True ) df_c . sort_values ( by = sort_list , inplace = True ) print ( 'Total number of rows in the concatenated file: {} ' . format ( df_c . shape [ 0 ])) print ( 'Total number of rows containing NaNs: {} ' . format ( df_c [ df_c . isna () . any ( axis = 1 )] . shape [ 0 ] ) ) if return_df or outName is None : return df_c else : # drop all lines with nans df_c . dropna ( how = 'any' , inplace = True ) # drop all duplicate lines df_c . drop_duplicates ( inplace = True ) df_c . to_csv ( outName , index = False )","title":"concatDelayFiles()"},{"location":"reference/#RAiDER.gnss.processDelayFiles.create_parser","text":"Parse command line arguments using argparse. Source code in RAiDER/gnss/processDelayFiles.py 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 def create_parser (): \"\"\"Parse command line arguments using argparse.\"\"\" p = argparse . ArgumentParser ( formatter_class = argparse . RawDescriptionHelpFormatter , description = dedent ( \"\"\" \\ Combine delay files from a weather model and GPS Zenith delays Usage examples: raiderCombine.py --raiderDir './*' --raider 'combined_raider_delays.csv' raiderCombine.py --raiderDir ERA5/ --raider ERA5_combined_delays.csv --raider_column totalDelay --gnssDir GNSS/ --gnss UNRCombined_gnss.csv --column ZTD -o Combined_delays.csv raiderCombine.py --raiderDir ERA5_2019/ --raider ERA5_combined_delays_2019.csv --raider_column totalDelay --gnssDir GNSS_2019/ --gnss UNRCombined_gnss_2019.csv --column ZTD -o Combined_delays_2019_UTTC18.csv --localtime '18:00:00 1' \"\"\" ) ) p . add_argument ( '--raider' , dest = 'raider_file' , help = dedent ( \"\"\" \\ .csv file containing RAiDER-derived Zenith Delays. Should contain columns \"ID\" and \"Datetime\" in addition to the delay column If the file does not exist, I will attempt to create it from a directory of delay files. \"\"\" ), required = True ) p . add_argument ( '--raiderDir' , '-d' , dest = 'raider_folder' , help = dedent ( \"\"\" \\ Directory containing RAiDER-derived Zenith Delay files. Files should be named with a Datetime in the name and contain the column \"ID\" as the delay column names. \"\"\" ), default = os . getcwd () ) p . add_argument ( '--gnssDir' , '-gd' , dest = 'gnss_folder' , help = dedent ( \"\"\" \\ Directory containing GNSS-derived Zenith Delay files. Files should contain the column \"ID\" as the delay column names and times should be denoted by the \"Date\" key. \"\"\" ), default = os . getcwd () ) p . add_argument ( '--gnss' , dest = 'gnss_file' , help = dedent ( \"\"\" \\ Optional .csv file containing GPS Zenith Delays. Should contain columns \"ID\", \"ZTD\", and \"Datetime\" \"\"\" ), default = None ) p . add_argument ( '--raider_column' , '-r' , dest = 'raider_column_name' , help = dedent ( \"\"\" \\ Name of the column containing RAiDER delays. Only used with the \"--gnss\" option \"\"\" ), default = 'totalDelay' ) p . add_argument ( '--column' , '-c' , dest = 'column_name' , help = dedent ( \"\"\" \\ Name of the column containing GPS Zenith delays. Only used with the \"--gnss\" option \"\"\" ), default = 'ZTD' ) p . add_argument ( '--out' , '-o' , dest = 'out_name' , help = dedent ( \"\"\" \\ Name to use for the combined delay file. Only used with the \"--gnss\" option \"\"\" ), default = 'Combined_delays.csv' ) p . add_argument ( '--localtime' , '-lt' , dest = 'local_time' , help = dedent ( \"\"\" \\ \"Optional control to pass only data at local-time (in integer hours) WRT user-defined time at 0 longitude (1st argument), and within +/- specified hour threshold (2nd argument). By default UTC is passed as is without local-time conversions. Input in 'HH H', e.g. '16 1'\" \"\"\" ), default = None ) return p","title":"create_parser()"},{"location":"reference/#RAiDER.gnss.processDelayFiles.getDateTime","text":"Parse a datetime from a RAiDER delay filename Source code in RAiDER/gnss/processDelayFiles.py 87 88 89 90 91 92 93 94 95 def getDateTime ( filename ): ''' Parse a datetime from a RAiDER delay filename ''' filename = os . path . basename ( filename ) dtr = re . compile ( r '\\d {8} T\\d {6} ' ) dt = dtr . search ( filename ) return datetime . datetime . strptime ( dt . group (), '%Y%m %d T%H%M%S' )","title":"getDateTime()"},{"location":"reference/#RAiDER.gnss.processDelayFiles.local_time_filter","text":"Convert to local-time reference frame WRT 0 longitude Source code in RAiDER/gnss/processDelayFiles.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 def local_time_filter ( raiderFile , ztdFile , dfr , dfz , localTime ): ''' Convert to local-time reference frame WRT 0 longitude ''' localTime_hrs = int ( localTime . split ( ' ' )[ 0 ]) localTime_hrthreshold = int ( localTime . split ( ' ' )[ 1 ]) # with rotation rate and distance to 0 lon, get localtime shift WRT 00 UTC at 0 lon # *rotation rate at given point = (360deg/23.9333333333hr) = 15.041782729825965 deg/hr dfr [ 'Localtime' ] = ( dfr [ 'Lon' ] / 15.041782729825965 ) dfz [ 'Localtime' ] = ( dfz [ 'Lon' ] / 15.041782729825965 ) # estimate local-times dfr [ 'Localtime' ] = dfr . apply ( lambda r : update_time ( r , localTime_hrs ), axis = 1 ) dfz [ 'Localtime' ] = dfz . apply ( lambda r : update_time ( r , localTime_hrs ), axis = 1 ) # filter out data outside of --localtime hour threshold dfr [ 'Localtime_u' ] = dfr [ 'Localtime' ] + \\ datetime . timedelta ( hours = localTime_hrthreshold ) dfr [ 'Localtime_l' ] = dfr [ 'Localtime' ] - \\ datetime . timedelta ( hours = localTime_hrthreshold ) OG_total = dfr . shape [ 0 ] dfr = dfr [( dfr [ 'Datetime' ] >= dfr [ 'Localtime_l' ]) & ( dfr [ 'Datetime' ] <= dfr [ 'Localtime_u' ])] # only keep observation closest to Localtime print ( 'Total number of datapoints dropped in {} for not being within ' ' {} hrs of specified local-time {} : {} out of {} ' . format ( raiderFile , localTime . split ( ' ' )[ 1 ], localTime . split ( ' ' )[ 0 ], dfr . shape [ 0 ], OG_total )) dfz [ 'Localtime_u' ] = dfz [ 'Localtime' ] + \\ datetime . timedelta ( hours = localTime_hrthreshold ) dfz [ 'Localtime_l' ] = dfz [ 'Localtime' ] - \\ datetime . timedelta ( hours = localTime_hrthreshold ) OG_total = dfz . shape [ 0 ] dfz = dfz [( dfz [ 'Datetime' ] >= dfz [ 'Localtime_l' ]) & ( dfz [ 'Datetime' ] <= dfz [ 'Localtime_u' ])] # only keep observation closest to Localtime print ( 'Total number of datapoints dropped in {} for not being within ' ' {} hrs of specified local-time {} : {} out of {} ' . format ( ztdFile , localTime . split ( ' ' )[ 1 ], localTime . split ( ' ' )[ 0 ], dfz . shape [ 0 ], OG_total )) # drop all lines with nans dfr . dropna ( how = 'any' , inplace = True ) dfz . dropna ( how = 'any' , inplace = True ) # drop all duplicate lines dfr . drop_duplicates ( inplace = True ) dfz . drop_duplicates ( inplace = True ) # drop and rename columns dfr . drop ( columns = [ 'Localtime_l' , 'Localtime_u' ], inplace = True ) dfz . drop ( columns = [ 'Localtime_l' , 'Localtime_u' ], inplace = True ) return dfr , dfz","title":"local_time_filter()"},{"location":"reference/#RAiDER.gnss.processDelayFiles.mergeDelayFiles","text":"Merge a combined RAiDER delays file with a GPS ZTD delay file Source code in RAiDER/gnss/processDelayFiles.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 def mergeDelayFiles ( raiderFile , ztdFile , col_name = 'ZTD' , raider_delay = 'totalDelay' , outName = None , localTime = None ): ''' Merge a combined RAiDER delays file with a GPS ZTD delay file ''' print ( 'Merging delay files {} and {} ' . format ( raiderFile , ztdFile )) dfr = pd . read_csv ( raiderFile , parse_dates = [ 'Datetime' ]) # drop extra columns expected_data_columns = [ 'ID' , 'Lat' , 'Lon' , 'Hgt_m' , 'Datetime' , 'wetDelay' , 'hydroDelay' , raider_delay ] dfr = dfr . drop ( columns = [ col for col in dfr if col not in expected_data_columns ]) dfz = pd . read_csv ( ztdFile , parse_dates = [ 'Datetime' ]) # drop extra columns expected_data_columns = [ 'ID' , 'Date' , 'wet_delay' , 'hydrostatic_delay' , 'times' , 'sigZTD' , 'Lat' , 'Lon' , 'Hgt_m' , 'Datetime' , col_name ] dfz = dfz . drop ( columns = [ col for col in dfz if col not in expected_data_columns ]) # only pass common locations and times dfz = pass_common_obs ( dfr , dfz ) dfr = pass_common_obs ( dfz , dfr ) # If specified, convert to local-time reference frame WRT 0 longitude common_keys = [ 'Datetime' , 'ID' ] if localTime is not None : dfr , dfz = local_time_filter ( raiderFile , ztdFile , dfr , dfz , localTime ) common_keys . append ( 'Localtime' ) # only pass common locations and times dfz = pass_common_obs ( dfr , dfz , localtime = 'Localtime' ) dfr = pass_common_obs ( dfz , dfr , localtime = 'Localtime' ) # drop all lines with nans dfr . dropna ( how = 'any' , inplace = True ) dfz . dropna ( how = 'any' , inplace = True ) # drop all duplicate lines dfr . drop_duplicates ( inplace = True ) dfz . drop_duplicates ( inplace = True ) print ( 'Beginning merge' ) dfc = dfr . merge ( dfz [ common_keys + [ 'ZTD' , 'sigZTD' ]], how = 'left' , left_on = common_keys , right_on = common_keys , sort = True ) # only keep observation closest to Localtime if 'Localtime' in dfc . keys (): dfc [ 'Localtimediff' ] = abs (( dfc [ 'Datetime' ] - dfc [ 'Localtime' ]) . dt . total_seconds () / 3600 ) dfc = dfc . loc [ dfc . groupby ([ 'ID' , 'Localtime' ]) . Localtimediff . idxmin () ] . reset_index ( drop = True ) dfc . drop ( columns = [ 'Localtimediff' ], inplace = True ) # estimate residual dfc [ 'ZTD_minus_RAiDER' ] = dfc [ 'ZTD' ] - dfc [ raider_delay ] print ( 'Total number of rows in the concatenated file: ' ' {} ' . format ( dfc . shape [ 0 ])) print ( 'Total number of rows containing NaNs: {} ' . format ( dfc [ dfc . isna () . any ( axis = 1 )] . shape [ 0 ] ) ) print ( 'Merge finished' ) if outName is None : return dfc else : # drop all lines with nans dfc . dropna ( how = 'any' , inplace = True ) # drop all duplicate lines dfc . drop_duplicates ( inplace = True ) dfc . to_csv ( outName , index = False )","title":"mergeDelayFiles()"},{"location":"reference/#RAiDER.gnss.processDelayFiles.parseCMD","text":"Parse command-line arguments and pass to delay.py Source code in RAiDER/gnss/processDelayFiles.py 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 def parseCMD (): \"\"\" Parse command-line arguments and pass to delay.py \"\"\" p = create_parser () args = p . parse_args () if not os . path . exists ( args . raider_file ): combineDelayFiles ( args . raider_file , loc = args . raider_folder ) if not os . path . exists ( args . gnss_file ): combineDelayFiles ( args . gnss_file , loc = args . gnss_folder , source = 'GNSS' , ref = args . raider_file , col_name = args . column_name ) if args . gnss_file is not None : mergeDelayFiles ( args . raider_file , args . gnss_file , col_name = args . column_name , raider_delay = args . raider_column_name , outName = args . out_name , localTime = args . local_time )","title":"parseCMD()"},{"location":"reference/#RAiDER.gnss.processDelayFiles.pass_common_obs","text":"Pass only observations in target spatiotemporally common to reference Source code in RAiDER/gnss/processDelayFiles.py 123 124 125 126 127 128 129 130 131 132 133 def pass_common_obs ( reference , target , localtime = None ): '''Pass only observations in target spatiotemporally common to reference''' if localtime : return target [ target [ 'Datetime' ] . dt . date . isin ( reference [ 'Datetime' ] . dt . date ) & target [ 'ID' ] . isin ( reference [ 'ID' ]) & target [ localtime ] . isin ( reference [ localtime ])] else : return target [ target [ 'Datetime' ] . dt . date . isin ( reference [ 'Datetime' ] . dt . date ) & target [ 'ID' ] . isin ( reference [ 'ID' ])]","title":"pass_common_obs()"},{"location":"reference/#RAiDER.gnss.processDelayFiles.readZTDFile","text":"Read and parse a GPS zenith delay file Source code in RAiDER/gnss/processDelayFiles.py 328 329 330 331 332 333 334 335 336 337 338 339 340 def readZTDFile ( filename , col_name = 'ZTD' ): ''' Read and parse a GPS zenith delay file ''' try : data = pd . read_csv ( filename , parse_dates = [ 'Date' ]) times = data [ 'times' ] . apply ( lambda x : datetime . timedelta ( seconds = x )) data [ 'Datetime' ] = data [ 'Date' ] + times except ( KeyError , ValueError ): data = pd . read_csv ( filename , parse_dates = [ 'Datetime' ]) data . rename ( columns = { col_name : 'ZTD' }, inplace = True ) return data","title":"readZTDFile()"},{"location":"reference/#RAiDER.gnss.processDelayFiles.update_time","text":"Update with local origin time Source code in RAiDER/gnss/processDelayFiles.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def update_time ( row , localTime_hrs ): '''Update with local origin time''' localTime_estimate = row [ 'Datetime' ] . replace ( hour = localTime_hrs , minute = 0 , second = 0 ) # determine if you need to shift days time_shift = datetime . timedelta ( days = 0 ) # round to nearest hour days_diff = ( row [ 'Datetime' ] - datetime . timedelta ( seconds = math . floor ( row [ 'Localtime' ]) * 3600 )) . day - \\ localTime_estimate . day # if lon <0, check if you need to add day if row [ 'Lon' ] < 0 : # add day if days_diff != 0 : time_shift = datetime . timedelta ( days = 1 ) # if lon >0, check if you need to subtract day if row [ 'Lon' ] > 0 : # subtract day if days_diff != 0 : time_shift = - datetime . timedelta ( days = 1 ) return localTime_estimate + datetime . timedelta ( seconds = row [ 'Localtime' ] * 3600 ) + time_shift","title":"update_time()"},{"location":"reference/#RAiDER.interpolate","text":"Fast linear interpolator over a regular grid","title":"interpolate"},{"location":"reference/#RAiDER.interpolate.__doc__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__doc__"},{"location":"reference/#RAiDER.interpolate.__file__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__file__"},{"location":"reference/#RAiDER.interpolate.__name__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__name__"},{"location":"reference/#RAiDER.interpolate.__package__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__package__"},{"location":"reference/#RAiDER.interpolate.interpolate","text":"interpolate(points: List[numpy.ndarray[numpy.float64]], values: numpy.ndarray[numpy.float64], interp_points: numpy.ndarray[numpy.float64], fill_value: Optional[float] = None, assume_sorted: bool = False, max_threads: int = 8) -> numpy.ndarray[numpy.float64] Linear interpolator in any dimension. Arguments are similar to scipy.interpolate.RegularGridInterpolator :param points: Tuple of N axis coordinates specifying the grid. :param values: Nd array containing the grid point values. :param interp_points: List of points to interpolate, should have dimension (x, N). If this list is guaranteed to be sorted make sure to use the assume_sorted option. :param fill_value: The value to return for interpolation points outside of the grid range. :param assume_sorted: Enable optimization when the list of interpolation points is sorted. :param max_threads: Limit the number of threads to a certain amount. Note: The number of threads will always be one of {1, 2, 4, 8}","title":"interpolate()"},{"location":"reference/#RAiDER.interpolate.interpolate_along_axis","text":"interpolate_along_axis(points: numpy.ndarray[numpy.float64], values: numpy.ndarray[numpy.float64], interp_points: numpy.ndarray[numpy.float64], axis: int = -1, fill_value: Optional[float] = None, assume_sorted: bool = False, max_threads: int = 8) -> numpy.ndarray[numpy.float64] 1D linear interpolator along a specific axis. :param points: N-dimensional x coordinates. Axis specified by 'axis' must contain at least 2 points. :param values: N-dimensional y values. Must have the same shape as 'points'. :param interp_points: N-dimensional x coordinates to interpolate at. The shape may only differ from that of 'points' at the axis specified by 'axis'. For example if 'points' has shape (1, 2, 3) and 'axis' is 2, then and shape like (1, 2, X) is valid. :param axis: The axis to interpolate along. :param fill_value: The value to return for interpolation points outside of the grid range. :param assume_sorted: Enable optimization when the list of interpolation points is sorted along the axis of interpolation. :param max_threads: Limit the number of threads to a certain amount.","title":"interpolate_along_axis()"},{"location":"reference/#RAiDER.interpolator","text":"","title":"interpolator"},{"location":"reference/#RAiDER.interpolator.RegularGridInterpolator","text":"Bases: object Provides a wrapper around RAiDER.interpolate.interpolate with a similar interface to scipy.interpolate.RegularGridInterpolator. Source code in RAiDER/interpolator.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 class RegularGridInterpolator ( object ): \"\"\" Provides a wrapper around RAiDER.interpolate.interpolate with a similar interface to scipy.interpolate.RegularGridInterpolator. \"\"\" def __init__ ( self , grid , values , fill_value = None , assume_sorted = False , max_threads = 8 ): self . grid = grid self . values = values self . fill_value = fill_value self . assume_sorted = assume_sorted self . max_threads = max_threads def __call__ ( self , points ): if isinstance ( points , tuple ): shape = points [ 0 ] . shape for arr in points : assert arr . shape == shape , \"All dimensions must contain the same number of points!\" interp_points = np . stack ( points , axis =- 1 ) in_shape = interp_points . shape elif points . ndim > 2 : in_shape = points . shape interp_points = points . reshape (( np . prod ( points . shape [: - 1 ]),) + ( points . shape [ - 1 ],)) else : interp_points = points in_shape = interp_points . shape out = interpolate ( self . grid , self . values , interp_points , fill_value = self . fill_value , assume_sorted = self . assume_sorted , max_threads = self . max_threads ) return out . reshape ( in_shape [: - 1 ])","title":"RegularGridInterpolator"},{"location":"reference/#RAiDER.interpolator.interpV","text":"Rearrange np.interp's arguments Source code in RAiDER/interpolator.py 80 81 82 83 84 def interpV ( y , old_x , new_x , left = None , right = None , period = None ): ''' Rearrange np.interp's arguments ''' return np . interp ( new_x , old_x , y , left = left , right = right , period = period )","title":"interpV()"},{"location":"reference/#RAiDER.interpolator.interpVector","text":"Interpolate data from a single vector containing the original x, the original y, and the new x, in that order. Nx tells the number of original x-points. Source code in RAiDER/interpolator.py 87 88 89 90 91 92 93 94 95 96 97 def interpVector ( vec , Nx ): ''' Interpolate data from a single vector containing the original x, the original y, and the new x, in that order. Nx tells the number of original x-points. ''' x = vec [: Nx ] y = vec [ Nx : 2 * Nx ] xnew = vec [ 2 * Nx :] f = interp1d ( x , y , bounds_error = False , copy = False , assume_sorted = True ) return f ( xnew )","title":"interpVector()"},{"location":"reference/#RAiDER.interpolator.interp_along_axis","text":"DEPRECATED: Use RAiDER.interpolate.interpolate_along_axis instead (it is much faster). This function now primarily exists to verify the behavior of the new one. Interpolate an array of 3-D data along one axis. This function assumes that the x-coordinate increases monotonically. Source code in RAiDER/interpolator.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def interp_along_axis ( oldCoord , newCoord , data , axis = 2 , pad = False ): ''' DEPRECATED: Use RAiDER.interpolate.interpolate_along_axis instead (it is much faster). This function now primarily exists to verify the behavior of the new one. Interpolate an array of 3-D data along one axis. This function assumes that the x-coordinate increases monotonically. ''' if oldCoord . ndim > 1 : stackedData = np . concatenate ([ oldCoord , data , newCoord ], axis = axis ) out = np . apply_along_axis ( interpVector , axis = axis , arr = stackedData , Nx = oldCoord . shape [ axis ]) else : out = np . apply_along_axis ( interpV , axis = axis , arr = data , old_x = oldCoord , new_x = newCoord , left = np . nan , right = np . nan ) return out","title":"interp_along_axis()"},{"location":"reference/#RAiDER.interpolator.interpolateDEM","text":"Interpolate a DEM raster to a set of lat/lon query points Source code in RAiDER/interpolator.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def interpolateDEM ( demRaster , extent , outLL , method = 'linear' ): ''' Interpolate a DEM raster to a set of lat/lon query points ''' minlat , maxlat , minlon , maxlon = extent nPixLat = demRaster . shape [ 0 ] nPixLon = demRaster . shape [ 1 ] xlats = np . linspace ( minlat , maxlat , nPixLat ) xlons = np . linspace ( minlon , maxlon , nPixLon ) interpolator = rgi ( points = ( xlats , xlons ), values = demRaster , method = method , bounds_error = False ) outInterp = interpolator ( outLL ) return outInterp","title":"interpolateDEM()"},{"location":"reference/#RAiDER.llreader","text":"","title":"llreader"},{"location":"reference/#RAiDER.llreader.AOI","text":"Bases: object This instantiates a generic AOI class object Source code in RAiDER/llreader.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 class AOI ( object ): ''' This instantiates a generic AOI class object ''' def __init__ ( self ): self . _bounding_box = None self . _proj = CRS . from_epsg ( 4326 ) def type ( self ): return self . _type def bounds ( self ): return self . _bounding_box def projection ( self ): return self . _proj def add_buffer ( self , buffer ): ''' Check whether an extra lat/lon buffer is needed for raytracing ''' # if raytracing, add a 1-degree buffer all around try : ll_bounds = self . _bounding_box . copy () except AttributeError : ll_bounds = list ( self . _bounding_box ) ll_bounds [ 0 ] = np . max ([ ll_bounds [ 0 ] - buffer , - 90 ]) ll_bounds [ 1 ] = np . min ([ ll_bounds [ 1 ] + buffer , 90 ]) ll_bounds [ 2 ] = np . max ([ ll_bounds [ 2 ] - buffer , - 180 ]) ll_bounds [ 3 ] = np . min ([ ll_bounds [ 3 ] + buffer , 180 ]) return ll_bounds","title":"AOI"},{"location":"reference/#RAiDER.llreader.AOI.add_buffer","text":"Check whether an extra lat/lon buffer is needed for raytracing Source code in RAiDER/llreader.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def add_buffer ( self , buffer ): ''' Check whether an extra lat/lon buffer is needed for raytracing ''' # if raytracing, add a 1-degree buffer all around try : ll_bounds = self . _bounding_box . copy () except AttributeError : ll_bounds = list ( self . _bounding_box ) ll_bounds [ 0 ] = np . max ([ ll_bounds [ 0 ] - buffer , - 90 ]) ll_bounds [ 1 ] = np . min ([ ll_bounds [ 1 ] + buffer , 90 ]) ll_bounds [ 2 ] = np . max ([ ll_bounds [ 2 ] - buffer , - 180 ]) ll_bounds [ 3 ] = np . min ([ ll_bounds [ 3 ] + buffer , 180 ]) return ll_bounds","title":"add_buffer()"},{"location":"reference/#RAiDER.llreader.BoundingBox","text":"Bases: AOI Parse a bounding box AOI Source code in RAiDER/llreader.py 132 133 134 135 136 137 class BoundingBox ( AOI ): '''Parse a bounding box AOI''' def __init__ ( self , bbox ): AOI . __init__ ( self ) self . _bounding_box = bbox self . _type = 'bounding_box'","title":"BoundingBox"},{"location":"reference/#RAiDER.llreader.GeocodedFile","text":"Bases: AOI Parse a Geocoded file for coordinates Source code in RAiDER/llreader.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 class GeocodedFile ( AOI ): '''Parse a Geocoded file for coordinates''' def __init__ ( self , filename , is_dem = False ): super () . __init__ () self . _filename = filename self . p = rio_profile ( filename ) self . _bounding_box = rio_extents ( self . p ) self . _is_dem = is_dem _ , self . _proj , self . _gt = rio_stats ( filename ) self . _type = 'geocoded_file' def readLL ( self ): # ll_bounds are SNWE S , N , W , E = self . _bounding_box w , h = self . p [ 'width' ], self . p [ 'height' ] px = ( E - W ) / w py = ( N - S ) / h x = np . array ([ W + ( t * px ) for t in range ( w )]) y = np . array ([ S + ( t * py ) for t in range ( h )]) X , Y = np . meshgrid ( x , y ) return Y , X # lats, lons def readZ ( self ): demFile = self . _filename if self . _is_dem else 'GLO30_fullres_dem.tif' bbox = self . _bounding_box zvals , metadata = download_dem ( bbox , writeDEM = True , outName = demFile ) z_bounds = get_bbox ( metadata ) z_out = interpolateDEM ( zvals , z_bounds , self . readLL (), method = 'nearest' ) return z_out","title":"GeocodedFile"},{"location":"reference/#RAiDER.llreader.Geocube","text":"Bases: AOI Parse a georeferenced data cube Source code in RAiDER/llreader.py 173 174 175 176 177 178 179 180 181 182 class Geocube ( AOI ): '''Parse a georeferenced data cube''' def __init__ ( self ): super () . __init__ () self . _type = 'geocube' raise NotImplementedError def readLL ( self ): return None , None","title":"Geocube"},{"location":"reference/#RAiDER.llreader.StationFile","text":"Bases: AOI Use a .csv file containing at least Lat, Lon, and optionally Hgt_m columns Source code in RAiDER/llreader.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 class StationFile ( AOI ): '''Use a .csv file containing at least Lat, Lon, and optionally Hgt_m columns''' def __init__ ( self , station_file ): super () . __init__ () self . _filename = station_file self . _bounding_box = bounds_from_csv ( station_file ) self . _type = 'station_file' def readLL ( self ): df = pd . read_csv ( self . _filename ) . drop_duplicates ( subset = [ \"Lat\" , \"Lon\" ]) return df [ 'Lat' ] . values , df [ 'Lon' ] . values def readZ ( self ): df = pd . read_csv ( self . _filename ) if 'Hgt_m' in df . columns : return df [ 'Hgt_m' ] . values else : zvals , metadata = download_dem ( self . _bounding_box ) z_bounds = get_bbox ( metadata ) z_out = interpolateDEM ( zvals , z_bounds , self . readLL (), method = 'nearest' ) df [ 'Hgt_m' ] = z_out df . to_csv ( self . _filename , index = False ) self . __init__ ( self . _filename ) return z_out","title":"StationFile"},{"location":"reference/#RAiDER.llreader.bounds_from_csv","text":"station_file should be a comma-delimited file with at least \"Lat\" and \"Lon\" columns, which should be EPSG: 4326 projection (i.e WGS84) Source code in RAiDER/llreader.py 207 208 209 210 211 212 213 214 215 216 def bounds_from_csv ( station_file ): ''' station_file should be a comma-delimited file with at least \"Lat\" and \"Lon\" columns, which should be EPSG: 4326 projection (i.e WGS84) ''' stats = pd . read_csv ( station_file ) . drop_duplicates ( subset = [ \"Lat\" , \"Lon\" ]) if 'Hgt_m' in stats . columns : use_csv_heights = True snwe = [ stats [ 'Lat' ] . min (), stats [ 'Lat' ] . max (), stats [ 'Lon' ] . min (), stats [ 'Lon' ] . max ()] return snwe","title":"bounds_from_csv()"},{"location":"reference/#RAiDER.llreader.bounds_from_latlon_rasters","text":"Parse lat/lon/height inputs and return the appropriate outputs Source code in RAiDER/llreader.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 def bounds_from_latlon_rasters ( latfile , lonfile ): ''' Parse lat/lon/height inputs and return the appropriate outputs ''' latinfo = get_file_and_band ( latfile ) loninfo = get_file_and_band ( lonfile ) lat_stats , lat_proj , _ = rio_stats ( latinfo [ 0 ], band = latinfo [ 1 ]) lon_stats , lon_proj , _ = rio_stats ( loninfo [ 0 ], band = loninfo [ 1 ]) if lat_proj != lon_proj : raise ValueError ( 'Projection information for Latitude and Longitude files does not match' ) # TODO - handle dateline crossing here snwe = ( lat_stats . min , lat_stats . max , lon_stats . min , lon_stats . max ) fname = os . path . basename ( latfile ) . split ( '.' )[ 0 ] return lat_proj , snwe , fname","title":"bounds_from_latlon_rasters()"},{"location":"reference/#RAiDER.logger","text":"Global logging configuration","title":"logger"},{"location":"reference/#RAiDER.logger.CustomFormatter","text":"Bases: UnixColorFormatter Adds levelname prefixes to the message on warning or above. Source code in RAiDER/logger.py 49 50 51 52 53 54 55 56 class CustomFormatter ( UnixColorFormatter ): \"\"\"Adds levelname prefixes to the message on warning or above.\"\"\" def formatMessage ( self , record ): message = super () . formatMessage ( record ) if record . levelno >= logging . WARNING : message = \": \" . join (( record . levelname , message )) return message","title":"CustomFormatter"},{"location":"reference/#RAiDER.losreader","text":"","title":"losreader"},{"location":"reference/#RAiDER.losreader.Conventional","text":"Bases: LOS Special value indicating that the zenith delay will be projected using the standard cos(inc) scaling. Source code in RAiDER/losreader.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 class Conventional ( LOS ): \"\"\" Special value indicating that the zenith delay will be projected using the standard cos(inc) scaling. \"\"\" def __init__ ( self , filename = None , los_convention = 'isce' , time = None , pad = 600 ): super () . __init__ () self . _file = filename self . _time = time self . _pad = pad self . _is_projected = True self . _convention = los_convention if self . _convention . lower () != 'isce' : raise NotImplementedError () def __call__ ( self , delays ): ''' Read the LOS file and convert it to look vectors ''' if self . _lats is None : raise ValueError ( 'Target points not set' ) if self . _file is None : raise ValueError ( 'LOS file not set' ) try : # if an ISCE-style los file is passed open it with GDAL LOS_enu = inc_hd_to_enu ( * rio_open ( self . _file )) except OSError : # Otherwise, treat it as an orbit / statevector file svs = np . stack ( get_sv ( self . _file , self . _time , self . _pad ), axis =- 1 ) LOS_enu = state_to_los ( svs , [ self . _lats , self . _lons , self . _heights ], out = \"lookangle\" ) if delays . shape == LOS_enu . shape : return delays / LOS_enu else : return delays / LOS_enu [ ... , - 1 ]","title":"Conventional"},{"location":"reference/#RAiDER.losreader.Conventional.__call__","text":"Read the LOS file and convert it to look vectors Source code in RAiDER/losreader.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def __call__ ( self , delays ): ''' Read the LOS file and convert it to look vectors ''' if self . _lats is None : raise ValueError ( 'Target points not set' ) if self . _file is None : raise ValueError ( 'LOS file not set' ) try : # if an ISCE-style los file is passed open it with GDAL LOS_enu = inc_hd_to_enu ( * rio_open ( self . _file )) except OSError : # Otherwise, treat it as an orbit / statevector file svs = np . stack ( get_sv ( self . _file , self . _time , self . _pad ), axis =- 1 ) LOS_enu = state_to_los ( svs , [ self . _lats , self . _lons , self . _heights ], out = \"lookangle\" ) if delays . shape == LOS_enu . shape : return delays / LOS_enu else : return delays / LOS_enu [ ... , - 1 ]","title":"__call__()"},{"location":"reference/#RAiDER.losreader.LOS","text":"Bases: ABC LOS Class definition for handling look vectors Source code in RAiDER/losreader.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 class LOS ( ABC ): ''' LOS Class definition for handling look vectors ''' def __init__ ( self ): self . _lats , self . _lons , self . _heights = None , None , None self . _look_vecs = None self . _ray_trace = False self . _is_zenith = False self . _is_projected = False def setPoints ( self , lats , lons = None , heights = None ): '''Set the pixel locations''' if ( lats is None ) and ( self . _lats is None ): raise RuntimeError ( \"You haven't given any point locations yet\" ) # Will overwrite points by default if lons is None : llh = lats # assume points are [lats lons heights] self . _lats = llh [ ... , 0 ] self . _lons = llh [ ... , 1 ] self . _heights = llh [ ... , 2 ] elif heights is None : self . _lats = lats self . _lons = lons self . _heights = np . zeros (( len ( lats ), 1 )) else : self . _lats = lats self . _lons = lons self . _heights = heights def setTime ( self , dt ): self . _time = dt def is_Zenith ( self ): return self . _is_zenith def is_Projected ( self ): return self . _is_projected def ray_trace ( self ): return self . _ray_trace","title":"LOS"},{"location":"reference/#RAiDER.losreader.LOS.setPoints","text":"Set the pixel locations Source code in RAiDER/losreader.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def setPoints ( self , lats , lons = None , heights = None ): '''Set the pixel locations''' if ( lats is None ) and ( self . _lats is None ): raise RuntimeError ( \"You haven't given any point locations yet\" ) # Will overwrite points by default if lons is None : llh = lats # assume points are [lats lons heights] self . _lats = llh [ ... , 0 ] self . _lons = llh [ ... , 1 ] self . _heights = llh [ ... , 2 ] elif heights is None : self . _lats = lats self . _lons = lons self . _heights = np . zeros (( len ( lats ), 1 )) else : self . _lats = lats self . _lons = lons self . _heights = heights","title":"setPoints()"},{"location":"reference/#RAiDER.losreader.Raytracing","text":"Bases: LOS Special value indicating that full raytracing will be used to calculate slant delays. Get unit look vectors pointing from the ground (target) pixels to the sensor, or to Zenith. Can be accomplished using an ISCE-style 2-band LOS file or a file containing orbital statevectors. NOTE : These line-of-sight vectors will NOT match ordinary LOS vectors for InSAR because they are in an ECEF reference frame instead of a local ENU. This is done because the construction of rays is done in ECEF rather than the local ENU. python datetime - user-requested query time. Must be compatible with the orbit file passed. Only required for a statevector file. int - integer number of seconds to pad around the user-specified time; default 3 hours Only required for a statevector file. ndarray - an x 3 array of unit look vectors, defined in an Earth-centered, earth-fixed reference frame (ECEF). Convention is vectors point from the target pixel to the sensor. ndarray - array of of the distnce from the surface to the top of the troposphere (denoted by zref)","title":"Raytracing"},{"location":"reference/#RAiDER.losreader.Raytracing--example","text":"from RAiDER.losreader import Raytracing import numpy as np TODO Source code in RAiDER/losreader.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 class Raytracing ( LOS ): \"\"\" Special value indicating that full raytracing will be used to calculate slant delays. Get unit look vectors pointing from the ground (target) pixels to the sensor, or to Zenith. Can be accomplished using an ISCE-style 2-band LOS file or a file containing orbital statevectors. *NOTE*: These line-of-sight vectors will NOT match ordinary LOS vectors for InSAR because they are in an ECEF reference frame instead of a local ENU. This is done because the construction of rays is done in ECEF rather than the local ENU. Args: ---------- time: python datetime - user-requested query time. Must be compatible with the orbit file passed. Only required for a statevector file. pad: int - integer number of seconds to pad around the user-specified time; default 3 hours Only required for a statevector file. Returns: ------- ndarray - an <in_shape> x 3 array of unit look vectors, defined in an Earth-centered, earth-fixed reference frame (ECEF). Convention is vectors point from the target pixel to the sensor. ndarray - array of <in_shape> of the distnce from the surface to the top of the troposphere (denoted by zref) Example: -------- >>> from RAiDER.losreader import Raytracing >>> import numpy as np >>> TODO \"\"\" def __init__ ( self , filename = None , los_convention = 'isce' , time = None , pad = 600 ): '''read in and parse a statevector file''' super () . __init__ () self . _ray_trace = True self . _file = filename self . _time = time self . _pad = pad self . _convention = los_convention if self . _convention . lower () != 'isce' : raise NotImplementedError () def getLookVectors ( self , time , pad = 3 * 60 ): ''' Calculate look vectors for raytracing ''' if self . _lats is None : raise ValueError ( 'Target points not set' ) if self . _file is None : raise ValueError ( 'LOS file not set' ) try : # if an ISCE-style los file is provided, use GDAL LOS_enu = inc_hd_to_enu ( * rio_open ( self . _file )) self . _look_vecs = enu2ecef ( LOS_enu [ ... , 0 ], LOS_enu [ ... , 1 ], LOS_enu [ ... , 2 ], self . _lats , self . _lons , self . _heights ) self . _xyz = np . stack ( lla2ecef ([ self . _lats , self . _lons , self . _heights ]), axis =- 1 ) except OSError : # Otherwise treat it as a state vector file svs = np . stack ( get_sv ( self . _file , self . _time , self . _pad ), axis =- 1 ) self . _look_vecs , self . _xyz = state_to_los ( svs , [ self . _lats , self . _lons , self . _heights ], out = \"ecef\" ) def getIntersectionWithHeight ( self , height ): \"\"\" This function computes the intersection point of a ray at a height level \"\"\" # We just leverage the same code as finding top of atmosphere here return getTopOfAtmosphere ( self . _xyz , self . _look_vecs , height ) def getIntersectionWithLevels ( self , levels ): \"\"\" This function returns the points at which rays intersect the given height levels. This way we have same number of points in each ray and only at level transitions. For targets that are above a given height level, the ray points are set to nan to indicate that it does not contribute to the integration of rays. Output: rays: (self._lats.shape, len(levels), 3) \"\"\" rays = np . zeros ( list ( self . _lats . shape ) + [ len ( levels ), 3 ]) # This can be further vectorized, if there is enough memory for ind , z in enumerate ( levels ): rays [ ... , ind , :] = self . getIntersectionWithHeight ( z ) # Set pixels above level to nan value = rays [ ... , ind , :] value [ self . _heights > z , :] = np . nan return rays def calculateDelays ( self , delays ): ''' Here \"delays\" is point-wise delays (i.e. refractivities), not integrated ZTD/STD. ''' # Create rays (Use getIntersectionWithLevels above) # Interpolate delays to rays # Integrate along rays # Return STD raise NotImplementedError","title":"Example:"},{"location":"reference/#RAiDER.losreader.Raytracing.__init__","text":"read in and parse a statevector file Source code in RAiDER/losreader.py 179 180 181 182 183 184 185 186 187 188 def __init__ ( self , filename = None , los_convention = 'isce' , time = None , pad = 600 ): '''read in and parse a statevector file''' super () . __init__ () self . _ray_trace = True self . _file = filename self . _time = time self . _pad = pad self . _convention = los_convention if self . _convention . lower () != 'isce' : raise NotImplementedError ()","title":"__init__()"},{"location":"reference/#RAiDER.losreader.Raytracing.calculateDelays","text":"Here \"delays\" is point-wise delays (i.e. refractivities), not integrated ZTD/STD. Source code in RAiDER/losreader.py 265 266 267 268 269 270 271 272 273 274 def calculateDelays ( self , delays ): ''' Here \"delays\" is point-wise delays (i.e. refractivities), not integrated ZTD/STD. ''' # Create rays (Use getIntersectionWithLevels above) # Interpolate delays to rays # Integrate along rays # Return STD raise NotImplementedError","title":"calculateDelays()"},{"location":"reference/#RAiDER.losreader.Raytracing.getIntersectionWithHeight","text":"This function computes the intersection point of a ray at a height level Source code in RAiDER/losreader.py 230 231 232 233 234 235 236 def getIntersectionWithHeight ( self , height ): \"\"\" This function computes the intersection point of a ray at a height level \"\"\" # We just leverage the same code as finding top of atmosphere here return getTopOfAtmosphere ( self . _xyz , self . _look_vecs , height )","title":"getIntersectionWithHeight()"},{"location":"reference/#RAiDER.losreader.Raytracing.getIntersectionWithLevels","text":"This function returns the points at which rays intersect the given height levels. This way we have same number of points in each ray and only at level transitions. For targets that are above a given height level, the ray points are set to nan to indicate that it does not contribute to the integration of rays. Output rays: (self._lats.shape, len(levels), 3) Source code in RAiDER/losreader.py 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 def getIntersectionWithLevels ( self , levels ): \"\"\" This function returns the points at which rays intersect the given height levels. This way we have same number of points in each ray and only at level transitions. For targets that are above a given height level, the ray points are set to nan to indicate that it does not contribute to the integration of rays. Output: rays: (self._lats.shape, len(levels), 3) \"\"\" rays = np . zeros ( list ( self . _lats . shape ) + [ len ( levels ), 3 ]) # This can be further vectorized, if there is enough memory for ind , z in enumerate ( levels ): rays [ ... , ind , :] = self . getIntersectionWithHeight ( z ) # Set pixels above level to nan value = rays [ ... , ind , :] value [ self . _heights > z , :] = np . nan return rays","title":"getIntersectionWithLevels()"},{"location":"reference/#RAiDER.losreader.Raytracing.getLookVectors","text":"Calculate look vectors for raytracing Source code in RAiDER/losreader.py 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 def getLookVectors ( self , time , pad = 3 * 60 ): ''' Calculate look vectors for raytracing ''' if self . _lats is None : raise ValueError ( 'Target points not set' ) if self . _file is None : raise ValueError ( 'LOS file not set' ) try : # if an ISCE-style los file is provided, use GDAL LOS_enu = inc_hd_to_enu ( * rio_open ( self . _file )) self . _look_vecs = enu2ecef ( LOS_enu [ ... , 0 ], LOS_enu [ ... , 1 ], LOS_enu [ ... , 2 ], self . _lats , self . _lons , self . _heights ) self . _xyz = np . stack ( lla2ecef ([ self . _lats , self . _lons , self . _heights ]), axis =- 1 ) except OSError : # Otherwise treat it as a state vector file svs = np . stack ( get_sv ( self . _file , self . _time , self . _pad ), axis =- 1 ) self . _look_vecs , self . _xyz = state_to_los ( svs , [ self . _lats , self . _lons , self . _heights ], out = \"ecef\" )","title":"getLookVectors()"},{"location":"reference/#RAiDER.losreader.Zenith","text":"Bases: LOS Class definition for a \"Zenith\" object. Source code in RAiDER/losreader.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 class Zenith ( LOS ): \"\"\" Class definition for a \"Zenith\" object. \"\"\" def __init__ ( self ): super () . __init__ () self . _is_zenith = True def setLookVectors ( self ): '''Set point locations and calculate Zenith look vectors''' if self . _lats is None : raise ValueError ( 'Target points not set' ) if self . _look_vecs is None : self . _look_vecs = getZenithLookVecs ( self . _lats , self . _lons , self . _heights ) def __call__ ( self , delays ): '''Placeholder method for consistency with the other classes''' return delays","title":"Zenith"},{"location":"reference/#RAiDER.losreader.Zenith.__call__","text":"Placeholder method for consistency with the other classes Source code in RAiDER/losreader.py 91 92 93 def __call__ ( self , delays ): '''Placeholder method for consistency with the other classes''' return delays","title":"__call__()"},{"location":"reference/#RAiDER.losreader.Zenith.setLookVectors","text":"Set point locations and calculate Zenith look vectors Source code in RAiDER/losreader.py 83 84 85 86 87 88 def setLookVectors ( self ): '''Set point locations and calculate Zenith look vectors''' if self . _lats is None : raise ValueError ( 'Target points not set' ) if self . _look_vecs is None : self . _look_vecs = getZenithLookVecs ( self . _lats , self . _lons , self . _heights )","title":"setLookVectors()"},{"location":"reference/#RAiDER.losreader.cut_times","text":"Slice the orbit file around the reference aquisition time. This is done by default using a three-hour window, which for Sentinel-1 empirically works out to be roughly the largest window allowed by the orbit time. times: Nt x 1 ndarray - Vector of orbit times as datetime ref_time: datetime - Reference time pad: int - integer time in seconds to use as padding idx: Nt x 1 logical ndarray - a mask of times within the padded request time. Source code in RAiDER/losreader.py 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 def cut_times ( times , ref_time , pad = 3600 * 3 ): \"\"\" Slice the orbit file around the reference aquisition time. This is done by default using a three-hour window, which for Sentinel-1 empirically works out to be roughly the largest window allowed by the orbit time. Args: ---------- times: Nt x 1 ndarray - Vector of orbit times as datetime ref_time: datetime - Reference time pad: int - integer time in seconds to use as padding Returns: ------- idx: Nt x 1 logical ndarray - a mask of times within the padded request time. \"\"\" diff = np . array ( [( x - ref_time ) . total_seconds () for x in times ] ) return np . abs ( diff ) < pad","title":"cut_times()"},{"location":"reference/#RAiDER.losreader.getTopOfAtmosphere","text":"Get ray intersection at given height. We use simple Newton-Raphson for this computation. This cannot be done exactly since closed form expression from xyz to llh is super compliated. If a factor (cos of inc angle) is provided - iterations are lot faster. If factor is not provided solutions converges to - 0.01 mm at heights near zero in 10 iterations - 10 cm at heights above 40km in 10 iterations If factor is know, we converge in 3 iterations to less than a micron. Source code in RAiDER/losreader.py 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 def getTopOfAtmosphere ( xyz , look_vecs , toaheight , factor = None ): \"\"\" Get ray intersection at given height. We use simple Newton-Raphson for this computation. This cannot be done exactly since closed form expression from xyz to llh is super compliated. If a factor (cos of inc angle) is provided - iterations are lot faster. If factor is not provided solutions converges to - 0.01 mm at heights near zero in 10 iterations - 10 cm at heights above 40km in 10 iterations If factor is know, we converge in 3 iterations to less than a micron. \"\"\" if factor is not None : maxIter = 3 else : maxIter = 10 factor = 1. # Guess top point pos = xyz + toaheight * look_vecs for niter in range ( 10 ): pos_llh = ecef2lla ( pos [ ... , 0 ], pos [ ... , 1 ], pos [ ... , 2 ]) pos = pos + look_vecs * (( toaheight - pos_llh [ 2 ]) / factor )[ ... , None ] # This is for debugging the approach # print(\"Stats for TOA computation: \", toaheight, # toaheight - np.nanmin(pos_llh[2]), # toaheight - np.nanmax(pos_llh[2]), # ) # The converged solution represents top of the rays return pos","title":"getTopOfAtmosphere()"},{"location":"reference/#RAiDER.losreader.getZenithLookVecs","text":"Returns look vectors when Zenith is used. Parameters: Name Type Description Default lats/lons/heights ndarray Numpy arrays containing WGS-84 target locations required Returns: Name Type Description zenLookVecs ndarray (in_shape) x 3 unit look vectors in an ECEF reference frame Source code in RAiDER/losreader.py 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 def getZenithLookVecs ( lats , lons , heights ): ''' Returns look vectors when Zenith is used. Args: lats/lons/heights (ndarray): - Numpy arrays containing WGS-84 target locations Returns: zenLookVecs (ndarray): - (in_shape) x 3 unit look vectors in an ECEF reference frame ''' x = np . cos ( np . radians ( lats )) * np . cos ( np . radians ( lons )) y = np . cos ( np . radians ( lats )) * np . sin ( np . radians ( lons )) z = np . sin ( np . radians ( lats )) return np . stack ([ x , y , z ], axis =- 1 )","title":"getZenithLookVecs()"},{"location":"reference/#RAiDER.losreader.get_radar_pos","text":"Calculate the coordinate of the sensor in ECEF at the time corresponding to ***. orb: isce3.core.Orbit - Nt x 7 matrix of statevectors: [t x y z vx vy vz] llh: ndarray - position of the target in LLH out: str - either lookangle or ecef for vector if out == \"lookangle\" los: ndarray - Satellite incidence angle sr: ndarray - Slant range in meters if out == \"ecef\" los_xyz: ndarray - Satellite LOS in ECEF from target to satellite targ_xyz: ndarray - Target XYZ positions in ECEF Source code in RAiDER/losreader.py 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 def get_radar_pos ( llh , orb , out = \"lookangle\" ): ''' Calculate the coordinate of the sensor in ECEF at the time corresponding to ***. Args: ---------- orb: isce3.core.Orbit - Nt x 7 matrix of statevectors: [t x y z vx vy vz] llh: ndarray - position of the target in LLH out: str - either lookangle or ecef for vector Returns: ------- if out == \"lookangle\" los: ndarray - Satellite incidence angle sr: ndarray - Slant range in meters if out == \"ecef\" los_xyz: ndarray - Satellite LOS in ECEF from target to satellite targ_xyz: ndarray - Target XYZ positions in ECEF ''' if out not in [ \"lookangle\" , \"ecef\" ]: raise ValueError ( f \"out kwarg must be lookangle or ecef - not { out } \" ) num_iteration = 30 residual_threshold = 1.0e-7 # Get xyz positions of targets here targ_xyz = np . stack ( lla2ecef ( llh [:, 1 ], llh [:, 0 ], llh [:, 2 ]), axis =- 1 ) # Get some isce3 constants for this inversion # TODO - Assuming right-looking for now elp = isce . core . Ellipsoid () dop = isce . core . LUT2d () look = isce . core . LookSide . Right # Iterate for each point # TODO - vectorize / parallelize sr = np . empty (( llh . shape [ 0 ],), dtype = np . float64 ) if out == \"lookangle\" : output = np . empty (( llh . shape [ 0 ],), dtype = np . float64 ) else : output = np . empty (( llh . shape [ 0 ], 3 ), dtype = np . float64 ) for ind , pt in enumerate ( llh ): if not any ( np . isnan ( pt )): # ISCE3 always uses xy convention inp = np . array ([ np . deg2rad ( pt [ 1 ]), np . deg2rad ( pt [ 0 ]), pt [ 2 ]]) # Local normal vector nv = elp . n_vector ( inp [ 0 ], inp [ 1 ]) # Wavelength does not matter for zero doppler try : aztime , slant_range = isce . geometry . geo2rdr ( inp , elp , orb , dop , 0.06 , look , threshold = residual_threshold , maxiter = num_iteration , delta_range = 10.0 ) sat_xyz , _ = orb . interpolate ( aztime ) sr [ ind ] = slant_range delta = sat_xyz - targ_xyz [ ind , :] if out == \"lookangle\" : # TODO - if we only ever need cos(lookang), # skip the arccos here and cos above delta = delta / np . linalg . norm ( delta ) output [ ind ] = np . rad2deg ( np . arccos ( np . dot ( delta , nv )) ) else : output [ ind , :] = ( sat_xyz - targ_xyz ) / slant_range except Exception as e : raise e sat_xyz [ ind , :] = np . nan sr [ ind ] = np . nan output [ ind , ... ] = np . nan else : sat_xyz [ ind , :] = np . nan sr [ ind ] = np . nan output [ ind , ... ] = np . nan # If lookangle is requested if out == \"lookangle\" : return output , sr elif out == \"ecef\" : return output , targ_xyz else : raise NotImplementedError ( \"Unexpected logic in get_radar_pos\" )","title":"get_radar_pos()"},{"location":"reference/#RAiDER.losreader.get_sv","text":"Read an LOS file and return orbital state vectors Parameters: Name Type Description Default los_file str user-passed file containing either look vectors or statevectors for the sensor required ref_time datetime User-requested datetime; if not encompassed by the orbit times will raise a ValueError required pad int number of seconds to keep around the requested time 3 * 60 Returns: Name Type Description svs list of ndarrays the times, x/y/z positions and velocities of the sensor for the given window around the reference time Source code in RAiDER/losreader.py 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 def get_sv ( los_file , ref_time , pad = 3 * 60 ): \"\"\" Read an LOS file and return orbital state vectors Args: los_file (str): - user-passed file containing either look vectors or statevectors for the sensor ref_time (datetime):- User-requested datetime; if not encompassed by the orbit times will raise a ValueError pad (int): - number of seconds to keep around the requested time Returns: svs (list of ndarrays): - the times, x/y/z positions and velocities of the sensor for the given window around the reference time \"\"\" try : svs = read_txt_file ( los_file ) except ValueError : try : svs = read_ESA_Orbit_file ( los_file ) except BaseException : try : svs = read_shelve ( los_file ) except BaseException : raise ValueError ( 'get_sv: I cannot parse the statevector file {} ' . format ( los_file ) ) if ref_time : idx = cut_times ( svs [ 0 ], ref_time , pad = pad ) svs = [ d [ idx ] for d in svs ] return svs","title":"get_sv()"},{"location":"reference/#RAiDER.losreader.inc_hd_to_enu","text":"Convert incidence and heading to line-of-sight vectors from the ground to the top of the troposphere. Parameters: Name Type Description Default incidence ndarray - incidence angle in deg from vertical required heading ndarray - heading angle in deg clockwise from north required lats/lons/heights ndarray - WGS84 ellipsoidal target (ground pixel) locations required Returns: Name Type Description LOS ndarray - (input_shape) x 3 array of unit look vectors in local ENU Algorithm referenced from http://earthdef.caltech.edu/boards/4/topics/327 Source code in RAiDER/losreader.py 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 def inc_hd_to_enu ( incidence , heading ): ''' Convert incidence and heading to line-of-sight vectors from the ground to the top of the troposphere. Args: incidence: ndarray - incidence angle in deg from vertical heading: ndarray - heading angle in deg clockwise from north lats/lons/heights: ndarray - WGS84 ellipsoidal target (ground pixel) locations Returns: LOS: ndarray - (input_shape) x 3 array of unit look vectors in local ENU Algorithm referenced from http://earthdef.caltech.edu/boards/4/topics/327 ''' if np . any ( incidence < 0 ): raise ValueError ( 'inc_hd_to_enu: Incidence angle cannot be less than 0' ) east = sind ( incidence ) * cosd ( heading + 90 ) north = sind ( incidence ) * sind ( heading + 90 ) up = cosd ( incidence ) return np . stack (( east , north , up ), axis =- 1 )","title":"inc_hd_to_enu()"},{"location":"reference/#RAiDER.losreader.read_ESA_Orbit_file","text":"Read orbit data from an orbit file supplied by ESA filename: str - string of the orbit filename Nt x 1 ndarray - a numpy vector with Nt elements containing time in python datetime x, y, z: Nt x 1 ndarrays - x/y/z positions of the sensor at the times t vx, vy, vz: Nt x 1 ndarrays - x/y/z velocities of the sensor at the times t Source code in RAiDER/losreader.py 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 def read_ESA_Orbit_file ( filename ): ''' Read orbit data from an orbit file supplied by ESA Args: ---------- filename: str - string of the orbit filename Returns: ------- t: Nt x 1 ndarray - a numpy vector with Nt elements containing time in python datetime x, y, z: Nt x 1 ndarrays - x/y/z positions of the sensor at the times t vx, vy, vz: Nt x 1 ndarrays - x/y/z velocities of the sensor at the times t ''' tree = ET . parse ( filename ) root = tree . getroot () data_block = root [ 1 ] numOSV = len ( data_block [ 0 ]) t = [] x = np . ones ( numOSV ) y = np . ones ( numOSV ) z = np . ones ( numOSV ) vx = np . ones ( numOSV ) vy = np . ones ( numOSV ) vz = np . ones ( numOSV ) for i , st in enumerate ( data_block [ 0 ]): t . append ( datetime . datetime . strptime ( st [ 1 ] . text , 'UTC=%Y-%m- %d T%H:%M:%S. %f ' ) ) x [ i ] = float ( st [ 4 ] . text ) y [ i ] = float ( st [ 5 ] . text ) z [ i ] = float ( st [ 6 ] . text ) vx [ i ] = float ( st [ 7 ] . text ) vy [ i ] = float ( st [ 8 ] . text ) vz [ i ] = float ( st [ 9 ] . text ) t = np . array ( t ) return [ t , x , y , z , vx , vy , vz ]","title":"read_ESA_Orbit_file()"},{"location":"reference/#RAiDER.losreader.read_txt_file","text":"Read a 7-column text file containing orbit statevectors. Time should be denoted as integer time in seconds since the reference epoch (user-requested time). Parameters: Name Type Description Default filename str user-supplied space-delimited text file with no header containing orbital statevectors as 7 columns: - time in seconds since the user-supplied epoch - x / y / z locations in ECEF cartesian coordinates - vx / vy / vz velocities in m/s in ECEF coordinates required Returns: Name Type Description svs list a length-7 list of numpy vectors containing the above variables Source code in RAiDER/losreader.py 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 def read_txt_file ( filename ): ''' Read a 7-column text file containing orbit statevectors. Time should be denoted as integer time in seconds since the reference epoch (user-requested time). Args: filename (str): - user-supplied space-delimited text file with no header containing orbital statevectors as 7 columns: - time in seconds since the user-supplied epoch - x / y / z locations in ECEF cartesian coordinates - vx / vy / vz velocities in m/s in ECEF coordinates Returns: svs (list): - a length-7 list of numpy vectors containing the above variables ''' t = list () x = list () y = list () z = list () vx = list () vy = list () vz = list () with open ( filename , 'r' ) as f : for line in f : try : parts = line . strip () . split () t_ = datetime . datetime . fromisoformat ( parts [ 0 ]) x_ , y_ , z_ , vx_ , vy_ , vz_ = [ float ( t ) for t in parts [ 1 :]] except ValueError : raise ValueError ( \"I need {} to be a 7 column text file, with \" . format ( filename ) + \"columns t, x, y, z, vx, vy, vz (Couldn't parse line \" + \" {} )\" . format ( repr ( line ))) t . append ( t_ ) x . append ( x_ ) y . append ( y_ ) z . append ( z_ ) vx . append ( vx_ ) vy . append ( vy_ ) vz . append ( vz_ ) if len ( t ) < 4 : raise ValueError ( 'read_txt_file: File {} does not have enough statevectors' . format ( filename )) return [ np . array ( a ) for a in [ t , x , y , z , vx , vy , vz ]]","title":"read_txt_file()"},{"location":"reference/#RAiDER.losreader.state_to_los","text":"Converts information from a state vector for a satellite orbit, given in terms of position and velocity, to line-of-sight information at each (lon,lat, height) coordinate requested by the user. svs - t, x, y, z, vx, vy, vz - time, position, and velocity in ECEF of the sensor llh_targets - lats, lons, heights - Ellipsoidal (WGS84) positions of target ground pixels LOS - * x 3 matrix of LOS unit vectors in ECEF ( not ENU) Example: import datetime import numpy from RAiDER.utilFcns import rio_open import RAiDER.losreader as losr lats, lons, heights = np.array([-76.1]), np.array([36.83]), np.array([0]) time = datetime.datetime(2018,11,12,23,0,0)","title":"state_to_los()"},{"location":"reference/#RAiDER.losreader.state_to_los--download-the-orbit-file-beforehand","text":"esa_orbit_file = 'S1A_OPER_AUX_POEORB_OPOD_20181203T120749_V20181112T225942_20181114T005942.EOF' svs = losr.read_ESA_Orbit_file(esa_orbit_file) LOS = losr.state_to_los(*svs, [lats, lons, heights], xyz) Source code in RAiDER/losreader.py 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 def state_to_los ( svs , llh_targets , out = \"lookangle\" ): ''' Converts information from a state vector for a satellite orbit, given in terms of position and velocity, to line-of-sight information at each (lon,lat, height) coordinate requested by the user. Args: ---------- svs - t, x, y, z, vx, vy, vz - time, position, and velocity in ECEF of the sensor llh_targets - lats, lons, heights - Ellipsoidal (WGS84) positions of target ground pixels Returns: ------- LOS - * x 3 matrix of LOS unit vectors in ECEF (*not* ENU) Example: >>> import datetime >>> import numpy >>> from RAiDER.utilFcns import rio_open >>> import RAiDER.losreader as losr >>> lats, lons, heights = np.array([-76.1]), np.array([36.83]), np.array([0]) >>> time = datetime.datetime(2018,11,12,23,0,0) >>> # download the orbit file beforehand >>> esa_orbit_file = 'S1A_OPER_AUX_POEORB_OPOD_20181203T120749_V20181112T225942_20181114T005942.EOF' >>> svs = losr.read_ESA_Orbit_file(esa_orbit_file) >>> LOS = losr.state_to_los(*svs, [lats, lons, heights], xyz) ''' if out not in [ \"lookangle\" , \"ecef\" ]: raise ValueError ( f \"Output type can be lookangle or ecef - not { out } \" ) # check the inputs if np . min ( svs . shape ) < 4 : raise RuntimeError ( 'state_to_los: At least 4 state vectors are required' ' for orbit interpolation' ) # Convert svs to isce3 orbit orb = isce . core . Orbit ([ isce . core . StateVector ( isce . core . DateTime ( row [ 0 ]), row [ 1 : 4 ], row [ 4 : 7 ] ) for row in svs ]) # Flatten the input array for convenience in_shape = llh_targets [ 0 ] . shape target_llh = np . stack ([ x . flatten () for x in llh_targets ], axis =- 1 ) Npts = len ( target_llh ) # Iterate through targets and compute LOS if out == \"lookangle\" : los_ang , slant_range = get_radar_pos ( target_llh , orb , out = \"lookangle\" ) los_factor = np . cos ( np . deg2rad ( los_ang )) . reshape ( in_shape ) return los_factor elif out == \"ecef\" : los_xyz , targ_xyz = get_radar_pos ( target_llh , orb , out = \"ecef\" ) return los_xyz , targ_xyz else : raise NotImplementedError ( \"Unexpected logic in state_to_los\" )","title":"download the orbit file beforehand"},{"location":"reference/#RAiDER.makePoints","text":"","title":"makePoints"},{"location":"reference/#RAiDER.makePoints.__file__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__file__"},{"location":"reference/#RAiDER.makePoints.__name__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__name__"},{"location":"reference/#RAiDER.makePoints.__package__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__package__"},{"location":"reference/#RAiDER.makePoints.__test__","text":"dict() -> new empty dictionary dict(mapping) -> new dictionary initialized from a mapping object's (key, value) pairs dict(iterable) -> new dictionary initialized as if via: d = {} for k, v in iterable: d[k] = v dict(**kwargs) -> new dictionary initialized with the name=value pairs in the keyword argument list. For example: dict(one=1, two=2)","title":"__test__"},{"location":"reference/#RAiDER.makePoints.makePoints0D","text":"Fast cython code to create the rays needed for ray-tracing. Inputs: max_len: maximum length of the rays Rays_SP: 1 x 3 numpy array of the location of the ground pixels in an earth-centered, earth-fixed coordinate system Rays_SLV: 1 x 3 numpy array of the look vectors pointing from the ground pixel to the sensor stepSize: Distance between points along the ray-path Output ray: a 3 x Npts array containing the rays tracing a path from the ground pixels, along the line-of-sight vectors, up to the maximum length specified.","title":"makePoints0D()"},{"location":"reference/#RAiDER.makePoints.makePoints1D","text":"Fast cython code to create the rays needed for ray-tracing. Inputs: max_len: maximum length of the rays Rays_SP: Nx x 3 numpy array of the location of the ground pixels in an earth-centered, earth-fixed coordinate system Rays_SLV: Nx x 3 numpy array of the look vectors pointing from the ground pixel to the sensor stepSize: Distance between points along the ray-path Output ray: a Nx x 3 x Npts array containing the rays tracing a path from the ground pixels, along the line-of-sight vectors, up to the maximum length specified.","title":"makePoints1D()"},{"location":"reference/#RAiDER.makePoints.makePoints2D","text":"Fast cython code to create the rays needed for ray-tracing. Inputs: max_len: maximum length of the rays Rays_SP: Nx x Ny x 3 numpy array of the location of the ground pixels in an earth-centered, earth-fixed coordinate system Rays_SLV: Nx x Ny x 3 numpy array of the look vectors pointing from the ground pixel to the sensor stepSize: Distance between points along the ray-path Output ray: a Nx x Ny x 3 x Npts array containing the rays tracing a path from the ground pixels, along the line-of-sight vectors, up to the maximum length specified.","title":"makePoints2D()"},{"location":"reference/#RAiDER.makePoints.makePoints3D","text":"Fast cython code to create the rays needed for ray-tracing Inputs: max_len: maximum length of the rays Rays_SP: Nx x Ny x Nz x 3 numpy array of the location of the ground pixels in an earth-centered, earth-fixed coordinate system Rays_SLV: Nx x Ny x Nz x 3 numpy array of the look vectors pointing from the ground pixel to the sensor stepSize: Distance between points along the ray-path Output ray: a Nx x Ny x Nz x 3 x Npts array containing the rays tracing a path from the ground pixels, along the line-of-sight vectors, up to the maximum length specified.","title":"makePoints3D()"},{"location":"reference/#RAiDER.models","text":"","title":"models"},{"location":"reference/#RAiDER.models.ecmwf","text":"","title":"ecmwf"},{"location":"reference/#RAiDER.models.ecmwf.ECMWF","text":"Bases: WeatherModel Implement ECMWF models Source code in RAiDER/models/ecmwf.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 class ECMWF ( WeatherModel ): ''' Implement ECMWF models ''' def __init__ ( self ): # initialize a weather model WeatherModel . __init__ ( self ) # model constants self . _k1 = 0.776 # [K/Pa] self . _k2 = 0.233 # [K/Pa] self . _k3 = 3.75e3 # [K^2/Pa] self . _time_res = 1 self . _lon_res = 0.2 self . _lat_res = 0.2 self . _proj = CRS . from_epsg ( 4326 ) self . _model_level_type = 'ml' # Default def setLevelType ( self , levelType ): '''Set the level type to model levels or pressure levels''' if levelType in [ 'ml' , 'pl' ]: self . _model_level_type = levelType else : raise RuntimeError ( 'Level type {} is not recognized' . format ( levelType )) if levelType == 'ml' : self . __model_levels__ () else : self . __pressure_levels__ () def __pressure_levels__ ( self ): self . _zlevels = np . flipud ( LEVELS_25_HEIGHTS ) self . _levels = len ( self . _zlevels ) def __model_levels__ ( self ): self . _levels = 137 self . _zlevels = np . flipud ( LEVELS_137_HEIGHTS ) self . _a = A_137_HRES self . _b = B_137_HRES def load_weather ( self , * args , ** kwargs ): ''' Consistent class method to be implemented across all weather model types. As a result of calling this method, all of the variables (x, y, z, p, q, t, wet_refractivity, hydrostatic refractivity, e) should be fully populated. ''' self . _load_model_level ( * self . files ) def _load_model_level ( self , fname ): # read data from netcdf file lats , lons , xs , ys , t , q , lnsp , z = self . _makeDataCubes ( fname , verbose = False ) # ECMWF appears to give me this backwards if lats [ 0 ] > lats [ 1 ]: z = z [:: - 1 ] lnsp = lnsp [:: - 1 ] t = t [:, :: - 1 ] q = q [:, :: - 1 ] lats = lats [:: - 1 ] # Lons is usually ok, but we'll throw in a check to be safe if lons [ 0 ] > lons [ 1 ]: z = z [ ... , :: - 1 ] lnsp = lnsp [ ... , :: - 1 ] t = t [ ... , :: - 1 ] q = q [ ... , :: - 1 ] lons = lons [:: - 1 ] # pyproj gets fussy if the latitude is wrong, plus our # interpolator isn't clever enough to pick up on the fact that # they are the same lons [ lons > 180 ] -= 360 self . _t = t self . _q = q geo_hgt , pres , hgt = self . _calculategeoh ( z , lnsp ) # re-assign lons, lats to match heights _lons = np . broadcast_to ( lons [ np . newaxis , np . newaxis , :], hgt . shape ) _lats = np . broadcast_to ( lats [ np . newaxis , :, np . newaxis ], hgt . shape ) # ys is latitude self . _get_heights ( _lats , hgt ) h = self . _zs . copy () # We want to support both pressure levels and true pressure grids. # If the shape has one dimension, we'll scale it up to act as a # grid, otherwise we'll leave it alone. if len ( pres . shape ) == 1 : self . _p = np . broadcast_to ( pres [:, np . newaxis , np . newaxis ], self . _zs . shape ) else : self . _p = pres # Re-structure everything from (heights, lats, lons) to (lons, lats, heights) self . _p = np . transpose ( self . _p , ( 1 , 2 , 0 )) self . _t = np . transpose ( self . _t , ( 1 , 2 , 0 )) self . _q = np . transpose ( self . _q , ( 1 , 2 , 0 )) h = np . transpose ( h , ( 1 , 2 , 0 )) self . _lats = np . transpose ( _lats , ( 1 , 2 , 0 )) self . _lons = np . transpose ( _lons , ( 1 , 2 , 0 )) # Flip all the axis so that zs are in order from bottom to top # lats / lons are simply replicated to all heights so they don't need flipped self . _p = np . flip ( self . _p , axis = 2 ) self . _t = np . flip ( self . _t , axis = 2 ) self . _q = np . flip ( self . _q , axis = 2 ) self . _ys = self . _lats . copy () self . _xs = self . _lons . copy () self . _zs = np . flip ( h , axis = 2 ) def _fetch ( self , out ): ''' Fetch a weather model from ECMWF ''' # bounding box plus a buffer lat_min , lat_max , lon_min , lon_max = self . _ll_bounds # execute the search at ECMWF try : self . _get_from_ecmwf ( lat_min , lat_max , self . _lat_res , lon_min , lon_max , self . _lon_res , self . _time , out ) except Exception as e : logger . warning ( 'Query point bounds are {} / {} / {} / {} ' . format ( lat_min , lat_max , lon_min , lon_max )) logger . warning ( 'Query time: {} ' . format ( self . _time )) logger . exception ( e ) def _get_from_ecmwf ( self , lat_min , lat_max , lat_step , lon_min , lon_max , lon_step , time , out ): import ecmwfapi server = ecmwfapi . ECMWFDataServer () corrected_date = util . round_date ( time , datetime . timedelta ( hours = 6 )) server . retrieve ({ \"class\" : self . _classname , # ERA-Interim 'dataset' : self . _dataset , \"expver\" : \" {} \" . format ( self . _expver ), # They warn me against all, but it works well \"levelist\" : 'all' , \"levtype\" : \"ml\" , # Model levels \"param\" : \"lnsp/q/z/t\" , # Necessary variables \"stream\" : \"oper\" , # date: Specify a single date as \"2015-08-01\" or a period as # \"2015-08-01/to/2015-08-31\". \"date\" : datetime . datetime . strftime ( corrected_date , \"%Y-%m- %d \" ), # type: Use an (analysis) unless you have a particular reason to # use fc (forecast). \"type\" : \"an\" , # time: With type=an, time can be any of # \"00:00:00/06:00:00/12:00:00/18:00:00\". With type=fc, time can # be any of \"00:00:00/12:00:00\", \"time\" : datetime . time . strftime ( corrected_date . time (), \"%H:%M:%S\" ), # step: With type=an, step is always \"0\". With type=fc, step can # be any of \"3/6/9/12\". \"step\" : \"0\" , # grid: Only regular lat/lon grids are supported. \"grid\" : ' {} / {} ' . format ( lat_step , lon_step ), \"area\" : ' {} / {} / {} / {} ' . format ( lat_max , lon_min , lat_min , lon_max ), # area: N/W/S/E \"format\" : \"netcdf\" , \"resol\" : \"av\" , \"target\" : out , # target: the name of the output file. }) def _get_from_cds ( self , lat_min , lat_max , lon_min , lon_max , acqTime , outname ): import cdsapi c = cdsapi . Client ( verify = 0 ) if self . _model_level_type == 'pl' : var = [ 'z' , 'q' , 't' ] levType = 'pressure_level' else : var = \"129/130/133/152\" # 'lnsp', 'q', 'z', 't' levType = 'model_level' bbox = [ lat_max , lon_min , lat_min , lon_max ] # round to the closest legal time corrected_date = util . round_date ( acqTime , datetime . timedelta ( hours = self . _time_res )) # I referenced https://confluence.ecmwf.int/display/CKB/How+to+download+ERA5 dataDict = { \"class\" : \"ea\" , \"expver\" : \"1\" , \"levelist\" : 'all' , \"levtype\" : \" {} \" . format ( self . _model_level_type ), # 'ml' for model levels or 'pl' for pressure levels 'param' : var , \"stream\" : \"oper\" , \"type\" : \"an\" , \"date\" : \" {} \" . format ( corrected_date . strftime ( '%Y-%m- %d ' )), \"time\" : \" {} \" . format ( datetime . time . strftime ( corrected_date . time (), '%H:%M' )), # step: With type=an, step is always \"0\". With type=fc, step can # be any of \"3/6/9/12\". \"step\" : \"0\" , \"area\" : bbox , \"grid\" : [ 0.25 , .25 ], \"format\" : \"netcdf\" } try : c . retrieve ( 'reanalysis-era5-complete' , dataDict , outname ) except Exception as e : logger . warning ( 'Query point bounds are {} / {} latitude and {} / {} longitude' . format ( lat_min , lat_max , lon_min , lon_max )) logger . warning ( 'Query time: {} ' . format ( acqTime )) logger . exception ( e ) raise Exception def _download_ecmwf ( self , lat_min , lat_max , lat_step , lon_min , lon_max , lon_step , time , out ): from ecmwfapi import ECMWFService server = ECMWFService ( \"mars\" ) # round to the closest legal time corrected_date = util . round_date ( time , datetime . timedelta ( hours = self . _time_res )) if self . _model_level_type == 'ml' : param = \"129/130/133/152\" else : param = \"129.128/130.128/133.128/152\" server . execute ( { 'class' : self . _classname , 'dataset' : self . _dataset , 'expver' : \" {} \" . format ( self . _expver ), 'resol' : \"av\" , 'stream' : \"oper\" , 'type' : \"an\" , 'levelist' : \"all\" , 'levtype' : \" {} \" . format ( self . _model_level_type ), 'param' : param , 'date' : datetime . datetime . strftime ( corrected_date , \"%Y-%m- %d \" ), 'time' : \" {} \" . format ( datetime . time . strftime ( corrected_date . time (), '%H:%M' )), 'step' : \"0\" , 'grid' : \" {} / {} \" . format ( lon_step , lat_step ), 'area' : \" {} / {} / {} / {} \" . format ( lat_max , util . floorish ( lon_min , 0.1 ), util . floorish ( lat_min , 0.1 ), lon_max ), 'format' : \"netcdf\" , }, out ) def _load_pressure_level ( self , filename , * args , ** kwargs ): with xr . open_dataset ( filename ) as block : # Pull the data z = np . squeeze ( block [ 'z' ] . values ) t = np . squeeze ( block [ 't' ] . values ) q = np . squeeze ( block [ 'q' ] . values ) lats = np . squeeze ( block . latitude . values ) lons = np . squeeze ( block . longitude . values ) levels = np . squeeze ( block . level . values ) * 100 z = np . flip ( z , axis = 1 ) # ECMWF appears to give me this backwards if lats [ 0 ] > lats [ 1 ]: z = z [:: - 1 ] t = t [:, :: - 1 ] q = q [:, :: - 1 ] lats = lats [:: - 1 ] # Lons is usually ok, but we'll throw in a check to be safe if lons [ 0 ] > lons [ 1 ]: z = z [ ... , :: - 1 ] t = t [ ... , :: - 1 ] q = q [ ... , :: - 1 ] lons = lons [:: - 1 ] # pyproj gets fussy if the latitude is wrong, plus our # interpolator isn't clever enough to pick up on the fact that # they are the same lons [ lons > 180 ] -= 360 self . _t = t self . _q = q geo_hgt = z / self . _g0 # re-assign lons, lats to match heights _lons = np . broadcast_to ( lons [ np . newaxis , np . newaxis , :], geo_hgt . shape ) _lats = np . broadcast_to ( lats [ np . newaxis , :, np . newaxis ], geo_hgt . shape ) # correct heights for latitude self . _get_heights ( _lats , geo_hgt ) self . _p = np . broadcast_to ( levels [:, np . newaxis , np . newaxis ], self . _zs . shape ) # Re-structure everything from (heights, lats, lons) to (lons, lats, heights) self . _p = np . transpose ( self . _p ) self . _t = np . transpose ( self . _t ) self . _q = np . transpose ( self . _q ) self . _lats = np . transpose ( _lats ) self . _lons = np . transpose ( _lons ) self . _ys = self . _lats . copy () self . _xs = self . _lons . copy () self . _zs = np . transpose ( self . _zs ) # check this # data cube format should be lats,lons,heights self . _lats = self . _lats . swapaxes ( 0 , 1 ) self . _lons = self . _lons . swapaxes ( 0 , 1 ) self . _xs = self . _xs . swapaxes ( 0 , 1 ) self . _ys = self . _ys . swapaxes ( 0 , 1 ) self . _zs = self . _zs . swapaxes ( 0 , 1 ) self . _p = self . _p . swapaxes ( 0 , 1 ) self . _q = self . _q . swapaxes ( 0 , 1 ) self . _t = self . _t . swapaxes ( 0 , 1 ) # For some reason z is opposite the others self . _p = np . flip ( self . _p , axis = 2 ) self . _t = np . flip ( self . _t , axis = 2 ) self . _q = np . flip ( self . _q , axis = 2 ) def _makeDataCubes ( self , fname , verbose = False ): ''' Create a cube of data representing temperature and relative humidity at specified pressure levels ''' # get ll_bounds S , N , W , E = self . _ll_bounds with xr . open_dataset ( fname ) as ds : ds = ds . assign_coords ( longitude = ((( ds . longitude + 180 ) % 360 ) - 180 )) # mask based on query bounds m1 = ( S <= ds . latitude ) & ( N >= ds . latitude ) m2 = ( W <= ds . longitude ) & ( E >= ds . longitude ) block = ds . where ( m1 & m2 , drop = True ) # Pull the data z = np . squeeze ( block [ 'z' ] . values )[ 0 , ... ] t = np . squeeze ( block [ 't' ] . values ) q = np . squeeze ( block [ 'q' ] . values ) lnsp = np . squeeze ( block [ 'lnsp' ] . values )[ 0 , ... ] lats = np . squeeze ( block . latitude . values ) lons = np . squeeze ( block . longitude . values ) xs = lons . copy () ys = lats . copy () if z . size == 0 : raise RuntimeError ( 'There is no data in z, ' 'you may have a problem with your mask' ) return lats , lons , xs , ys , t , q , lnsp , z","title":"ECMWF"},{"location":"reference/#RAiDER.models.ecmwf.ECMWF.load_weather","text":"Consistent class method to be implemented across all weather model types. As a result of calling this method, all of the variables (x, y, z, p, q, t, wet_refractivity, hydrostatic refractivity, e) should be fully populated. Source code in RAiDER/models/ecmwf.py 64 65 66 67 68 69 70 71 def load_weather ( self , * args , ** kwargs ): ''' Consistent class method to be implemented across all weather model types. As a result of calling this method, all of the variables (x, y, z, p, q, t, wet_refractivity, hydrostatic refractivity, e) should be fully populated. ''' self . _load_model_level ( * self . files )","title":"load_weather()"},{"location":"reference/#RAiDER.models.ecmwf.ECMWF.setLevelType","text":"Set the level type to model levels or pressure levels Source code in RAiDER/models/ecmwf.py 42 43 44 45 46 47 48 49 50 51 52 def setLevelType ( self , levelType ): '''Set the level type to model levels or pressure levels''' if levelType in [ 'ml' , 'pl' ]: self . _model_level_type = levelType else : raise RuntimeError ( 'Level type {} is not recognized' . format ( levelType )) if levelType == 'ml' : self . __model_levels__ () else : self . __pressure_levels__ ()","title":"setLevelType()"},{"location":"reference/#RAiDER.models.era5","text":"","title":"era5"},{"location":"reference/#RAiDER.models.era5.ERA5","text":"Bases: ECMWF Source code in RAiDER/models/era5.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 class ERA5 ( ECMWF ): # I took this from # https://www.ecmwf.int/en/forecasts/documentation-and-support/137-model-levels. def __init__ ( self ): ECMWF . __init__ ( self ) self . _humidityType = 'q' self . _expver = '0001' self . _classname = 'ea' self . _dataset = 'era5' self . _Name = 'ERA-5' self . _proj = CRS . from_epsg ( 4326 ) # Tuple of min/max years where data is available. self . _valid_range = ( datetime . datetime ( 1950 , 1 , 1 ), \"Present\" ) # Availability lag time in days self . _lag_time = datetime . timedelta ( days = 30 ) # Default, need to change to ml self . setLevelType ( 'pl' ) def _fetch ( self , out ): ''' Fetch a weather model from ECMWF ''' # bounding box plus a buffer lat_min , lat_max , lon_min , lon_max = self . _ll_bounds time = self . _time # execute the search at ECMWF try : self . _get_from_cds ( lat_min , lat_max , lon_min , lon_max , time , out ) except Exception as e : logger . warning ( e ) raise RuntimeError ( 'Could not access or download from the CDS API' ) def load_weather ( self , * args , ** kwargs ): '''Load either pressure or model level data''' if self . _model_level_type == 'pl' : self . _load_pressure_level ( * self . files , * args , ** kwargs ) elif self . _model_level_type == 'ml' : self . _load_model_level ( * self . files , * args , ** kwargs ) else : raise RuntimeError ( ' {} is not a valid model type' . format ( self . _model_level_type ) )","title":"ERA5"},{"location":"reference/#RAiDER.models.era5.ERA5.load_weather","text":"Load either pressure or model level data Source code in RAiDER/models/era5.py 46 47 48 49 50 51 52 53 54 55 def load_weather ( self , * args , ** kwargs ): '''Load either pressure or model level data''' if self . _model_level_type == 'pl' : self . _load_pressure_level ( * self . files , * args , ** kwargs ) elif self . _model_level_type == 'ml' : self . _load_model_level ( * self . files , * args , ** kwargs ) else : raise RuntimeError ( ' {} is not a valid model type' . format ( self . _model_level_type ) )","title":"load_weather()"},{"location":"reference/#RAiDER.models.generateGACOSVRT","text":"","title":"generateGACOSVRT"},{"location":"reference/#RAiDER.models.generateGACOSVRT.convertAllFiles","text":"convert all RSC files to VRT files contained in dirLoc Source code in RAiDER/models/generateGACOSVRT.py 45 46 47 48 49 50 51 52 def convertAllFiles ( dirLoc ): ''' convert all RSC files to VRT files contained in dirLoc ''' import glob files = glob . glob ( '*.rsc' ) for f in files : makeVRT ( f )","title":"convertAllFiles()"},{"location":"reference/#RAiDER.models.generateGACOSVRT.makeVRT","text":"Use an RSC file to create a GDAL-compatible VRT file for opening GACOS weather model files Source code in RAiDER/models/generateGACOSVRT.py 6 7 8 9 10 11 12 def makeVRT ( filename , dtype = 'Float32' ): ''' Use an RSC file to create a GDAL-compatible VRT file for opening GACOS weather model files ''' fields = readRSC ( filename ) string = vrtStr ( fields [ 'XMAX' ], fields [ 'YMAX' ], fields [ 'X_FIRST' ], fields [ 'Y_FIRST' ], fields [ 'X_STEP' ], fields [ 'Y_STEP' ], filename . replace ( '.rsc' , '' ), dtype = dtype ) writeStringToFile ( string , filename . replace ( '.rsc' , '' ) . replace ( '.ztd' , '' ) + '.vrt' )","title":"makeVRT()"},{"location":"reference/#RAiDER.models.generateGACOSVRT.writeStringToFile","text":"Write a string to a VRT file Source code in RAiDER/models/generateGACOSVRT.py 15 16 17 18 19 20 def writeStringToFile ( string , filename ): ''' Write a string to a VRT file ''' with open ( filename , 'w' ) as f : f . write ( string )","title":"writeStringToFile()"},{"location":"reference/#RAiDER.models.gmao","text":"","title":"gmao"},{"location":"reference/#RAiDER.models.gmao.GMAO","text":"Bases: WeatherModel Source code in RAiDER/models/gmao.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 class GMAO ( WeatherModel ): # I took this from GMAO model level weblink # https://opendap.nccs.nasa.gov/dods/GEOS-5/fp/0.25_deg/assim/inst3_3d_asm_Nv def __init__ ( self ): # initialize a weather model WeatherModel . __init__ ( self ) self . _humidityType = 'q' self . _model_level_type = 'ml' # Default, pressure levels are 'pl' self . _classname = 'gmao' self . _dataset = 'gmao' # Tuple of min/max years where data is available. self . _valid_range = ( dt . datetime ( 2014 , 2 , 20 ), \"Present\" ) self . _lag_time = dt . timedelta ( hours = 24.0 ) # Availability lag time in hours # model constants self . _k1 = 0.776 # [K/Pa] self . _k2 = 0.233 # [K/Pa] self . _k3 = 3.75e3 # [K^2/Pa] self . _time_res = 1 # horizontal grid spacing self . _lat_res = 0.25 self . _lon_res = 0.3125 self . _x_res = 0.3125 self . _y_res = 0.25 self . _zlevels = np . flipud ( LEVELS_137_HEIGHTS ) self . _Name = 'GMAO' self . files = None self . _bounds = None # Projection self . _proj = CRS . from_epsg ( 4326 ) def _fetch ( self , out ): ''' Fetch weather model data from GMAO ''' time = self . _time # calculate the array indices for slicing the GMAO variable arrays lat_min_ind = int (( self . _ll_bounds [ 0 ] - ( - 90.0 )) / self . _lat_res ) lat_max_ind = int (( self . _ll_bounds [ 1 ] - ( - 90.0 )) / self . _lat_res ) lon_min_ind = int (( self . _ll_bounds [ 2 ] - ( - 180.0 )) / self . _lon_res ) lon_max_ind = int (( self . _ll_bounds [ 3 ] - ( - 180.0 )) / self . _lon_res ) T0 = dt . datetime ( 2017 , 12 , 1 , 0 , 0 , 0 ) # round time to nearest third hour time1 = time time = round_time ( time , 3 * 60 * 60 ) if not time1 == time : logger . warning ( 'Rounded given hour from %d to %d ' , time1 . hour , time . hour ) DT = time - T0 time_ind = int ( DT . total_seconds () / 3600.0 / 3.0 ) ml_min = 0 ml_max = 71 if time >= T0 : # open the dataset and pull the data url = 'https://opendap.nccs.nasa.gov/dods/GEOS-5/fp/0.25_deg/assim/inst3_3d_asm_Nv' session = pydap . cas . urs . setup_session ( 'username' , 'password' , check_url = url ) ds = pydap . client . open_url ( url , session = session ) qv = ds [ 'qv' ] . array [ time_ind , ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 ) ] . data [ 0 ] p = ds [ 'pl' ] . array [ time_ind , ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 ) ] . data [ 0 ] t = ds [ 't' ] . array [ time_ind , ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 ) ] . data [ 0 ] h = ds [ 'h' ] . array [ time_ind , ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 ) ] . data [ 0 ] else : root = 'https://portal.nccs.nasa.gov/datashare/gmao/geos-fp/das/Y {} /M {:02d} /D {:02d} ' base = f 'GEOS.fp.asm.inst3_3d_asm_Nv. { time . strftime ( \"%Y%m %d \" ) } _ { time . hour : 02 } 00.V01.nc4' url = f ' { root . format ( time . year , time . month , time . day ) } / { base } ' f = ' {} _raw {} ' . format ( * os . path . splitext ( out )) if not os . path . exists ( f ): logger . info ( 'Fetching URL: %s ' , url ) session = requests_retry_session () resp = session . get ( url , stream = True ) assert resp . ok , f 'Could not access url for time: { time } ' with open ( f , 'wb' ) as fh : shutil . copyfileobj ( resp . raw , fh ) else : logger . warning ( 'Weather model already exists, skipping download' ) with h5py . File ( f , 'r' ) as ds : q = ds [ 'QV' ][ 0 , :, lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )] p = ds [ 'PL' ][ 0 , :, lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )] t = ds [ 'T' ][ 0 , :, lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )] h = ds [ 'H' ][ 0 , :, lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )] os . remove ( f ) lats = np . arange ( ( - 90 + lat_min_ind * self . _lat_res ), ( - 90 + ( lat_max_ind + 1 ) * self . _lat_res ), self . _lat_res ) lons = np . arange ( ( - 180 + lon_min_ind * self . _lon_res ), ( - 180 + ( lon_max_ind + 1 ) * self . _lon_res ), self . _lon_res ) try : # Note that lat/lon gets written twice for GMAO because they are the same as y/x writeWeatherVars2NETCDF4 ( self , lats , lons , h , qv , p , t , outName = out ) except Exception : logger . exception ( \"Unable to save weathermodel to file\" ) def load_weather ( self , f = None ): ''' Consistent class method to be implemented across all weather model types. As a result of calling this method, all of the variables (x, y, z, p, q, t, wet_refractivity, hydrostatic refractivity, e) should be fully populated. ''' if f is None : f = self . files [ 0 ] self . _load_model_level ( f ) def _load_model_level ( self , filename ): ''' Get the variables from the GMAO link using OpenDAP ''' # adding the import here should become absolute when transition to netcdf from netCDF4 import Dataset with Dataset ( filename , mode = 'r' ) as f : lons = np . array ( f . variables [ 'x' ][:]) lats = np . array ( f . variables [ 'y' ][:]) h = np . array ( f . variables [ 'H' ][:]) q = np . array ( f . variables [ 'QV' ][:]) p = np . array ( f . variables [ 'PL' ][:]) t = np . array ( f . variables [ 'T' ][:]) # restructure the 3-D lat/lon/h in regular grid _lons = np . broadcast_to ( lons [ np . newaxis , np . newaxis , :], t . shape ) _lats = np . broadcast_to ( lats [ np . newaxis , :, np . newaxis ], t . shape ) # Re-structure everything from (heights, lats, lons) to (lons, lats, heights) p = np . transpose ( p ) q = np . transpose ( q ) t = np . transpose ( t ) h = np . transpose ( h ) _lats = np . transpose ( _lats ) _lons = np . transpose ( _lons ) # check this # data cube format should be lats,lons,heights p = p . swapaxes ( 0 , 1 ) q = q . swapaxes ( 0 , 1 ) t = t . swapaxes ( 0 , 1 ) h = h . swapaxes ( 0 , 1 ) _lats = _lats . swapaxes ( 0 , 1 ) _lons = _lons . swapaxes ( 0 , 1 ) # For some reason z is opposite the others p = np . flip ( p , axis = 2 ) q = np . flip ( q , axis = 2 ) t = np . flip ( t , axis = 2 ) h = np . flip ( h , axis = 2 ) _lats = np . flip ( _lats , axis = 2 ) _lons = np . flip ( _lons , axis = 2 ) # assign the regular-grid (lat/lon/h) variables self . _p = p self . _q = q self . _t = t self . _lats = _lats self . _lons = _lons self . _xs = _lons self . _ys = _lats self . _zs = h","title":"GMAO"},{"location":"reference/#RAiDER.models.gmao.GMAO.load_weather","text":"Consistent class method to be implemented across all weather model types. As a result of calling this method, all of the variables (x, y, z, p, q, t, wet_refractivity, hydrostatic refractivity, e) should be fully populated. Source code in RAiDER/models/gmao.py 151 152 153 154 155 156 157 158 159 160 def load_weather ( self , f = None ): ''' Consistent class method to be implemented across all weather model types. As a result of calling this method, all of the variables (x, y, z, p, q, t, wet_refractivity, hydrostatic refractivity, e) should be fully populated. ''' if f is None : f = self . files [ 0 ] self . _load_model_level ( f )","title":"load_weather()"},{"location":"reference/#RAiDER.models.hres","text":"","title":"hres"},{"location":"reference/#RAiDER.models.hres.HRES","text":"Bases: ECMWF Implement ECMWF models Source code in RAiDER/models/hres.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 class HRES ( ECMWF ): ''' Implement ECMWF models ''' def __init__ ( self , level_type = 'ml' ): # initialize a weather model WeatherModel . __init__ ( self ) # model constants self . _k1 = 0.776 # [K/Pa] self . _k2 = 0.233 # [K/Pa] self . _k3 = 3.75e3 # [K^2/Pa] self . _time_res = 6 # 9 km horizontal grid spacing. This is only used for extending the download-buffer, i.e. not in subsequent processing. self . _lon_res = 9. / 111 # 0.08108115 self . _lat_res = 9. / 111 # 0.08108115 self . _x_res = 9. / 111 # 0.08108115 self . _y_res = 9. / 111 # 0.08108115 self . _humidityType = 'q' # Default, pressure levels are 'pl' self . _expver = '1' self . _classname = 'od' self . _dataset = 'hres' self . _Name = 'HRES' self . _proj = CRS . from_epsg ( 4326 ) # Tuple of min/max years where data is available. self . _valid_range = ( datetime . datetime ( 1983 , 4 , 20 ), \"Present\" ) # Availability lag time in days self . _lag_time = datetime . timedelta ( hours = 6 ) self . setLevelType ( 'ml' ) def update_a_b ( self ): # Before 2013-06-26, there were only 91 model levels. The mapping coefficients below are extracted # based on https://www.ecmwf.int/en/forecasts/documentation-and-support/91-model-levels self . _levels = 91 self . _zlevels = np . flipud ( LEVELS_91_HEIGHTS ) self . _a = A_91_HRES self . _b = B_91_HRES def load_weather ( self , filename = None ): ''' Consistent class method to be implemented across all weather model types. As a result of calling this method, all of the variables (x, y, z, p, q, t, wet_refractivity, hydrostatic refractivity, e) should be fully populated. ''' if filename is None : filename = self . files [ 0 ] if self . _model_level_type == 'ml' : if ( self . _time < datetime . datetime ( 2013 , 6 , 26 , 0 , 0 , 0 )): self . update_a_b () self . _load_model_level ( filename ) elif self . _model_level_type == 'pl' : self . _load_pressure_levels ( filename ) def _fetch ( self , out ): ''' Fetch a weather model from ECMWF ''' # bounding box plus a buffer lat_min , lat_max , lon_min , lon_max = self . _ll_bounds time = self . _time if ( time < datetime . datetime ( 2013 , 6 , 26 , 0 , 0 , 0 )): self . update_a_b () # execute the search at ECMWF self . _download_ecmwf ( lat_min , lat_max , self . _lat_res , lon_min , lon_max , self . _lon_res , time , out )","title":"HRES"},{"location":"reference/#RAiDER.models.hres.HRES.load_weather","text":"Consistent class method to be implemented across all weather model types. As a result of calling this method, all of the variables (x, y, z, p, q, t, wet_refractivity, hydrostatic refractivity, e) should be fully populated. Source code in RAiDER/models/hres.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def load_weather ( self , filename = None ): ''' Consistent class method to be implemented across all weather model types. As a result of calling this method, all of the variables (x, y, z, p, q, t, wet_refractivity, hydrostatic refractivity, e) should be fully populated. ''' if filename is None : filename = self . files [ 0 ] if self . _model_level_type == 'ml' : if ( self . _time < datetime . datetime ( 2013 , 6 , 26 , 0 , 0 , 0 )): self . update_a_b () self . _load_model_level ( filename ) elif self . _model_level_type == 'pl' : self . _load_pressure_levels ( filename )","title":"load_weather()"},{"location":"reference/#RAiDER.models.hrrr","text":"","title":"hrrr"},{"location":"reference/#RAiDER.models.hrrr.HRRR","text":"Bases: WeatherModel Source code in RAiDER/models/hrrr.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 class HRRR ( WeatherModel ): def __init__ ( self ): # initialize a weather model super () . __init__ () self . _humidityType = 'q' self . _model_level_type = 'pl' # Default, pressure levels are 'pl' self . _expver = '0001' self . _classname = 'hrrr' self . _dataset = 'hrrr' self . _time_res = 1 # Tuple of min/max years where data is available. self . _valid_range = ( datetime . datetime ( 2016 , 7 , 15 ), \"Present\" ) self . _lag_time = datetime . timedelta ( hours = 3 ) # Availability lag time in days # model constants: TODO: need to update/double-check these self . _k1 = 0.776 # [K/Pa] self . _k2 = 0.233 # [K/Pa] self . _k3 = 3.75e3 # [K^2/Pa] # 3 km horizontal grid spacing self . _lat_res = 3. / 111 self . _lon_res = 3. / 111 self . _x_res = 3. self . _y_res = 3. self . _Nproc = 1 self . _Name = 'HRRR' self . _Npl = 0 self . files = None self . _bounds = None self . _zlevels = np . flipud ( LEVELS_137_HEIGHTS ) # Projection # See https://github.com/blaylockbk/pyBKB_v2/blob/master/demos/HRRR_earthRelative_vs_gridRelative_winds.ipynb and code lower down # '262.5:38.5:38.5:38.5 237.280472:1799:3000.00 21.138123:1059:3000.00' # 'lov:latin1:latin2:latd lon1:nx:dx lat1:ny:dy' # LCC parameters lon0 = 262.5 lat0 = 38.5 lat1 = 38.5 lat2 = 38.5 x0 = 0 y0 = 0 earth_radius = 6371229 p1 = CRS ( f '+proj=lcc +lat_1= { lat1 } +lat_2= { lat2 } +lat_0= { lat0 } ' \\ f '+lon_0= { lon0 } +x_0= { x0 } +y_0= { y0 } +a= { earth_radius } ' \\ f '+b= { earth_radius } +units=m +no_defs' ) self . _proj = p1 def _fetch ( self , out ): ''' Fetch weather model data from HRRR ''' # bounding box plus a buffer time = self . _time self . files = self . _download_hrrr_file ( time , out ) def load_weather ( self , * args , filename = None , ** kwargs ): ''' Load a weather model into a python weatherModel object, from self.files if no filename is passed. ''' if filename is None : filename = self . files # read data from grib file try : ds = xarray . open_dataset ( filename , engine = 'cfgrib' ) except EOFError : ds = xarray . open_dataset ( filename , engine = 'netcdf4' ) pl = np . array ([ self . _convertmb2Pa ( p ) for p in ds . levels . values ]) xArr = ds [ 'x' ] . values yArr = ds [ 'y' ] . values lats = ds [ 'lats' ] . values lons = ds [ 'lons' ] . values temps = ds [ 't' ] . values . transpose ( 1 , 2 , 0 ) qs = ds [ 'q' ] . values . transpose ( 1 , 2 , 0 ) geo_hgt = ds [ 'z' ] . values . transpose ( 1 , 2 , 0 ) Ny , Nx = lats . shape lons [ lons > 180 ] -= 360 # data cube format should be lats,lons,heights _xs = np . broadcast_to ( xArr [ np . newaxis , :, np . newaxis ], geo_hgt . shape ) _ys = np . broadcast_to ( yArr [:, np . newaxis , np . newaxis ], geo_hgt . shape ) _lons = np . broadcast_to ( lons [ ... , np . newaxis ], geo_hgt . shape ) _lats = np . broadcast_to ( lats [ ... , np . newaxis ], geo_hgt . shape ) # correct for latitude self . _get_heights ( _lats , geo_hgt ) self . _t = temps self . _q = qs self . _p = np . broadcast_to ( pl [ np . newaxis , np . newaxis , :], geo_hgt . shape ) self . _xs = _xs self . _ys = _ys self . _lats = _lats self . _lons = _lons def _makeDataCubes ( self , filename , out = None ): ''' Create a cube of data representing temperature and relative humidity at specified pressure levels ''' if out is None : out = filename # Get profile information from gdal prof = rio_profile ( str ( filename )) # Now get bounds S , N , W , E = self . _ll_bounds # open the dataset and pull the data ds = xarray . open_dataset ( filename , engine = 'cfgrib' , filter_by_keys = { 'typeOfLevel' : 'isobaricInhPa' }) # Determine mask based on query bounds lats = ds [ \"latitude\" ] . to_numpy () lons = ds [ \"longitude\" ] . to_numpy () levels = ds [ \"isobaricInhPa\" ] . to_numpy () shp = lats . shape lons [ lons > 180.0 ] -= 360. m1 = ( S <= lats ) & ( N >= lats ) & \\ ( W <= lons ) & ( E >= lons ) if np . sum ( m1 ) == 0 : raise RuntimeError ( 'Area of Interest has no overlap with the HRRR model available extent' ) # Y extent m1_y = np . argwhere ( np . sum ( m1 , axis = 1 ) != 0 ) y_min = max ( m1_y [ 0 ][ 0 ] - 2 , 0 ) y_max = min ( m1_y [ - 1 ][ 0 ] + 3 , shp [ 0 ]) m1_y = None # X extent m1_x = np . argwhere ( np . sum ( m1 , axis = 0 ) != 0 ) x_min = max ( m1_x [ 0 ][ 0 ] - 2 , 0 ) x_max = min ( m1_x [ - 1 ][ 0 ] + 3 , shp [ 1 ]) m1_x = None m1 = None # Coordinate arrays # HRRR GRIB has data in south-up format trans = prof [ \"transform\" ] . to_gdal () xArr = trans [ 0 ] + ( np . arange ( x_min , x_max ) + 0.5 ) * trans [ 1 ] yArr = trans [ 3 ] + ( prof [ \"height\" ] * trans [ 5 ]) - ( np . arange ( y_min , y_max ) + 0.5 ) * trans [ 5 ] lats = lats [ y_min : y_max , x_min : x_max ] lons = lons [ y_min : y_max , x_min : x_max ] # Data variables t = ds [ 't' ][:, y_min : y_max , x_min : x_max ] . to_numpy () z = ds [ 'gh' ][:, y_min : y_max , x_min : x_max ] . to_numpy () q = ds [ 'q' ][:, y_min : y_max , x_min : x_max ] . to_numpy () ds . close () # This section is purely for flipping arrays as needed # to match ECMWF reader is doing # All flips are views - no extra memory use # Lon -> From west to east # Lat -> From south to north (ECMWF reads north to south and flips it # load_weather) - So we do south to north here # Pres -> High to Los - (ECWMF does now to high and flips it back) - so # we do high to low # Data is currently in [levels, y, x] order flip_axes = [] if levels [ - 1 ] > levels [ 0 ]: flip_axes . append ( 0 ) levels = np . flip ( levels ) if lats [ 0 , 0 ] > lats [ - 1 , 0 ]: flip_axes . append ( 1 ) lats = np . flip ( lats , 0 ) yArr = np . flip ( yArr ) if lons [ 0 , 0 ] > lons [ 0 , - 1 ]: flip_axes . append ( 2 ) lons = np . flip ( lons , 1 ) xArr = np . flip ( xArr ) flip_axes = tuple ( flip_axes ) if flip_axes : t = np . flip ( t , flip_axes ) z = np . flip ( z , flip_axes ) q = np . flip ( q , flip_axes ) # Create output dataset ds_new = xarray . Dataset ( data_vars = dict ( t = ([ \"level\" , \"y\" , \"x\" ], t , { \"grid_mapping\" : \"proj\" }), z = ([ \"level\" , \"y\" , \"x\" ], z , { \"grid_mapping\" : \"proj\" }), q = ([ \"level\" , \"y\" , \"x\" ], q , { \"grid_mapping\" : \"proj\" }), lats = ([ \"y\" , \"x\" ], lats ), lons = ([ \"y\" , \"x\" ], lons ), ), coords = dict ( levels = ([ \"level\" ], levels , { \"units\" : \"millibars\" , \"long_name\" : \"pressure_level\" , \"axis\" : \"Z\" }), x = ([ \"x\" ], xArr , { \"standard_name\" : \"projection_x_coordinate\" , \"units\" : \"m\" , \"axis\" : \"X\" }), y = ([ \"y\" ], yArr , { \"standard_name\" : \"projection_y_coordinate\" , \"units\" : \"m\" , \"axis\" : \"Y\" }), ), attrs = { 'Conventions' : 'CF-1.7' , 'Weather_model' : 'HRRR' , } ) # Write projection of output ds_new [ \"proj\" ] = int () for k , v in self . _proj . to_cf () . items (): ds_new . proj . attrs [ k ] = v ds_new . to_netcdf ( out , engine = 'netcdf4' ) def _download_hrrr_file ( self , DATE , out , model = 'hrrr' , product = 'prs' , fxx = 0 , verbose = False ): ''' Download a HRRR model ''' H = Herbie ( DATE . strftime ( '%Y-%m- %d %H:%M' ), model = model , product = product , overwrite = False , verbose = True , save_dir = Path ( os . path . dirname ( out )), ) pf = H . download ( \":(SPFH|PRES|TMP|HGT):\" , verbose = verbose ) self . _makeDataCubes ( pf , out ) return out","title":"HRRR"},{"location":"reference/#RAiDER.models.hrrr.HRRR.load_weather","text":"Load a weather model into a python weatherModel object, from self.files if no filename is passed. Source code in RAiDER/models/hrrr.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def load_weather ( self , * args , filename = None , ** kwargs ): ''' Load a weather model into a python weatherModel object, from self.files if no filename is passed. ''' if filename is None : filename = self . files # read data from grib file try : ds = xarray . open_dataset ( filename , engine = 'cfgrib' ) except EOFError : ds = xarray . open_dataset ( filename , engine = 'netcdf4' ) pl = np . array ([ self . _convertmb2Pa ( p ) for p in ds . levels . values ]) xArr = ds [ 'x' ] . values yArr = ds [ 'y' ] . values lats = ds [ 'lats' ] . values lons = ds [ 'lons' ] . values temps = ds [ 't' ] . values . transpose ( 1 , 2 , 0 ) qs = ds [ 'q' ] . values . transpose ( 1 , 2 , 0 ) geo_hgt = ds [ 'z' ] . values . transpose ( 1 , 2 , 0 ) Ny , Nx = lats . shape lons [ lons > 180 ] -= 360 # data cube format should be lats,lons,heights _xs = np . broadcast_to ( xArr [ np . newaxis , :, np . newaxis ], geo_hgt . shape ) _ys = np . broadcast_to ( yArr [:, np . newaxis , np . newaxis ], geo_hgt . shape ) _lons = np . broadcast_to ( lons [ ... , np . newaxis ], geo_hgt . shape ) _lats = np . broadcast_to ( lats [ ... , np . newaxis ], geo_hgt . shape ) # correct for latitude self . _get_heights ( _lats , geo_hgt ) self . _t = temps self . _q = qs self . _p = np . broadcast_to ( pl [ np . newaxis , np . newaxis , :], geo_hgt . shape ) self . _xs = _xs self . _ys = _ys self . _lats = _lats self . _lons = _lons","title":"load_weather()"},{"location":"reference/#RAiDER.models.merra2","text":"","title":"merra2"},{"location":"reference/#RAiDER.models.merra2.MERRA2","text":"Bases: WeatherModel Source code in RAiDER/models/merra2.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 class MERRA2 ( WeatherModel ): # I took this from MERRA-2 model level weblink # https://goldsmr5.gesdisc.eosdis.nasa.gov:443/opendap/MERRA2/M2I3NVASM.5.12.4/ def __init__ ( self ): import calendar # initialize a weather model WeatherModel . __init__ ( self ) self . _humidityType = 'q' self . _model_level_type = 'ml' # Default, pressure levels are 'pl' self . _classname = 'merra2' self . _dataset = 'merra2' # Tuple of min/max years where data is available. utcnow = dt . datetime . utcnow () enddate = dt . datetime ( utcnow . year , utcnow . month , 15 ) - dt . timedelta ( days = 60 ) enddate = dt . datetime ( enddate . year , enddate . month , calendar . monthrange ( enddate . year , enddate . month )[ 1 ]) self . _valid_range = ( dt . datetime ( 1980 , 1 , 1 ), \"Present\" ) lag_time = utcnow - enddate self . _lag_time = dt . timedelta ( days = lag_time . days ) # Availability lag time in days self . _time_res = 1 # model constants self . _k1 = 0.776 # [K/Pa] self . _k2 = 0.233 # [K/Pa] self . _k3 = 3.75e3 # [K^2/Pa] # horizontal grid spacing self . _lat_res = 0.5 self . _lon_res = 0.625 self . _x_res = 0.625 self . _y_res = 0.5 self . _Name = 'MERRA2' self . files = None self . _bounds = None self . _zlevels = np . flipud ( LEVELS_137_HEIGHTS ) # Projection self . _proj = CRS . from_epsg ( 4326 ) def _fetch ( self , out ): ''' Fetch weather model data from GMAO: note we only extract the lat/lon bounds for this weather model; fetching data is not needed here as we don't actually download any data using OpenDAP ''' time = self . _time # check whether the file already exists if os . path . exists ( out ): return # calculate the array indices for slicing the GMAO variable arrays lat_min_ind = int (( self . _ll_bounds [ 0 ] - ( - 90.0 )) / self . _lat_res ) lat_max_ind = int (( self . _ll_bounds [ 1 ] - ( - 90.0 )) / self . _lat_res ) lon_min_ind = int (( self . _ll_bounds [ 2 ] - ( - 180.0 )) / self . _lon_res ) lon_max_ind = int (( self . _ll_bounds [ 3 ] - ( - 180.0 )) / self . _lon_res ) lats = np . arange ( ( - 90 + lat_min_ind * self . _lat_res ), ( - 90 + ( lat_max_ind + 1 ) * self . _lat_res ), self . _lat_res ) lons = np . arange ( ( - 180 + lon_min_ind * self . _lon_res ), ( - 180 + ( lon_max_ind + 1 ) * self . _lon_res ), self . _lon_res ) if time . year < 1992 : url_sub = 100 elif time . year < 2001 : url_sub = 200 elif time . year < 2011 : url_sub = 300 else : url_sub = 400 T0 = dt . datetime ( time . year , time . month , time . day , 0 , 0 , 0 ) DT = time - T0 time_ind = int ( DT . total_seconds () / 3600.0 / 3.0 ) ml_min = 0 ml_max = 71 # Earthdata credentials earthdata_usr , earthdata_pwd = read_EarthData_loginInfo ( EARTHDATA_RC ) # open the dataset and pull the data url = 'https://goldsmr5.gesdisc.eosdis.nasa.gov:443/opendap/MERRA2/M2I3NVASM.5.12.4/' + time . strftime ( '%Y/%m' ) + '/MERRA2_' + str ( url_sub ) + '.inst3_3d_asm_Nv.' + time . strftime ( '%Y%m %d ' ) + '.nc4' session = pydap . cas . urs . setup_session ( earthdata_usr , earthdata_pwd , check_url = url ) ds = pydap . client . open_url ( url , session = session ) ############# The MERRA-2 server changes the pydap data retrieval format frequently between these two formats; so better to retain both of them rather than only using either one of them ############# try : q = ds [ 'QV' ] . data [ 0 ][ time_ind , ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )][ 0 ] p = ds [ 'PL' ] . data [ 0 ][ time_ind , ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )][ 0 ] t = ds [ 'T' ] . data [ 0 ][ time_ind , ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )][ 0 ] h = ds [ 'H' ] . data [ 0 ][ time_ind , ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )][ 0 ] except IndexError : q = ds [ 'QV' ] . data [ time_ind , ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )][ 0 ] p = ds [ 'PL' ] . data [ time_ind , ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )][ 0 ] t = ds [ 'T' ] . data [ time_ind , ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )][ 0 ] h = ds [ 'H' ] . data [ time_ind , ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )][ 0 ] except AttributeError : q = ds [ 'QV' ][ time_ind , ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )][ 0 ] p = ds [ 'PL' ][ time_ind , ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )][ 0 ] t = ds [ 'T' ][ time_ind , ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )][ 0 ] h = ds [ 'H' ][ time_ind , ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )][ 0 ] except BaseException : logger . exception ( \"MERRA-2: Unable to read weathermodel data\" ) ######################################################################################################################## try : writeWeatherVars2NETCDF4 ( self , lats , lons , h , q , p , t , outName = out ) except Exception as e : logger . debug ( e ) logger . exception ( \"MERRA-2: Unable to save weathermodel to file\" ) raise RuntimeError ( 'MERRA-2 failed with the following error: {} ' . format ( e )) def load_weather ( self , * args , f = None , ** kwargs ): ''' Consistent class method to be implemented across all weather model types. As a result of calling this method, all of the variables (x, y, z, p, q, t, wet_refractivity, hydrostatic refractivity, e) should be fully populated. ''' if f is None : f = self . files [ 0 ] self . _load_model_level ( f ) def _load_model_level ( self , filename ): ''' Get the variables from the GMAO link using OpenDAP ''' # adding the import here should become absolute when transition to netcdf from netCDF4 import Dataset with Dataset ( filename , mode = 'r' ) as f : lons = np . array ( f . variables [ 'x' ][:]) lats = np . array ( f . variables [ 'y' ][:]) h = np . array ( f . variables [ 'H' ][:]) q = np . array ( f . variables [ 'QV' ][:]) p = np . array ( f . variables [ 'PL' ][:]) t = np . array ( f . variables [ 'T' ][:]) # restructure the 3-D lat/lon/h in regular grid _lons = np . broadcast_to ( lons [ np . newaxis , np . newaxis , :], t . shape ) _lats = np . broadcast_to ( lats [ np . newaxis , :, np . newaxis ], t . shape ) # Re-structure everything from (heights, lats, lons) to (lons, lats, heights) p = np . transpose ( p ) q = np . transpose ( q ) t = np . transpose ( t ) h = np . transpose ( h ) _lats = np . transpose ( _lats ) _lons = np . transpose ( _lons ) # check this # data cube format should be lats,lons,heights p = p . swapaxes ( 0 , 1 ) q = q . swapaxes ( 0 , 1 ) t = t . swapaxes ( 0 , 1 ) h = h . swapaxes ( 0 , 1 ) _lats = _lats . swapaxes ( 0 , 1 ) _lons = _lons . swapaxes ( 0 , 1 ) # For some reason z is opposite the others p = np . flip ( p , axis = 2 ) q = np . flip ( q , axis = 2 ) t = np . flip ( t , axis = 2 ) h = np . flip ( h , axis = 2 ) _lats = np . flip ( _lats , axis = 2 ) _lons = np . flip ( _lons , axis = 2 ) # assign the regular-grid (lat/lon/h) variables self . _p = p self . _q = q self . _t = t self . _lats = _lats self . _lons = _lons self . _xs = _lons self . _ys = _lats self . _zs = h","title":"MERRA2"},{"location":"reference/#RAiDER.models.merra2.MERRA2.load_weather","text":"Consistent class method to be implemented across all weather model types. As a result of calling this method, all of the variables (x, y, z, p, q, t, wet_refractivity, hydrostatic refractivity, e) should be fully populated. Source code in RAiDER/models/merra2.py 148 149 150 151 152 153 154 155 156 157 def load_weather ( self , * args , f = None , ** kwargs ): ''' Consistent class method to be implemented across all weather model types. As a result of calling this method, all of the variables (x, y, z, p, q, t, wet_refractivity, hydrostatic refractivity, e) should be fully populated. ''' if f is None : f = self . files [ 0 ] self . _load_model_level ( f )","title":"load_weather()"},{"location":"reference/#RAiDER.models.model_levels","text":"Pre-defined model levels and a, b constants for the different weather models NOTE : The fixed heights used here are from ECMWF's geometric altitudes (https://www.ecmwf.int/en/forecasts/documentation-and-support/137-model-levels), where \"geopotential altitude is calculated from a mathematical model that adjusts the altitude to include the variation of gravity with height, while geometric altitude is the standard direct vertical distance above mean sea level (MSL).\" - Wikipedia.org, https://en.wikipedia.org/wiki/International_Standard_Atmosphere","title":"model_levels"},{"location":"reference/#RAiDER.models.ncmr","text":"Created on Wed Sep 9 10:26:44 2020 @author: prashant Modified by Yang Lei, GPS/Caltech","title":"ncmr"},{"location":"reference/#RAiDER.models.ncmr.NCMR","text":"Bases: WeatherModel Implement NCMRWF NCUM (named as NCMR) model in future Source code in RAiDER/models/ncmr.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 class NCMR ( WeatherModel ): ''' Implement NCMRWF NCUM (named as NCMR) model in future ''' def __init__ ( self ): # initialize a weather model WeatherModel . __init__ ( self ) self . _humidityType = 'q' # q for specific humidity and rh for relative humidity self . _model_level_type = 'ml' # Default, pressure levels are 'pl', and model levels are \"ml\" self . _classname = 'ncmr' # name of the custom weather model self . _dataset = 'ncmr' # same name as above self . _Name = 'NCMR' # name of the new weather model (in Capital) # Tuple of min/max years where data is available. self . _valid_range = ( datetime . datetime ( 2015 , 12 , 1 ), \"Present\" ) # Availability lag time in days/hours self . _lag_time = datetime . timedelta ( hours = 6 ) self . _time_res = 1 # model constants self . _k1 = 0.776 # [K/Pa] self . _k2 = 0.233 # [K/Pa] self . _k3 = 3.75e3 # [K^2/Pa] # horizontal grid spacing self . _lon_res = .17578125 # grid spacing in longitude self . _lat_res = .11718750 # grid spacing in latitude self . _x_res = .17578125 # same as longitude self . _y_res = .11718750 # same as latitude self . _zlevels = np . flipud ( LEVELS_137_HEIGHTS ) self . _bounds = None # Projection self . _proj = CRS . from_epsg ( 4326 ) def _fetch ( self , out ): ''' Fetch weather model data from NCMR: note we only extract the lat/lon bounds for this weather model; fetching data is not needed here as we don't actually download data , data exist in same system ''' time = self . _time # Auxillary function: ''' download data of the NCMR model and save it in desired location ''' self . _files = self . _download_ncmr_file ( out , time , self . _ll_bounds ) def load_weather ( self , filename = None , * args , ** kwargs ): ''' Load NCMR model variables from existing file ''' if filename is None : filename = self . files [ 0 ] # bounding box plus a buffer lat_min , lat_max , lon_min , lon_max = self . _ll_bounds self . _bounds = ( lat_min , lat_max , lon_min , lon_max ) self . _makeDataCubes ( filename ) def _download_ncmr_file ( self , out , date_time , bounding_box ): ''' Download weather model data (whole globe) from NCMR weblink, crop it to the region of interest, and save the cropped data as a standard .nc file of RAiDER (e.g. \"NCMR_YYYY_MM_DD_THH_MM_SS.nc\"); Temporarily download data from NCMR ftp 'https://ftp.ncmrwf.gov.in/pub/outgoing/SAC/NCUM_OSF/' and copied in weather_models folder ''' from netCDF4 import Dataset ############# Use these lines and modify the link when actually downloading NCMR data from a weblink ############# url , username , password = read_NCMR_loginInfo () filename = os . path . basename ( out ) url = f 'ftp:// { username } : { password } @ { url } /TEST/ { filename } ' filepath = f ' { out [: - 3 ] } _raw.nc' if not os . path . exists ( filepath ): logger . info ( 'Fetching URL: %s ' , url ) local_filename , headers = urllib . request . urlretrieve ( url , filepath , show_progress ) else : logger . warning ( 'Weather model already exists, skipping download' ) ######################################################################################################################## ############# For debugging: use pre-downloaded files; Remove/comment out it when actually downloading NCMR data from a weblink ############# # filepath = os.path.dirname(out) + '/NCUM_ana_mdllev_20180701_00z.nc' ######################################################################################################################## # calculate the array indices for slicing the GMAO variable arrays lat_min_ind = int (( self . _bounds [ 0 ] - ( - 89.94141 )) / self . _lat_res ) lat_max_ind = int (( self . _bounds [ 1 ] - ( - 89.94141 )) / self . _lat_res ) if ( self . _bounds [ 2 ] < 0.0 ): lon_min_ind = int (( self . _bounds [ 2 ] + 360.0 - ( 0.087890625 )) / self . _lon_res ) else : lon_min_ind = int (( self . _bounds [ 2 ] - ( 0.087890625 )) / self . _lon_res ) if ( self . _bounds [ 3 ] < 0.0 ): lon_max_ind = int (( self . _bounds [ 3 ] + 360.0 - ( 0.087890625 )) / self . _lon_res ) else : lon_max_ind = int (( self . _bounds [ 3 ] - ( 0.087890625 )) / self . _lon_res ) ml_min = 0 ml_max = 70 with Dataset ( filepath , 'r' , maskandscale = True ) as f : lats = f . variables [ 'latitude' ][ lat_min_ind :( lat_max_ind + 1 )] . copy () if ( self . _bounds [ 2 ] * self . _bounds [ 3 ] < 0 ): lons1 = f . variables [ 'longitude' ][ lon_min_ind :] . copy () lons2 = f . variables [ 'longitude' ][ 0 :( lon_max_ind + 1 )] . copy () lons = np . append ( lons1 , lons2 ) else : lons = f . variables [ 'longitude' ][ lon_min_ind :( lon_max_ind + 1 )] . copy () if ( self . _bounds [ 2 ] * self . _bounds [ 3 ] < 0 ): t1 = f . variables [ 'air_temperature' ][ ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :] . copy () t2 = f . variables [ 'air_temperature' ][ ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), 0 :( lon_max_ind + 1 )] . copy () t = np . append ( t1 , t2 , axis = 2 ) else : t = f . variables [ 'air_temperature' ][ ml_min :( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )] . copy () # Skipping first pressure levels (below 20 meter) if ( self . _bounds [ 2 ] * self . _bounds [ 3 ] < 0 ): q1 = f . variables [ 'specific_humidity' ][( ml_min + 1 ):( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :] . copy () q2 = f . variables [ 'specific_humidity' ][( ml_min + 1 ):( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), 0 :( lon_max_ind + 1 )] . copy () q = np . append ( q1 , q2 , axis = 2 ) else : q = f . variables [ 'specific_humidity' ][( ml_min + 1 ):( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )] . copy () if ( self . _bounds [ 2 ] * self . _bounds [ 3 ] < 0 ): p1 = f . variables [ 'air_pressure' ][( ml_min + 1 ):( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :] . copy () p2 = f . variables [ 'air_pressure' ][( ml_min + 1 ):( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), 0 :( lon_max_ind + 1 )] . copy () p = np . append ( p1 , p2 , axis = 2 ) else : p = f . variables [ 'air_pressure' ][( ml_min + 1 ):( ml_max + 1 ), lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )] . copy () level_hgt = f . variables [ 'level_height' ][( ml_min + 1 ):( ml_max + 1 )] . copy () if ( self . _bounds [ 2 ] * self . _bounds [ 3 ] < 0 ): surface_alt1 = f . variables [ 'surface_altitude' ][ lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :] . copy () surface_alt2 = f . variables [ 'surface_altitude' ][ lat_min_ind :( lat_max_ind + 1 ), 0 :( lon_max_ind + 1 )] . copy () surface_alt = np . append ( surface_alt1 , surface_alt2 , axis = 1 ) else : surface_alt = f . variables [ 'surface_altitude' ][ lat_min_ind :( lat_max_ind + 1 ), lon_min_ind :( lon_max_ind + 1 )] . copy () hgt = np . zeros ([ len ( level_hgt ), len ( surface_alt [:, 1 ]), len ( surface_alt [ 1 , :])]) for i in range ( len ( level_hgt )): hgt [ i , :, :] = surface_alt [:, :] + level_hgt [ i ] lons [ lons > 180 ] -= 360 ############# For debugging: comment it out when using pre-downloaded raw data files and don't want to remove them for test; Uncomment it when actually downloading NCMR data from a weblink ############# os . remove ( filepath ) ######################################################################################################################## try : writeWeatherVars2NETCDF4 ( self , lats , lons , hgt , q , p , t , outName = out ) except Exception : logger . exception ( \"Unable to save weathermodel to file\" ) def _makeDataCubes ( self , filename ): ''' Get the variables from the saved .nc file (named as \"NCMR_YYYY_MM_DD_THH_MM_SS.nc\") ''' from netCDF4 import Dataset # adding the import here should become absolute when transition to netcdf with Dataset ( filename , mode = 'r' ) as f : lons = np . array ( f . variables [ 'x' ][:]) lats = np . array ( f . variables [ 'y' ][:]) hgt = np . array ( f . variables [ 'H' ][:]) q = np . array ( f . variables [ 'QV' ][:]) p = np . array ( f . variables [ 'PL' ][:]) t = np . array ( f . variables [ 'T' ][:]) # re-assign lons, lats to match heights _lons = np . broadcast_to ( lons [ np . newaxis , np . newaxis , :], t . shape ) _lats = np . broadcast_to ( lats [ np . newaxis , :, np . newaxis ], t . shape ) # Re-structure everything from (heights, lats, lons) to (lons, lats, heights) _lats = np . transpose ( _lats ) _lons = np . transpose ( _lons ) t = np . transpose ( t ) q = np . transpose ( q ) p = np . transpose ( p ) hgt = np . transpose ( hgt ) # data cube format should be lats,lons,heights p = p . swapaxes ( 0 , 1 ) q = q . swapaxes ( 0 , 1 ) t = t . swapaxes ( 0 , 1 ) hgt = hgt . swapaxes ( 0 , 1 ) _lats = _lats . swapaxes ( 0 , 1 ) _lons = _lons . swapaxes ( 0 , 1 ) # assign the regular-grid variables self . _p = p self . _q = q self . _t = t self . _lats = _lats self . _lons = _lons self . _xs = _lons self . _ys = _lats self . _zs = hgt","title":"NCMR"},{"location":"reference/#RAiDER.models.ncmr.NCMR.load_weather","text":"Load NCMR model variables from existing file Source code in RAiDER/models/ncmr.py 78 79 80 81 82 83 84 85 86 87 88 89 def load_weather ( self , filename = None , * args , ** kwargs ): ''' Load NCMR model variables from existing file ''' if filename is None : filename = self . files [ 0 ] # bounding box plus a buffer lat_min , lat_max , lon_min , lon_max = self . _ll_bounds self . _bounds = ( lat_min , lat_max , lon_min , lon_max ) self . _makeDataCubes ( filename )","title":"load_weather()"},{"location":"reference/#RAiDER.models.plotWeather","text":"This set of functions is designed to for plotting WeatherModel class objects. It is not designed to be used on its own apart from this class.","title":"plotWeather"},{"location":"reference/#RAiDER.models.plotWeather.plot_pqt","text":"Create a plot with pressure, temp, and humidity at two heights Source code in RAiDER/models/plotWeather.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def plot_pqt ( weatherObj , savefig = True , z1 = 500 , z2 = 15000 ): ''' Create a plot with pressure, temp, and humidity at two heights ''' # Get the interpolator intFcn_p = Interpolator (( weatherObj . _xs , weatherObj . _ys , weatherObj . _zs ), weatherObj . _p . swapaxes ( 0 , 1 )) intFcn_e = Interpolator (( weatherObj . _xs , weatherObj . _ys , weatherObj . _zs ), weatherObj . _e . swapaxes ( 0 , 1 )) intFcn_t = Interpolator (( weatherObj . _xs , weatherObj . _ys , weatherObj . _zs ), weatherObj . _t . swapaxes ( 0 , 1 )) # get the points needed XY = np . meshgrid ( weatherObj . _xs , weatherObj . _ys ) x = XY [ 0 ] y = XY [ 1 ] z1a = np . zeros ( x . shape ) + z1 z2a = np . zeros ( x . shape ) + z2 pts1 = np . stack (( x . flatten (), y . flatten (), z1a . flatten ()), axis = 1 ) pts2 = np . stack (( x . flatten (), y . flatten (), z2a . flatten ()), axis = 1 ) p1 = intFcn_p ( pts1 ) e1 = intFcn_e ( pts1 ) t1 = intFcn_t ( pts1 ) p2 = intFcn_p ( pts2 ) e2 = intFcn_e ( pts2 ) t2 = intFcn_t ( pts2 ) # Now get the data to plot plots = [ p1 / 1e2 , e1 / 1e2 , t1 - 273.15 , p2 / 1e2 , e2 / 1e2 , t2 - 273.15 ] # titles = ('P (hPa)', 'E (hPa)'.format(z1), 'T (C)', '', '', '') titles = ( 'P (hPa)' , 'E (hPa)' , 'T (C)' , '' , '' , '' ) # setup the plot f = plt . figure ( figsize = ( 18 , 14 )) f . suptitle ( ' {} Pressure/Humidity/Temperature at height {} m and {} m (values should drop as elevation increases)' . format ( weatherObj . _Name , z1 , z2 )) xind = int ( np . floor ( weatherObj . _xs . shape [ 0 ] / 2 )) yind = int ( np . floor ( weatherObj . _ys . shape [ 0 ] / 2 )) # loop over each plot for ind , plot , title in zip ( range ( len ( plots )), plots , titles ): sp = f . add_subplot ( 3 , 3 , ind + 1 ) im = sp . imshow ( np . reshape ( plot , x . shape ), cmap = 'viridis' , extent = [ np . nanmin ( x ), np . nanmax ( x ), np . nanmin ( y ), np . nanmax ( y )], origin = 'lower' ) sp . plot ( x [ yind , xind ], y [ yind , xind ], 'ko' ) divider = mal ( sp ) cax = divider . append_axes ( \"right\" , size = \"4%\" , pad = 0.05 ) plt . colorbar ( im , cax = cax ) sp . set_title ( title ) if ind == 0 : sp . set_ylabel ( ' {} m \\n ' . format ( z1 )) if ind == 3 : sp . set_ylabel ( ' {} m \\n ' . format ( z2 )) # add plots that show each variable with height zdata = weatherObj . _zs [:] / 1000 sp = f . add_subplot ( 3 , 3 , 7 ) sp . plot ( weatherObj . _p [ yind , xind , :] / 1e2 , zdata ) sp . set_ylabel ( 'Height (km)' ) sp . set_xlabel ( 'Pressure (hPa)' ) sp = f . add_subplot ( 3 , 3 , 8 ) sp . plot ( weatherObj . _e [ yind , xind , :] / 100 , zdata ) sp . set_xlabel ( 'E (hPa)' ) sp = f . add_subplot ( 3 , 3 , 9 ) sp . plot ( weatherObj . _t [ yind , xind , :] - 273.15 , zdata ) sp . set_xlabel ( 'Temp (C)' ) plt . subplots_adjust ( top = 0.95 , bottom = 0.1 , left = 0.1 , right = 0.95 , hspace = 0.2 , wspace = 0.3 ) if savefig : plt . savefig ( ' {} _weather_hgt {} _and_ {} m.pdf' . format ( weatherObj . _Name , z1 , z2 )) return f","title":"plot_pqt()"},{"location":"reference/#RAiDER.models.plotWeather.plot_wh","text":"Create a plot with wet refractivity and hydrostatic refractivity, at two different heights Source code in RAiDER/models/plotWeather.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def plot_wh ( weatherObj , savefig = True , z1 = 500 , z2 = 15000 ): ''' Create a plot with wet refractivity and hydrostatic refractivity, at two different heights ''' # Get the interpolator intFcn_w = Interpolator (( weatherObj . _xs , weatherObj . _ys , weatherObj . _zs ), weatherObj . _wet_refractivity . swapaxes ( 0 , 1 )) intFcn_h = Interpolator (( weatherObj . _xs , weatherObj . _ys , weatherObj . _zs ), weatherObj . _hydrostatic_refractivity . swapaxes ( 0 , 1 )) # get the points needed XY = np . meshgrid ( weatherObj . _xs , weatherObj . _ys ) x = XY [ 0 ] y = XY [ 1 ] z1a = np . zeros ( x . shape ) + z1 z2a = np . zeros ( x . shape ) + z2 pts1 = np . stack (( x . flatten (), y . flatten (), z1a . flatten ()), axis = 1 ) pts2 = np . stack (( x . flatten (), y . flatten (), z2a . flatten ()), axis = 1 ) w1 = intFcn_w ( pts1 ) h1 = intFcn_h ( pts1 ) w2 = intFcn_w ( pts2 ) h2 = intFcn_h ( pts2 ) # Now get the data to plot plots = [ w1 , h1 , w2 , h2 ] # titles titles = ( 'Wet refractivity {} ' . format ( z1 ), 'Hydrostatic refractivity {} ' . format ( z1 ), ' {} ' . format ( z2 ), ' {} ' . format ( z2 )) # setup the plot f = plt . figure ( figsize = ( 14 , 10 )) f . suptitle ( ' {} Wet and Hydrostatic refractivity at height {} m and {} m' . format ( weatherObj . _Name , z1 , z2 )) # loop over each plot for ind , plot , title in zip ( range ( len ( plots )), plots , titles ): sp = f . add_subplot ( 2 , 2 , ind + 1 ) im = sp . imshow ( np . reshape ( plot , x . shape ), cmap = 'viridis' , extent = [ np . nanmin ( x ), np . nanmax ( x ), np . nanmin ( y ), np . nanmax ( y )], origin = 'lower' ) divider = mal ( sp ) cax = divider . append_axes ( \"right\" , size = \"4%\" , pad = 0.05 ) plt . colorbar ( im , cax = cax ) sp . set_title ( title ) if ind == 0 : sp . set_ylabel ( ' {} m \\n ' . format ( z1 )) if ind == 2 : sp . set_ylabel ( ' {} m \\n ' . format ( z2 )) if savefig : plt . savefig ( ' {} _refractivity_hgt {} _and_ {} m.pdf' . format ( weatherObj . _Name , z1 , z2 )) return f","title":"plot_wh()"},{"location":"reference/#RAiDER.models.template","text":"","title":"template"},{"location":"reference/#RAiDER.models.template.customModelReader","text":"Bases: WeatherModel Source code in RAiDER/models/template.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 class customModelReader ( WeatherModel ): def __init__ ( self ): WeatherModel . __init__ ( self ) self . _humidityType = 'q' # can be \"q\" (specific humidity) or \"rh\" (relative humidity) self . _model_level_type = 'pl' # Default, pressure levels are \"pl\", and model levels are \"ml\" self . _classname = 'abcd' # name of the custom weather model self . _dataset = 'abcd' # same name as above # Tuple of min/max years where data is available. # valid range of the dataset. Users need to specify the start date and end date (can be \"present\") self . _valid_range = ( datetime . datetime ( 2016 , 7 , 15 ), \"Present\" ) # Availability lag time. Can be specified in hours \"hours=3\" or in days \"days=3\" self . _lag_time = datetime . timedelta ( hours = 3 ) # Availabile time resolution; i.e. minimum rate model is available in hours. 1 is hourly self . _time_res = 1 # model constants (these three constants are borrowed from ECMWF model and currently # set to be default for all other models, which may need to be double checked.) self . _k1 = 0.776 # [K/Pa] self . _k2 = 0.233 # [K/Pa] self . _k3 = 3.75e3 # [K^2/Pa] # horizontal grid spacing self . _lat_res = 3. / 111 # grid spacing in latitude self . _lon_res = 3. / 111 # grid spacing in longitude self . _x_res = 3. # x-direction grid spacing in the native weather model projection # (if the projection is in lat/lon, it is the same as \"self._lon_res\") self . _y_res = 3. # y-direction grid spacing in the weather model native projection # (if the projection is in lat/lon, it is the same as \"self._lat_res\") # zlevels specify fixed heights at which to interpolate the weather model variables self . _zlevels = np . flipud ( LEVELS_137_HEIGHTS ) self . _Name = 'ABCD' # name of the custom weather model (better to be capitalized) # Projections in RAiDER are defined using pyproj (python wrapper around Proj) # If the projection is defined with EPSG code, one can use \"self._proj = CRS.from_epsg(4326)\" # to replace the following lines to get \"self._proj\". # Below we show the example of HRRR model with the parameters of its Lambert Conformal Conic projection lon0 = 262.5 lat0 = 38.5 lat1 = 38.5 lat2 = 38.5 x0 = 0 y0 = 0 earth_radius = 6371229 p1 = CRS ( '+proj=lcc +lat_1= {lat1} +lat_2= {lat2} +lat_0= {lat0} +lon_0= {lon0} +x_0= {x0} +y_0= {y0} +a= {a} +b= {a} +units=m +no_defs' . format ( lat1 = lat1 , lat2 = lat2 , lat0 = lat0 , lon0 = lon0 , x0 = x0 , y0 = y0 , a = earth_radius )) self . _proj = p1 def _fetch ( self , out ): ''' Fetch weather model data from the custom weather model \"ABCD\" Inputs (no need to change in the custom weather model reader): lats - latitude lons - longitude time - datatime object (year,month,day,hour,minute,second) out - name of downloaded dataset file from the custom weather model server Nextra - buffer of latitude/longitude for determining the bounding box ''' # Auxilliary function: # download dataset of the custom weather model \"ABCD\" from a server and then save it to a file named out. # This function needs to be writen by the users. For download from the weather model server, the weather model # name, time and bounding box may be needed to retrieve the dataset; for cases where no file is actually # downloaded, e.g. the GMAO and MERRA-2 models using OpenDAP, this function can be omitted leaving the data # retrieval to the following \"load_weather\" function. self . _files = self . _download_abcd_file ( out , 'abcd' , self . _time , self . _ll_bounds ) def load_weather ( self , filename ): ''' Load weather model variables from the downloaded file named filename Inputs: filename - filename of the downloaded weather model file ''' # Auxilliary function: # read individual variables (in 3-D cube format with exactly the same dimension) from downloaded file # This function needs to be writen by the users. For downloaded file from the weather model server, # this function extracts the individual variables from the saved file named filename; # for cases where no file is actually downloaded, e.g. the GMAO and MERRA-2 models using OpenDAP, # this function retrieves the individual variables directly from the weblink of the weather model. lats , lons , xs , ys , t , q , p , hgt = self . _makeDataCubes ( filename ) # extra steps that may be needed to calculate topographic height and pressure level if not provided # directly by the weather model through the above auxilliary function \"self._makeDataCubes\" # if surface pressure (in logarithm) is provided as \"p\" along with the surface geopotential \"z\" (needs to be # added to the auxilliary function \"self._makeDataCubes\"), one can use the following line to convert to # geopotential, pressure level and geopotential height; otherwise commented out z , p , hgt = self . _calculategeoh ( z , p ) # TODO: z is undefined # if the geopotential is provided as \"z\" (needs to be added to the auxilliary function \"self._makeDataCubes\"), # one can use the following line to convert to geopotential height; otherwise, commented out hgt = z / self . _g0 # if geopotential height is provided/calculated as \"hgt\", one can use the following line to convert to # topographic height, which is then automatically assigned to \"self._zs\"; otherwise commented out self . _get_heights ( lats , hgt ) # if topographic height is provided as \"hgt\", use the following line directly; otherwise commented out self . _zs = hgt ########### ######### output of the weather model reader for delay calculations (all in 3-D data cubes) ######## # _t: temperture # _q: either relative or specific humidity # _p: must be pressure level # _xs: x-direction grid dimension of the native weather model coordinates (if in lat/lon, _xs = _lons) # _ys: y-direction grid dimension of the native weather model coordinates (if in lat/lon, _ys = _lats) # _zs: must be topographic height # _lats: latitude # _lons: longitude self . _t = t self . _q = q self . _p = p self . _xs = xs self . _ys = ys self . _lats = lats self . _lons = lons ########### def _download_abcd_file ( self , out , model_name , date_time , bounding_box ): ''' Auxilliary function: Download weather model data from a server Inputs: out - filename for saving the retrieved data file model_name - name of the custom weather model date_time - datatime object (year,month,day,hour,minute,second) bounding_box - lat/lon bounding box for the region of interest Output: out - returned filename from input ''' pass def _makeDataCubes ( self , filename ): ''' Auxilliary function: Read 3-D data cubes from downloaded file or directly from weather model weblink (in which case, there is no need to download and save any file; rather, the weblink needs to be hardcoded in the custom reader, e.g. GMAO) Input: filename - filename of the downloaded weather model file from the server Outputs: lats - latitude (3-D data cube) lons - longitude (3-D data cube) xs - x-direction grid dimension of the native weather model coordinates (3-D data cube; if in lat/lon, _xs = _lons) ys - y-direction grid dimension of the native weather model coordinates (3-D data cube; if in lat/lon, _ys = _lats) t - temperature (3-D data cube) q - humidity (3-D data cube; could be relative humidity or specific humidity) p - pressure level (3-D data cube; could be pressure level (preferred) or surface pressure) hgt - height (3-D data cube; could be geopotential height or topographic height (preferred)) ''' pass","title":"customModelReader"},{"location":"reference/#RAiDER.models.template.customModelReader.load_weather","text":"Load weather model variables from the downloaded file named filename Inputs: filename - filename of the downloaded weather model file Source code in RAiDER/models/template.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def load_weather ( self , filename ): ''' Load weather model variables from the downloaded file named filename Inputs: filename - filename of the downloaded weather model file ''' # Auxilliary function: # read individual variables (in 3-D cube format with exactly the same dimension) from downloaded file # This function needs to be writen by the users. For downloaded file from the weather model server, # this function extracts the individual variables from the saved file named filename; # for cases where no file is actually downloaded, e.g. the GMAO and MERRA-2 models using OpenDAP, # this function retrieves the individual variables directly from the weblink of the weather model. lats , lons , xs , ys , t , q , p , hgt = self . _makeDataCubes ( filename ) # extra steps that may be needed to calculate topographic height and pressure level if not provided # directly by the weather model through the above auxilliary function \"self._makeDataCubes\" # if surface pressure (in logarithm) is provided as \"p\" along with the surface geopotential \"z\" (needs to be # added to the auxilliary function \"self._makeDataCubes\"), one can use the following line to convert to # geopotential, pressure level and geopotential height; otherwise commented out z , p , hgt = self . _calculategeoh ( z , p ) # TODO: z is undefined # if the geopotential is provided as \"z\" (needs to be added to the auxilliary function \"self._makeDataCubes\"), # one can use the following line to convert to geopotential height; otherwise, commented out hgt = z / self . _g0 # if geopotential height is provided/calculated as \"hgt\", one can use the following line to convert to # topographic height, which is then automatically assigned to \"self._zs\"; otherwise commented out self . _get_heights ( lats , hgt ) # if topographic height is provided as \"hgt\", use the following line directly; otherwise commented out self . _zs = hgt ########### ######### output of the weather model reader for delay calculations (all in 3-D data cubes) ######## # _t: temperture # _q: either relative or specific humidity # _p: must be pressure level # _xs: x-direction grid dimension of the native weather model coordinates (if in lat/lon, _xs = _lons) # _ys: y-direction grid dimension of the native weather model coordinates (if in lat/lon, _ys = _lats) # _zs: must be topographic height # _lats: latitude # _lons: longitude self . _t = t self . _q = q self . _p = p self . _xs = xs self . _ys = ys self . _lats = lats self . _lons = lons","title":"load_weather()"},{"location":"reference/#RAiDER.models.weatherModel","text":"","title":"weatherModel"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel","text":"Bases: ABC Implement a generic weather model for getting estimated SAR delays Source code in RAiDER/models/weatherModel.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 class WeatherModel ( ABC ): ''' Implement a generic weather model for getting estimated SAR delays ''' def __init__ ( self ): # Initialize model-specific constants/parameters self . _k1 = None self . _k2 = None self . _k3 = None self . _humidityType = 'q' self . _a = [] self . _b = [] self . files = None self . _time_res = None # time resolution of the weather model in hours self . _lon_res = None self . _lat_res = None self . _x_res = None self . _y_res = None self . _classname = None self . _dataset = None self . _model_level_type = 'ml' self . _valid_range = ( datetime . date ( 1900 , 1 , 1 ), ) # Tuple of min/max years where data is available. self . _lag_time = datetime . timedelta ( days = 30 ) # Availability lag time in days self . _time = None self . _bbox = None # Define fixed constants self . _R_v = 461.524 self . _R_d = 287.06 # in our original code this was 287.053 self . _g0 = _g0 # gravity constant self . _zmin = _ZMIN # minimum integration height self . _zmax = _ZREF # max integration height self . _proj = None # setup data structures self . _levels = [] self . _xs = np . empty (( 1 , 1 , 1 )) # Use generic x/y/z instead of lon/lat/height self . _ys = np . empty (( 1 , 1 , 1 )) self . _zs = np . empty (( 1 , 1 , 1 )) self . _lats = None self . _lons = None self . _ll_bounds = None self . _p = None self . _q = None self . _rh = None self . _t = None self . _e = None self . _wet_refractivity = None self . _hydrostatic_refractivity = None self . _wet_ztd = None self . _hydrostatic_ztd = None def __str__ ( self ): string = ' \\n ' string += '======Weather Model class object===== \\n ' string += 'Weather model time: {} \\n ' . format ( self . _time ) string += 'Latitude resolution: {} \\n ' . format ( self . _lat_res ) string += 'Longitude resolution: {} \\n ' . format ( self . _lon_res ) string += 'Native projection: {} \\n ' . format ( self . _proj ) string += 'ZMIN: {} \\n ' . format ( self . _zmin ) string += 'ZMAX: {} \\n ' . format ( self . _zmax ) string += 'k1 = {} \\n ' . format ( self . _k1 ) string += 'k2 = {} \\n ' . format ( self . _k2 ) string += 'k3 = {} \\n ' . format ( self . _k3 ) string += 'Humidity type = {} \\n ' . format ( self . _humidityType ) string += '===================================== \\n ' string += 'Class name: {} \\n ' . format ( self . _classname ) string += 'Dataset: {} \\n ' . format ( self . _dataset ) string += '===================================== \\n ' string += 'A: {} \\n ' . format ( self . _a ) string += 'B: {} \\n ' . format ( self . _b ) if self . _p is not None : string += 'Number of points in Lon/Lat = {} / {} \\n ' . format ( * self . _p . shape [: 2 ]) string += 'Total number of grid points (3D): {} \\n ' . format ( np . prod ( self . _p . shape )) if self . _xs . size == 0 : string += 'Minimum/Maximum y: {: 4.2f} / {: 4.2f} \\n ' \\ . format ( robmin ( self . _ys ), robmax ( self . _ys )) string += 'Minimum/Maximum x: {: 4.2f} / {: 4.2f} \\n ' \\ . format ( robmin ( self . _xs ), robmax ( self . _xs )) string += 'Minimum/Maximum zs/heights: {: 10.2f} / {: 10.2f} \\n ' \\ . format ( robmin ( self . _zs ), robmax ( self . _zs )) string += '===================================== \\n ' return str ( string ) def Model ( self ): return self . _Name def fetch ( self , out , ll_bounds , time ): ''' Checks the input datetime against the valid date range for the model and then calls the model _fetch routine Args: ---------- out - ll_bounds - 4 x 1 array, SNWE time = UTC datetime ''' self . checkTime ( time ) self . set_latlon_bounds ( ll_bounds ) self . setTime ( time ) self . _fetch ( out ) @abstractmethod def _fetch ( self , out ): ''' Placeholder method. Should be implemented in each weather model type class ''' pass def setTime ( self , time , fmt = '%Y-%m- %d T%H:%M:%S' ): ''' Set the time for a weather model ''' if isinstance ( time , str ): self . _time = datetime . datetime . strptime ( time , fmt ) elif isinstance ( time , datetime . datetime ): self . _time = time else : raise ValueError ( '\"time\" must be a string or a datetime object' ) def get_latlon_bounds ( self ): raise NotImplementedError def set_latlon_bounds ( self , ll_bounds , Nextra = 2 ): ''' Need to correct lat/lon bounds because not all of the weather models have valid data exactly bounded by -90/90 (lats) and -180/180 (lons); for GMAO and MERRA2, need to adjust the longitude higher end with an extra buffer; for other models, the exact bounds are close to -90/90 (lats) and -180/180 (lons) and thus can be rounded to the above regions (either in the downloading-file API or subsetting- data API) without problems. ''' ex_buffer_lon_max = 0.0 if self . _Name == 'GMAO' or self . _Name == 'MERRA2' : ex_buffer_lon_max = self . _lon_res elif self . _Name == 'HRRR' : Nextra = 6 # have a bigger buffer # At boundary lats and lons, need to modify Nextra buffer so that the lats and lons do not exceed the boundary S , N , W , E = ll_bounds # Adjust bounds if they get near the poles or IDL S = np . max ([ S - Nextra * self . _lat_res , - 90.0 + Nextra * self . _lat_res ]) N = np . min ([ N + Nextra * self . _lat_res , 90.0 - Nextra * self . _lat_res ]) W = np . max ([ W - Nextra * self . _lon_res , - 180.0 + Nextra * self . _lon_res ]) E = np . min ([ E + Nextra * self . _lon_res + ex_buffer_lon_max , 180.0 - Nextra * self . _lon_res - ex_buffer_lon_max ]) self . _ll_bounds = np . array ([ S , N , W , E ]) def load ( self , outLoc , * args , ll_bounds = None , _zlevels = None , zref = _ZREF , ** kwargs ): ''' Calls the load_weather method. Each model class should define a load_weather method appropriate for that class. 'args' should be one or more filenames. ''' self . set_latlon_bounds ( ll_bounds ) # If the weather file has already been processed, do nothing self . _out_name = self . out_file ( outLoc ) if os . path . exists ( self . _out_name ): return self . _out_name else : # Load the weather just for the query points self . load_weather ( * args , ** kwargs ) # Process the weather model data self . _find_e () self . _uniform_in_z ( _zlevels = _zlevels ) self . _checkForNans () self . _get_wet_refractivity () self . _get_hydro_refractivity () self . _adjust_grid ( ll_bounds ) # Compute Zenith delays at the weather model grid nodes self . _getZTD ( zref ) return None @abstractmethod def load_weather ( self , * args , ** kwargs ): ''' Placeholder method. Should be implemented in each weather model type class ''' pass def _get_time ( self , filename = None ): if filename is None : filename = self . files [ 0 ] with netCDF4 . Dataset ( filename , mode = 'r' ) as f : time = f . attrs [ 'datetime' ] . copy () self . time = datetime . datetime . strptime ( time , \"%Y_%m_ %d T%H_%M_%S\" ) def plot ( self , plotType = 'pqt' , savefig = True ): ''' Plotting method. Valid plot types are 'pqt' ''' if plotType == 'pqt' : plot = plots . plot_pqt ( self , savefig ) elif plotType == 'wh' : plot = plots . plot_wh ( self , savefig ) else : raise RuntimeError ( 'WeatherModel.plot: No plotType named {} ' . format ( plotType )) return plot def checkTime ( self , time ): ''' Checks the time against the lag time and valid date range for the given model type ''' logger . info ( 'Weather model %s is available from %s - %s ' , self . Model (), self . _valid_range [ 0 ], self . _valid_range [ 1 ] ) if time < self . _valid_range [ 0 ]: raise RuntimeError ( \"Weather model {} is not available at {} \" . format ( self . Model (), time )) if self . _valid_range [ 1 ] is not None : if self . _valid_range [ 1 ] == 'Present' : pass elif self . _valid_range [ 1 ] < time : raise RuntimeError ( \"Weather model {} is not available at {} \" . format ( self . Model (), time )) if time > datetime . datetime . utcnow () - self . _lag_time : raise RuntimeError ( \"Weather model {} is not available at {} \" . format ( self . Model (), time )) def _convertmb2Pa ( self , pres ): ''' Convert pressure in millibars to Pascals ''' return 100 * pres def _get_heights ( self , lats , geo_hgt , geo_ht_fill = np . nan ): ''' Transform geo heights to actual heights ''' geo_ht_fix = np . where ( geo_hgt != geo_ht_fill , geo_hgt , np . nan ) self . _zs = util . _geo_to_ht ( lats , geo_ht_fix ) def _find_e ( self ): \"\"\"Check the type of e-calculation needed\"\"\" if self . _humidityType == 'rh' : self . _find_e_from_rh () elif self . _humidityType == 'q' : self . _find_e_from_q () else : raise RuntimeError ( 'Not a valid humidity type' ) self . _rh = None self . _q = None def _find_e_from_q ( self ): \"\"\"Calculate e, partial pressure of water vapor.\"\"\" svp = find_svp ( self . _t ) # We have q = w/(w + 1), so w = q/(1 - q) w = self . _q / ( 1 - self . _q ) self . _e = w * self . _R_v * ( self . _p - svp ) / self . _R_d def _find_e_from_rh ( self ): \"\"\"Calculate partial pressure of water vapor.\"\"\" svp = find_svp ( self . _t ) self . _e = self . _rh / 100 * svp def _get_wet_refractivity ( self ): ''' Calculate the wet delay from pressure, temperature, and e ''' self . _wet_refractivity = self . _k2 * self . _e / self . _t + self . _k3 * self . _e / self . _t ** 2 def _get_hydro_refractivity ( self ): ''' Calculate the hydrostatic delay from pressure and temperature ''' self . _hydrostatic_refractivity = self . _k1 * self . _p / self . _t def getWetRefractivity ( self ): return self . _wet_refractivity def getHydroRefractivity ( self ): return self . _hydrostatic_refractivity def _adjust_grid ( self , ll_bounds = None ): ''' This function pads the weather grid with a level at self._zmin, if it does not already go that low. <<The functionality below has been removed.>> <<It also removes levels that are above self._zmax, since they are not needed.>> ''' if self . _zmin < np . nanmin ( self . _zs ): # first add in a new layer at zmin self . _zs = np . insert ( self . _zs , 0 , self . _zmin ) self . _p = util . padLower ( self . _p ) self . _t = util . padLower ( self . _t ) self . _e = util . padLower ( self . _e ) self . _wet_refractivity = util . padLower ( self . _wet_refractivity ) self . _hydrostatic_refractivity = util . padLower ( self . _hydrostatic_refractivity ) if ll_bounds is not None : self . _trimExtent ( ll_bounds ) def _getZTD ( self , zref = None ): ''' Compute the full slant tropospheric delay for each weather model grid node, using the reference height zref ''' if zref is None : zref = self . _zmax wet = self . getWetRefractivity () hydro = self . getHydroRefractivity () # Get the integrated ZTD wet_total , hydro_total = np . zeros ( wet . shape ), np . zeros ( hydro . shape ) for level in range ( wet . shape [ 2 ]): wet_total [ ... , level ] = 1e-6 * np . trapz ( wet [ ... , level :], x = self . _zs [ level :], axis = 2 ) hydro_total [ ... , level ] = 1e-6 * np . trapz ( hydro [ ... , level :], x = self . _zs [ level :], axis = 2 ) self . _hydrostatic_ztd = hydro_total self . _wet_ztd = wet_total def _getExtent ( self , lats , lons ): ''' get the bounding box around a set of lats/lons ''' if ( lats . size == 1 ) & ( lons . size == 1 ): return [ lats - self . _lat_res , lats + self . _lat_res , lons - self . _lon_res , lons + self . _lon_res ] elif ( lats . size > 1 ) & ( lons . size > 1 ): return [ np . nanmin ( lats ), np . nanmax ( lats ), np . nanmin ( lons ), np . nanmax ( lons )] elif lats . size == 1 : return [ lats - self . _lat_res , lats + self . _lat_res , np . nanmin ( lons ), np . nanmax ( lons )] elif lons . size == 1 : return [ np . nanmin ( lats ), np . nanmax ( lats ), lons - self . _lon_res , lons + self . _lon_res ] else : raise RuntimeError ( 'Not a valid lat/lon shape' ) @property def bbox ( self ) -> list : \"\"\" Obtains the bounding box of the weather model in lat/lon CRS. Returns: ------- list xmin, ymin, xmax, ymax Raises ------ ValueError When `self.files` is None. \"\"\" if self . _bbox is None : if self . files is None : raise ValueError ( 'Need to save weather model as netcdf' ) weather_model_path = self . files [ 0 ] with xarray . load_dataset ( weather_model_path ) as ds : try : xmin , xmax = ds . x . min (), ds . x . max () ymin , ymax = ds . y . min (), ds . y . max () except : xmin , xmax = ds . longitude . min (), ds . longitude . max () ymin , ymax = ds . latitude . min (), ds . latitude . max () wm_proj = self . _proj lons , lats = transform_coords ( wm_proj , CRS ( 4326 ), [ xmin , xmax ], [ ymin , ymax ]) self . _bbox = [ lons [ 0 ], lats [ 0 ], lons [ 1 ], lats [ 1 ]] return self . _bbox def checkContainment ( self : weatherModel , ll_bounds : np . ndarray , buffer_deg : float = 1e-5 ) -> bool : \"\"\"\" Checks containment of weather model bbox of outLats and outLons provided. Args: ---------- weather_model : weatherModel outLats : np.ndarray An array of latitude points outLons : np.ndarray An array of longitude points buffer_deg : float For x-translates for extents that lie outside of world bounding box, this ensures that translates have some overlap. The default is 1e-5 or ~11.1 meters. Returns: ------- bool True if weather model contains bounding box of OutLats and outLons and False otherwise. \"\"\" ymin_input , ymax_input , xmin_input , xmax_input = ll_bounds input_box = box ( xmin_input , ymin_input , xmax_input , ymax_input ) xmin , ymin , xmax , ymax = self . bbox weather_model_box = box ( xmin , ymin , xmax , ymax ) world_box = box ( - 180 , - 90 , 180 , 90 ) # Logger input_box_str = [ f ' { x : 1.2f } ' for x in [ xmin_input , ymin_input , xmax_input , ymax_input ]] weath_box_str = [ f ' { x : 1.2f } ' for x in [ xmin , ymin , xmax , ymax ]] weath_box_str = ', ' . join ( weath_box_str ) input_box_str = ', ' . join ( input_box_str ) logger . info ( f 'Extent of the weather model is (xmin, ymin, xmax, ymax):' f ' { weath_box_str } ' ) logger . info ( f 'Extent of the input is (xmin, ymin, xmax, ymax): ' f ' { input_box_str } ' ) # If the bounding box goes beyond the normal world extents # Look at two x-translates, buffer them, and take their union. if not world_box . contains ( weather_model_box ): logger . info ( 'Considering x-translates of weather model +/-360 ' 'as bounding box outside of -180, -90, 180, 90' ) translates = [ weather_model_box . buffer ( buffer_deg ), translate ( weather_model_box , xoff = 360 ) . buffer ( buffer_deg ), translate ( weather_model_box , xoff =- 360 ) . buffer ( buffer_deg ) ] weather_model_box = unary_union ( translates ) return weather_model_box . contains ( input_box ) def _isOutside ( self , extent1 , extent2 ): ''' Determine whether any of extent1 lies outside extent2 extent1/2 should be a list containing [lower_lat, upper_lat, left_lon, right_lon] ''' t1 = extent1 [ 0 ] < extent2 [ 0 ] t2 = extent1 [ 1 ] > extent2 [ 1 ] t3 = extent1 [ 2 ] < extent2 [ 2 ] t4 = extent1 [ 3 ] > extent2 [ 3 ] if np . any ([ t1 , t2 , t3 , t4 ]): return True return False def _trimExtent ( self , extent ): ''' get the bounding box around a set of lats/lons ''' lat = self . _lats [:, :, 0 ] lon = self . _lons [:, :, 0 ] lat [ np . isnan ( lat )] = np . nanmean ( lat ) lon [ np . isnan ( lon )] = np . nanmean ( lon ) mask = ( lat >= extent [ 0 ]) & ( lat <= extent [ 1 ]) & \\ ( lon >= extent [ 2 ]) & ( lon <= extent [ 3 ]) ma1 = np . sum ( mask , axis = 1 ) . astype ( 'bool' ) ma2 = np . sum ( mask , axis = 0 ) . astype ( 'bool' ) if np . sum ( ma1 ) == 0 and np . sum ( ma2 ) == 0 : # Don't need to remove any points return # indices of the part of the grid to keep ny , nx , nz = self . _p . shape index1 = max ( np . arange ( len ( ma1 ))[ ma1 ][ 0 ] - 2 , 0 ) index2 = min ( np . arange ( len ( ma1 ))[ ma1 ][ - 1 ] + 2 , ny ) index3 = max ( np . arange ( len ( ma2 ))[ ma2 ][ 0 ] - 2 , 0 ) index4 = min ( np . arange ( len ( ma2 ))[ ma2 ][ - 1 ] + 2 , nx ) # subset around points of interest self . _lons = self . _lons [ index1 : index2 , index3 : index4 , :] self . _lats = self . _lats [ index1 : index2 , index3 : index4 , ... ] self . _xs = self . _xs [ index3 : index4 ] self . _ys = self . _ys [ index1 : index2 ] self . _p = self . _p [ index1 : index2 , index3 : index4 , ... ] self . _t = self . _t [ index1 : index2 , index3 : index4 , ... ] self . _e = self . _e [ index1 : index2 , index3 : index4 , ... ] self . _wet_refractivity = self . _wet_refractivity [ index1 : index2 , index3 : index4 , ... ] self . _hydrostatic_refractivity = self . _hydrostatic_refractivity [ index1 : index2 , index3 : index4 , :] def _calculategeoh ( self , z , lnsp ): ''' Function to calculate pressure, geopotential, and geopotential height from the surface pressure and model levels provided by a weather model. The model levels are numbered from the highest eleveation to the lowest. Inputs: self - weather model object with parameters a, b defined z - 3-D array of surface heights for the location(s) of interest lnsp - log of the surface pressure Outputs: geopotential - The geopotential in units of height times acceleration pressurelvs - The pressure at each of the model levels for each of the input points geoheight - The geopotential heights ''' return calcgeoh ( lnsp , self . _t , self . _q , z , self . _a , self . _b , self . _R_d , self . _levels ) def getProjection ( self ): ''' Returns: the native weather projection, which should be a pyproj object ''' return self . _proj def getPoints ( self ): return self . _xs . copy (), self . _ys . copy (), self . _zs . copy () def _uniform_in_z ( self , _zlevels = None ): ''' Interpolate all variables to a regular grid in z ''' nx , ny = self . _p . shape [: 2 ] # new regular z-spacing if _zlevels is None : try : _zlevels = self . _zlevels except BaseException : _zlevels = np . nanmean ( self . _zs , axis = ( 0 , 1 )) new_zs = np . tile ( _zlevels , ( nx , ny , 1 )) # re-assign values to the uniform z self . _t = interpolate_along_axis ( self . _zs , self . _t , new_zs , axis = 2 , fill_value = np . nan ) . astype ( np . float32 ) self . _p = interpolate_along_axis ( self . _zs , self . _p , new_zs , axis = 2 , fill_value = np . nan ) . astype ( np . float32 ) self . _e = interpolate_along_axis ( self . _zs , self . _e , new_zs , axis = 2 , fill_value = np . nan ) . astype ( np . float32 ) self . _lats = interpolate_along_axis ( self . _zs , self . _lats , new_zs , axis = 2 , fill_value = np . nan ) . astype ( np . float32 ) self . _lons = interpolate_along_axis ( self . _zs , self . _lons , new_zs , axis = 2 , fill_value = np . nan ) . astype ( np . float32 ) self . _zs = _zlevels self . _xs = np . unique ( self . _xs ) self . _ys = np . unique ( self . _ys ) def _checkForNans ( self ): ''' Fill in NaN-values ''' self . _p = fillna3D ( self . _p ) self . _t = fillna3D ( self . _t ) self . _e = fillna3D ( self . _e ) def out_file ( self , outLoc ): f = make_weather_model_filename ( self . _Name , self . _time , self . _ll_bounds , ) return os . path . join ( outLoc , f ) def filename ( self , time = None , outLoc = 'weather_files' ): ''' Create a filename to store the weather model ''' os . makedirs ( outLoc , exist_ok = True ) if time is None : if self . _time is None : raise ValueError ( 'Time must be specified before the file can be written' ) else : time = self . _time f = make_raw_weather_data_filename ( outLoc , self . _Name , time , ) self . files = [ f ] def write ( self , NoDataValue =- 3.4028234e+38 , chunk = ( 1 , 128 , 128 ), ): ''' By calling the abstract/modular netcdf writer (RAiDER.utilFcns.write2NETCDF4core), write the weather model data and refractivity to an NETCDF4 file that can be accessed by external programs. ''' # Generate the filename mapping_name = get_mapping ( self . _proj ) f = self . _out_name dimidY , dimidX , dimidZ = self . _t . shape chunk_lines_Y = np . min ([ chunk [ 1 ], dimidY ]) chunk_lines_X = np . min ([ chunk [ 2 ], dimidX ]) ChunkSize = [ 1 , chunk_lines_Y , chunk_lines_X ] nc_outfile = netCDF4 . Dataset ( f , 'w' , clobber = True , format = 'NETCDF4' ) nc_outfile . setncattr ( 'Conventions' , 'CF-1.6' ) nc_outfile . setncattr ( 'datetime' , datetime . datetime . strftime ( self . _time , \"%Y_%m_ %d T%H_%M_%S\" )) nc_outfile . setncattr ( 'date_created' , datetime . datetime . now () . strftime ( \"%Y_%m_ %d T%H_%M_%S\" )) title = 'Weather model data and delay calculations' nc_outfile . setncattr ( 'title' , title ) tran = [ self . _xs [ 0 ], self . _xs [ 1 ] - self . _xs [ 0 ], 0.0 , self . _ys [ 0 ], 0.0 , self . _ys [ 1 ] - self . _ys [ 0 ]] dimension_dict = { 'x' : { 'varname' : 'x' , 'datatype' : np . dtype ( 'float64' ), 'dimensions' : ( 'x' ), 'length' : dimidX , 'FillValue' : None , 'standard_name' : 'projection_x_coordinate' , 'description' : 'weather model native x' , 'dataset' : self . _xs , 'units' : 'degrees_east' }, 'y' : { 'varname' : 'y' , 'datatype' : np . dtype ( 'float64' ), 'dimensions' : ( 'y' ), 'length' : dimidY , 'FillValue' : None , 'standard_name' : 'projection_y_coordinate' , 'description' : 'weather model native y' , 'dataset' : self . _ys , 'units' : 'degrees_north' }, 'z' : { 'varname' : 'z' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' ), 'length' : dimidZ , 'FillValue' : None , 'standard_name' : 'projection_z_coordinate' , 'description' : 'vertical coordinate' , 'dataset' : self . _zs , 'units' : 'm' } } dataset_dict = { 'latitude' : { 'varname' : 'latitude' , 'datatype' : np . dtype ( 'float64' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'latitude' , 'description' : 'latitude' , 'dataset' : self . _lats . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 ), 'units' : 'degrees_north' }, 'longitude' : { 'varname' : 'longitude' , 'datatype' : np . dtype ( 'float64' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'longitude' , 'description' : 'longitude' , 'dataset' : self . _lons . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 ), 'units' : 'degrees_east' }, 't' : { 'varname' : 't' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'temperature' , 'description' : 'temperature' , 'dataset' : self . _t . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 ), 'units' : 'K' }, 'p' : { 'varname' : 'p' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'pressure' , 'description' : 'pressure' , 'dataset' : self . _p . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 ), 'units' : 'Pa' }, 'e' : { 'varname' : 'e' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'humidity' , 'description' : 'humidity' , 'dataset' : self . _e . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 ), 'units' : 'Pa' }, 'wet' : { 'varname' : 'wet' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'wet_refractivity' , 'description' : 'wet_refractivity' , 'dataset' : self . _wet_refractivity . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 )}, 'hydro' : { 'varname' : 'hydro' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'hydrostatic_refractivity' , 'description' : 'hydrostatic_refractivity' , 'dataset' : self . _hydrostatic_refractivity . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 )}, 'wet_total' : { 'varname' : 'wet_total' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'total_wet_refractivity' , 'description' : 'total_wet_refractivity' , 'dataset' : self . _wet_ztd . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 )}, 'hydro_total' : { 'varname' : 'hydro_total' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'total_hydrostatic_refractivity' , 'description' : 'total_hydrostatic_refractivity' , 'dataset' : self . _hydrostatic_ztd . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 )} } nc_outfile = write2NETCDF4core ( nc_outfile , dimension_dict , dataset_dict , tran , mapping_name = mapping_name ) nc_outfile . sync () # flush data to disk nc_outfile . close () return f","title":"WeatherModel"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.bbox","text":"Obtains the bounding box of the weather model in lat/lon CRS. list xmin, ymin, xmax, ymax","title":"bbox"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.bbox--raises","text":"ValueError When self.files is None.","title":"Raises"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.checkContainment","text":"\" Checks containment of weather model bbox of outLats and outLons provided. weather_model : weatherModel np.ndarray An array of latitude points np.ndarray An array of longitude points float For x-translates for extents that lie outside of world bounding box, this ensures that translates have some overlap. The default is 1e-5 or ~11.1 meters. bool True if weather model contains bounding box of OutLats and outLons and False otherwise. Source code in RAiDER/models/weatherModel.py 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 def checkContainment ( self : weatherModel , ll_bounds : np . ndarray , buffer_deg : float = 1e-5 ) -> bool : \"\"\"\" Checks containment of weather model bbox of outLats and outLons provided. Args: ---------- weather_model : weatherModel outLats : np.ndarray An array of latitude points outLons : np.ndarray An array of longitude points buffer_deg : float For x-translates for extents that lie outside of world bounding box, this ensures that translates have some overlap. The default is 1e-5 or ~11.1 meters. Returns: ------- bool True if weather model contains bounding box of OutLats and outLons and False otherwise. \"\"\" ymin_input , ymax_input , xmin_input , xmax_input = ll_bounds input_box = box ( xmin_input , ymin_input , xmax_input , ymax_input ) xmin , ymin , xmax , ymax = self . bbox weather_model_box = box ( xmin , ymin , xmax , ymax ) world_box = box ( - 180 , - 90 , 180 , 90 ) # Logger input_box_str = [ f ' { x : 1.2f } ' for x in [ xmin_input , ymin_input , xmax_input , ymax_input ]] weath_box_str = [ f ' { x : 1.2f } ' for x in [ xmin , ymin , xmax , ymax ]] weath_box_str = ', ' . join ( weath_box_str ) input_box_str = ', ' . join ( input_box_str ) logger . info ( f 'Extent of the weather model is (xmin, ymin, xmax, ymax):' f ' { weath_box_str } ' ) logger . info ( f 'Extent of the input is (xmin, ymin, xmax, ymax): ' f ' { input_box_str } ' ) # If the bounding box goes beyond the normal world extents # Look at two x-translates, buffer them, and take their union. if not world_box . contains ( weather_model_box ): logger . info ( 'Considering x-translates of weather model +/-360 ' 'as bounding box outside of -180, -90, 180, 90' ) translates = [ weather_model_box . buffer ( buffer_deg ), translate ( weather_model_box , xoff = 360 ) . buffer ( buffer_deg ), translate ( weather_model_box , xoff =- 360 ) . buffer ( buffer_deg ) ] weather_model_box = unary_union ( translates ) return weather_model_box . contains ( input_box )","title":"checkContainment()"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.checkTime","text":"Checks the time against the lag time and valid date range for the given model type Source code in RAiDER/models/weatherModel.py 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 def checkTime ( self , time ): ''' Checks the time against the lag time and valid date range for the given model type ''' logger . info ( 'Weather model %s is available from %s - %s ' , self . Model (), self . _valid_range [ 0 ], self . _valid_range [ 1 ] ) if time < self . _valid_range [ 0 ]: raise RuntimeError ( \"Weather model {} is not available at {} \" . format ( self . Model (), time )) if self . _valid_range [ 1 ] is not None : if self . _valid_range [ 1 ] == 'Present' : pass elif self . _valid_range [ 1 ] < time : raise RuntimeError ( \"Weather model {} is not available at {} \" . format ( self . Model (), time )) if time > datetime . datetime . utcnow () - self . _lag_time : raise RuntimeError ( \"Weather model {} is not available at {} \" . format ( self . Model (), time ))","title":"checkTime()"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.fetch","text":"Checks the input datetime against the valid date range for the model and then calls the model _fetch routine out - ll_bounds - 4 x 1 array, SNWE time = UTC datetime Source code in RAiDER/models/weatherModel.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def fetch ( self , out , ll_bounds , time ): ''' Checks the input datetime against the valid date range for the model and then calls the model _fetch routine Args: ---------- out - ll_bounds - 4 x 1 array, SNWE time = UTC datetime ''' self . checkTime ( time ) self . set_latlon_bounds ( ll_bounds ) self . setTime ( time ) self . _fetch ( out )","title":"fetch()"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.filename","text":"Create a filename to store the weather model Source code in RAiDER/models/weatherModel.py 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 def filename ( self , time = None , outLoc = 'weather_files' ): ''' Create a filename to store the weather model ''' os . makedirs ( outLoc , exist_ok = True ) if time is None : if self . _time is None : raise ValueError ( 'Time must be specified before the file can be written' ) else : time = self . _time f = make_raw_weather_data_filename ( outLoc , self . _Name , time , ) self . files = [ f ]","title":"filename()"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.getProjection","text":"Source code in RAiDER/models/weatherModel.py 539 540 541 542 543 def getProjection ( self ): ''' Returns: the native weather projection, which should be a pyproj object ''' return self . _proj","title":"getProjection()"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.load","text":"Calls the load_weather method. Each model class should define a load_weather method appropriate for that class. 'args' should be one or more filenames. Source code in RAiDER/models/weatherModel.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 def load ( self , outLoc , * args , ll_bounds = None , _zlevels = None , zref = _ZREF , ** kwargs ): ''' Calls the load_weather method. Each model class should define a load_weather method appropriate for that class. 'args' should be one or more filenames. ''' self . set_latlon_bounds ( ll_bounds ) # If the weather file has already been processed, do nothing self . _out_name = self . out_file ( outLoc ) if os . path . exists ( self . _out_name ): return self . _out_name else : # Load the weather just for the query points self . load_weather ( * args , ** kwargs ) # Process the weather model data self . _find_e () self . _uniform_in_z ( _zlevels = _zlevels ) self . _checkForNans () self . _get_wet_refractivity () self . _get_hydro_refractivity () self . _adjust_grid ( ll_bounds ) # Compute Zenith delays at the weather model grid nodes self . _getZTD ( zref ) return None","title":"load()"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.load_weather","text":"Placeholder method. Should be implemented in each weather model type class Source code in RAiDER/models/weatherModel.py 223 224 225 226 227 228 @abstractmethod def load_weather ( self , * args , ** kwargs ): ''' Placeholder method. Should be implemented in each weather model type class ''' pass","title":"load_weather()"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.plot","text":"Plotting method. Valid plot types are 'pqt' Source code in RAiDER/models/weatherModel.py 237 238 239 240 241 242 243 244 245 246 247 def plot ( self , plotType = 'pqt' , savefig = True ): ''' Plotting method. Valid plot types are 'pqt' ''' if plotType == 'pqt' : plot = plots . plot_pqt ( self , savefig ) elif plotType == 'wh' : plot = plots . plot_wh ( self , savefig ) else : raise RuntimeError ( 'WeatherModel.plot: No plotType named {} ' . format ( plotType )) return plot","title":"plot()"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.setTime","text":"Set the time for a weather model Source code in RAiDER/models/weatherModel.py 148 149 150 151 152 153 154 155 def setTime ( self , time , fmt = '%Y-%m- %d T%H:%M:%S' ): ''' Set the time for a weather model ''' if isinstance ( time , str ): self . _time = datetime . datetime . strptime ( time , fmt ) elif isinstance ( time , datetime . datetime ): self . _time = time else : raise ValueError ( '\"time\" must be a string or a datetime object' )","title":"setTime()"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.set_latlon_bounds","text":"Need to correct lat/lon bounds because not all of the weather models have valid data exactly bounded by -90/90 (lats) and -180/180 (lons); for GMAO and MERRA2, need to adjust the longitude higher end with an extra buffer; for other models, the exact bounds are close to -90/90 (lats) and -180/180 (lons) and thus can be rounded to the above regions (either in the downloading-file API or subsetting- data API) without problems. Source code in RAiDER/models/weatherModel.py 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def set_latlon_bounds ( self , ll_bounds , Nextra = 2 ): ''' Need to correct lat/lon bounds because not all of the weather models have valid data exactly bounded by -90/90 (lats) and -180/180 (lons); for GMAO and MERRA2, need to adjust the longitude higher end with an extra buffer; for other models, the exact bounds are close to -90/90 (lats) and -180/180 (lons) and thus can be rounded to the above regions (either in the downloading-file API or subsetting- data API) without problems. ''' ex_buffer_lon_max = 0.0 if self . _Name == 'GMAO' or self . _Name == 'MERRA2' : ex_buffer_lon_max = self . _lon_res elif self . _Name == 'HRRR' : Nextra = 6 # have a bigger buffer # At boundary lats and lons, need to modify Nextra buffer so that the lats and lons do not exceed the boundary S , N , W , E = ll_bounds # Adjust bounds if they get near the poles or IDL S = np . max ([ S - Nextra * self . _lat_res , - 90.0 + Nextra * self . _lat_res ]) N = np . min ([ N + Nextra * self . _lat_res , 90.0 - Nextra * self . _lat_res ]) W = np . max ([ W - Nextra * self . _lon_res , - 180.0 + Nextra * self . _lon_res ]) E = np . min ([ E + Nextra * self . _lon_res + ex_buffer_lon_max , 180.0 - Nextra * self . _lon_res - ex_buffer_lon_max ]) self . _ll_bounds = np . array ([ S , N , W , E ])","title":"set_latlon_bounds()"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.write","text":"By calling the abstract/modular netcdf writer (RAiDER.utilFcns.write2NETCDF4core), write the weather model data and refractivity to an NETCDF4 file that can be accessed by external programs. Source code in RAiDER/models/weatherModel.py 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 def write ( self , NoDataValue =- 3.4028234e+38 , chunk = ( 1 , 128 , 128 ), ): ''' By calling the abstract/modular netcdf writer (RAiDER.utilFcns.write2NETCDF4core), write the weather model data and refractivity to an NETCDF4 file that can be accessed by external programs. ''' # Generate the filename mapping_name = get_mapping ( self . _proj ) f = self . _out_name dimidY , dimidX , dimidZ = self . _t . shape chunk_lines_Y = np . min ([ chunk [ 1 ], dimidY ]) chunk_lines_X = np . min ([ chunk [ 2 ], dimidX ]) ChunkSize = [ 1 , chunk_lines_Y , chunk_lines_X ] nc_outfile = netCDF4 . Dataset ( f , 'w' , clobber = True , format = 'NETCDF4' ) nc_outfile . setncattr ( 'Conventions' , 'CF-1.6' ) nc_outfile . setncattr ( 'datetime' , datetime . datetime . strftime ( self . _time , \"%Y_%m_ %d T%H_%M_%S\" )) nc_outfile . setncattr ( 'date_created' , datetime . datetime . now () . strftime ( \"%Y_%m_ %d T%H_%M_%S\" )) title = 'Weather model data and delay calculations' nc_outfile . setncattr ( 'title' , title ) tran = [ self . _xs [ 0 ], self . _xs [ 1 ] - self . _xs [ 0 ], 0.0 , self . _ys [ 0 ], 0.0 , self . _ys [ 1 ] - self . _ys [ 0 ]] dimension_dict = { 'x' : { 'varname' : 'x' , 'datatype' : np . dtype ( 'float64' ), 'dimensions' : ( 'x' ), 'length' : dimidX , 'FillValue' : None , 'standard_name' : 'projection_x_coordinate' , 'description' : 'weather model native x' , 'dataset' : self . _xs , 'units' : 'degrees_east' }, 'y' : { 'varname' : 'y' , 'datatype' : np . dtype ( 'float64' ), 'dimensions' : ( 'y' ), 'length' : dimidY , 'FillValue' : None , 'standard_name' : 'projection_y_coordinate' , 'description' : 'weather model native y' , 'dataset' : self . _ys , 'units' : 'degrees_north' }, 'z' : { 'varname' : 'z' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' ), 'length' : dimidZ , 'FillValue' : None , 'standard_name' : 'projection_z_coordinate' , 'description' : 'vertical coordinate' , 'dataset' : self . _zs , 'units' : 'm' } } dataset_dict = { 'latitude' : { 'varname' : 'latitude' , 'datatype' : np . dtype ( 'float64' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'latitude' , 'description' : 'latitude' , 'dataset' : self . _lats . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 ), 'units' : 'degrees_north' }, 'longitude' : { 'varname' : 'longitude' , 'datatype' : np . dtype ( 'float64' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'longitude' , 'description' : 'longitude' , 'dataset' : self . _lons . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 ), 'units' : 'degrees_east' }, 't' : { 'varname' : 't' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'temperature' , 'description' : 'temperature' , 'dataset' : self . _t . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 ), 'units' : 'K' }, 'p' : { 'varname' : 'p' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'pressure' , 'description' : 'pressure' , 'dataset' : self . _p . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 ), 'units' : 'Pa' }, 'e' : { 'varname' : 'e' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'humidity' , 'description' : 'humidity' , 'dataset' : self . _e . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 ), 'units' : 'Pa' }, 'wet' : { 'varname' : 'wet' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'wet_refractivity' , 'description' : 'wet_refractivity' , 'dataset' : self . _wet_refractivity . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 )}, 'hydro' : { 'varname' : 'hydro' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'hydrostatic_refractivity' , 'description' : 'hydrostatic_refractivity' , 'dataset' : self . _hydrostatic_refractivity . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 )}, 'wet_total' : { 'varname' : 'wet_total' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'total_wet_refractivity' , 'description' : 'total_wet_refractivity' , 'dataset' : self . _wet_ztd . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 )}, 'hydro_total' : { 'varname' : 'hydro_total' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'total_hydrostatic_refractivity' , 'description' : 'total_hydrostatic_refractivity' , 'dataset' : self . _hydrostatic_ztd . swapaxes ( 0 , 2 ) . swapaxes ( 1 , 2 )} } nc_outfile = write2NETCDF4core ( nc_outfile , dimension_dict , dataset_dict , tran , mapping_name = mapping_name ) nc_outfile . sync () # flush data to disk nc_outfile . close () return f","title":"write()"},{"location":"reference/#RAiDER.models.weatherModel.find_svp","text":"Calculate standard vapor presure. Should be model-specific Source code in RAiDER/models/weatherModel.py 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 def find_svp ( t ): \"\"\" Calculate standard vapor presure. Should be model-specific \"\"\" # From TRAIN: # Could not find the wrf used equation as they appear to be # mixed with latent heat etc. Istead I used the equations used # in ERA-I (see IFS documentation part 2: Data assimilation # (CY25R1)). Calculate saturated water vapour pressure (svp) for # water (svpw) using Buck 1881 and for ice (swpi) from Alduchow # and Eskridge (1996) euation AERKi # TODO: figure out the sources of all these magic numbers and move # them somewhere more visible. # TODO: (Jeremy) - Need to fix/get the equation for the other # weather model types. Right now this will be used for all models, # except WRF, which is yet to be implemented in my new structure. t1 = 273.15 # O Celsius t2 = 250.15 # -23 Celsius tref = t - t1 wgt = ( t - t2 ) / ( t1 - t2 ) svpw = ( 6.1121 * np . exp (( 17.502 * tref ) / ( 240.97 + tref ))) svpi = ( 6.1121 * np . exp (( 22.587 * tref ) / ( 273.86 + tref ))) svp = svpi + ( svpw - svpi ) * wgt ** 2 ix_bound1 = t > t1 svp [ ix_bound1 ] = svpw [ ix_bound1 ] ix_bound2 = t < t2 svp [ ix_bound2 ] = svpi [ ix_bound2 ] svp = svp * 100 return svp . astype ( np . float32 )","title":"find_svp()"},{"location":"reference/#RAiDER.models.weatherModel.get_mapping","text":"Get CF-complient projection information from a proj Source code in RAiDER/models/weatherModel.py 858 859 860 861 862 863 864 def get_mapping ( proj ): '''Get CF-complient projection information from a proj''' # In case of WGS-84 lat/lon, keep it simple if proj . to_epsg () == 4326 : return 'WGS84' else : return proj . to_wkt ()","title":"get_mapping()"},{"location":"reference/#RAiDER.models.weatherModel.make_raw_weather_data_filename","text":"Filename generator for the raw downloaded weather model data Source code in RAiDER/models/weatherModel.py 810 811 812 813 814 815 816 817 818 819 820 def make_raw_weather_data_filename ( outLoc , name , time ): ''' Filename generator for the raw downloaded weather model data ''' f = os . path . join ( outLoc , ' {} _ {} . {} ' . format ( name , datetime . datetime . strftime ( time , '%Y_%m_ %d _T%H_%M_%S' ), 'nc' ) ) return f","title":"make_raw_weather_data_filename()"},{"location":"reference/#RAiDER.models.wrf","text":"","title":"wrf"},{"location":"reference/#RAiDER.models.wrf.UnitTypeError","text":"Bases: Exception Define a unit type exception for easily formatting error messages for units Source code in RAiDER/models/wrf.py 162 163 164 165 166 167 168 169 170 class UnitTypeError ( Exception ): ''' Define a unit type exception for easily formatting error messages for units ''' def __init___ ( self , varName , unittype ): msg = \"Unknown units for {} : ' {} '\" . format ( varName , unittype ) Exception . __init__ ( self , msg )","title":"UnitTypeError"},{"location":"reference/#RAiDER.models.wrf.WRF","text":"Bases: WeatherModel WRF class definition, based on the WeatherModel base class. Source code in RAiDER/models/wrf.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 class WRF ( WeatherModel ): ''' WRF class definition, based on the WeatherModel base class. ''' # TODO: finish implementing def __init__ ( self ): WeatherModel . __init__ ( self ) self . _k1 = 0.776 # K/Pa self . _k2 = 0.233 # K/Pa self . _k3 = 3.75e3 # K^2/Pa # Currently WRF is using RH instead of Q to get E self . _humidityType = 'rh' self . _Name = 'WRF' def _fetch ( self ): pass def load_weather ( self , file1 , file2 , * args , ** kwargs ): ''' Consistent class method to be implemented across all weather model types ''' try : lons , lats = self . _get_wm_nodes ( file1 ) self . _read_netcdf ( file2 ) except KeyError : self . _get_wm_nodes ( file2 ) self . _read_netcdf ( file1 ) # WRF doesn't give us the coordinates of the points in the native projection, # only the coordinates in lat/long. Ray transformed these to the native # projection, then used an average to enforce a regular grid. It does matter # for the interpolation whether the grid is regular. lla = CRS . from_epsg ( 4326 ) t = Transformer . from_proj ( lla , self . _proj ) xs , ys = t . transform ( lons . flatten (), lats . flatten ()) xs = xs . reshape ( lons . shape ) ys = ys . reshape ( lats . shape ) # Expected accuracy here is to two decimal places (five significant digits) xs = np . mean ( xs , axis = 0 ) ys = np . mean ( ys , axis = 1 ) _xs = np . broadcast_to ( xs [ np . newaxis , np . newaxis , :], self . _p . shape ) _ys = np . broadcast_to ( ys [ np . newaxis , :, np . newaxis ], self . _p . shape ) # Re-structure everything from (heights, lats, lons) to (lons, lats, heights) self . _p = np . transpose ( self . _p ) self . _t = np . transpose ( self . _t ) self . _rh = np . transpose ( self . _rh ) self . _ys = np . transpose ( _ys ) self . _xs = np . transpose ( _xs ) self . _zs = np . transpose ( self . _zs ) # TODO: Not sure if WRF provides this self . _levels = list ( range ( self . _zs . shape [ 2 ])) def _get_wm_nodes ( self , nodeFile ): with netcdf . netcdf_file ( nodeFile , 'r' , maskandscale = True ) as outf : lats = outf . variables [ 'XLAT' ][ 0 ] . copy () # Takes only the first date! lons = outf . variables [ 'XLONG' ][ 0 ] . copy () lons [ lons > 180 ] -= 360 return lons , lats def _read_netcdf ( self , weatherFile , defNul = None ): \"\"\" Read weather variables from a netCDF file \"\"\" if defNul is None : defNul = np . nan # TODO: it'd be cool to use some kind of units package # TODO: extract partial pressure directly (q?) with netcdf . netcdf_file ( weatherFile , 'r' , maskandscale = True ) as f : spvar = f . variables [ 'P_PL' ] temp = f . variables [ 'T_PL' ] humid = f . variables [ 'RH_PL' ] geohvar = f . variables [ 'GHT_PL' ] lon0 = f . STAND_LON . copy () lat0 = f . MOAD_CEN_LAT . copy () lat1 = f . TRUELAT1 . copy () lat2 = f . TRUELAT2 . copy () checkUnits ( spvar . units . decode ( 'utf-8' ), 'pressure' ) checkUnits ( temp . units . decode ( 'utf-8' ), 'temperature' ) checkUnits ( humid . units . decode ( 'utf-8' ), 'relative humidity' ) checkUnits ( geohvar . units . decode ( 'utf-8' ), 'geopotential' ) # _FillValue is not always set, but when it is we want to read it tNull = getNullValue ( temp ) hNull = getNullValue ( humid ) gNull = getNullValue ( geohvar ) pNull = getNullValue ( spvar ) sp = spvar [ 0 ] . copy () temps = temp [ 0 ] . copy () humids = humid [ 0 ] . copy () geoh = geohvar [ 0 ] . copy () spvar = None temp = None humid = None geohvar = None # Projection # See http://www.pkrc.net/wrf-lambert.html earthRadius = 6370e3 # <- note Ray had a bug here p1 = CRS ( proj = 'lcc' , lat_1 = lat1 , lat_2 = lat2 , lat_0 = lat0 , lon_0 = lon0 , a = earthRadius , b = earthRadius , towgs84 = ( 0 , 0 , 0 ), no_defs = True ) self . _proj = p1 temps [ temps == tNull ] = np . nan sp [ sp == pNull ] = np . nan humids [ humids == hNull ] = np . nan geoh [ geoh == gNull ] = np . nan self . _t = temps self . _rh = humids # Zs are problematic because any z below the topography is nan. # For a temporary fix, I will assign any nan value to equal the # nanmean of that level. zmeans = np . nanmean ( geoh , axis = ( 1 , 2 )) nz , ny , nx = geoh . shape Zmeans = np . tile ( zmeans , ( nx , ny , 1 )) Zmeans = Zmeans . T ix = np . isnan ( geoh ) geoh [ ix ] = Zmeans [ ix ] self . _zs = geoh if len ( sp . shape ) == 1 : self . _p = np . broadcast_to ( sp [:, np . newaxis , np . newaxis ], self . _zs . shape ) else : self . _p = sp","title":"WRF"},{"location":"reference/#RAiDER.models.wrf.WRF.load_weather","text":"Consistent class method to be implemented across all weather model types Source code in RAiDER/models/wrf.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def load_weather ( self , file1 , file2 , * args , ** kwargs ): ''' Consistent class method to be implemented across all weather model types ''' try : lons , lats = self . _get_wm_nodes ( file1 ) self . _read_netcdf ( file2 ) except KeyError : self . _get_wm_nodes ( file2 ) self . _read_netcdf ( file1 ) # WRF doesn't give us the coordinates of the points in the native projection, # only the coordinates in lat/long. Ray transformed these to the native # projection, then used an average to enforce a regular grid. It does matter # for the interpolation whether the grid is regular. lla = CRS . from_epsg ( 4326 ) t = Transformer . from_proj ( lla , self . _proj ) xs , ys = t . transform ( lons . flatten (), lats . flatten ()) xs = xs . reshape ( lons . shape ) ys = ys . reshape ( lats . shape ) # Expected accuracy here is to two decimal places (five significant digits) xs = np . mean ( xs , axis = 0 ) ys = np . mean ( ys , axis = 1 ) _xs = np . broadcast_to ( xs [ np . newaxis , np . newaxis , :], self . _p . shape ) _ys = np . broadcast_to ( ys [ np . newaxis , :, np . newaxis ], self . _p . shape ) # Re-structure everything from (heights, lats, lons) to (lons, lats, heights) self . _p = np . transpose ( self . _p ) self . _t = np . transpose ( self . _t ) self . _rh = np . transpose ( self . _rh ) self . _ys = np . transpose ( _ys ) self . _xs = np . transpose ( _xs ) self . _zs = np . transpose ( self . _zs ) # TODO: Not sure if WRF provides this self . _levels = list ( range ( self . _zs . shape [ 2 ]))","title":"load_weather()"},{"location":"reference/#RAiDER.models.wrf.checkUnits","text":"Implement a check that the units are as expected Source code in RAiDER/models/wrf.py 173 174 175 176 177 178 179 def checkUnits ( unitCheck , varName ): ''' Implement a check that the units are as expected ''' unitDict = { 'pressure' : 'Pa' , 'temperature' : 'K' , 'relative humidity' : '%' , 'geopotential' : 'm' } if unitCheck != unitDict [ varName ]: raise UnitTypeError ( varName , unitCheck )","title":"checkUnits()"},{"location":"reference/#RAiDER.models.wrf.getNullValue","text":"Get the null (or fill) value if it exists, otherwise set the null value to defNullValue Source code in RAiDER/models/wrf.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def getNullValue ( var ): ''' Get the null (or fill) value if it exists, otherwise set the null value to defNullValue ''' # NetCDF files have the ability to record their nodata value, but in the # particular NetCDF files that I'm reading, this field is left # unspecified and a nodata value of -999 is used. The solution I'm using # is to check if nodata is specified, and otherwise assume it's -999. _default_fill_value = - 999 try : var_fill = var . _FillValue except AttributeError : var_fill = _default_fill_value return var_fill","title":"getNullValue()"},{"location":"reference/#RAiDER.processWM","text":"","title":"processWM"},{"location":"reference/#RAiDER.processWM.prepareWeatherModel","text":"Parse inputs to download and prepare a weather model grid for interpolation Parameters: Name Type Description Default weather_model WeatherModel - instantiated weather model object required time datetime - Python datetime to request. Will be rounded to nearest available time None wmLoc str str - file path to which to write weather model file(s) None ll_bounds List [ float ] list of float - bounding box to download in [S, N, W, E] format None download_only bool bool - False if preprocessing weather model data False makePlots bool bool - whether to write debug plots False force_download bool bool - True if you want to download even when the weather model exists False Returns: Name Type Description str str filename of the netcdf file to which the weather model has been written Source code in RAiDER/processWM.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def prepareWeatherModel ( weather_model , time = None , wmLoc : str = None , ll_bounds : List [ float ] = None , download_only : bool = False , makePlots : bool = False , force_download : bool = False , ) -> str : \"\"\"Parse inputs to download and prepare a weather model grid for interpolation Args: weather_model: WeatherModel - instantiated weather model object time: datetime - Python datetime to request. Will be rounded to nearest available time wmLoc: str - file path to which to write weather model file(s) ll_bounds: list of float - bounding box to download in [S, N, W, E] format download_only: bool - False if preprocessing weather model data makePlots: bool - whether to write debug plots force_download: bool - True if you want to download even when the weather model exists Returns: str: filename of the netcdf file to which the weather model has been written \"\"\" # Ensure the file output location exists if wmLoc is None : wmLoc = os . path . join ( os . getcwd (), 'weather_files' ) os . makedirs ( wmLoc , exist_ok = True ) # check whether weather model files are supplied or should be downloaded download_flag = True if weather_model . files is None : if time is None : raise RuntimeError ( 'prepareWeatherModel: Either a file or a time must be specified' ) weather_model . filename ( time , wmLoc ) if os . path . exists ( weather_model . files [ 0 ]): if not force_download : logger . warning ( 'Weather model already exists, please remove it (\" %s \") if you want ' 'to download a new one.' , weather_model . files ) download_flag = False else : download_flag = False # if no weather model files supplied, check the standard location if download_flag : weather_model . fetch ( * weather_model . files , ll_bounds , time ) else : time = getTimeFromFile ( weather_model . files [ 0 ]) weather_model . setTime ( time ) containment = weather_model . checkContainment ( ll_bounds ) if not containment : logger . warning ( 'The weather model passed does not cover all of the input ' 'points; you may need to download a larger area.' ) # If only downloading, exit now if download_only : logger . warning ( 'download_only flag selected. No further processing will happen.' ) return None # Otherwise, load the weather model data f = weather_model . load ( wmLoc , ll_bounds = ll_bounds , ) if f is not None : logger . warning ( 'The processed weather model file already exists,' ' so I will use that.' ) return f # Logging some basic info logger . debug ( 'Number of weather model nodes: {} ' . format ( np . prod ( weather_model . getWetRefractivity () . shape ) ) ) shape = weather_model . getWetRefractivity () . shape logger . debug ( f 'Shape of weather model: { shape } ' ) logger . debug ( 'Bounds of the weather model: %.2f / %.2f / %.2f / %.2f (SNWE)' , np . nanmin ( weather_model . _ys ), np . nanmax ( weather_model . _ys ), np . nanmin ( weather_model . _xs ), np . nanmax ( weather_model . _xs ) ) logger . debug ( 'Weather model: %s ' , weather_model . Model ()) logger . debug ( 'Mean value of the wet refractivity: %f ' , np . nanmean ( weather_model . getWetRefractivity ()) ) logger . debug ( 'Mean value of the hydrostatic refractivity: %f ' , np . nanmean ( weather_model . getHydroRefractivity ()) ) logger . debug ( weather_model ) if makePlots : weather_model . plot ( 'wh' , True ) weather_model . plot ( 'pqt' , True ) plt . close ( 'all' ) try : f = weather_model . write () return f except Exception as e : logger . exception ( \"Unable to save weathermodel to file\" ) logger . exception ( e ) raise RuntimeError ( \"Unable to save weathermodel to file\" ) finally : del weather_model","title":"prepareWeatherModel()"},{"location":"reference/#RAiDER.utilFcns","text":"Geodesy-related utility functions.","title":"utilFcns"},{"location":"reference/#RAiDER.utilFcns.calcgeoh","text":"Calculate pressure, geopotential, and geopotential height from the surface pressure and model levels provided by a weather model. The model levels are numbered from the highest eleveation to the lowest. lnsp: ndarray - [y, x] array of log surface pressure t: ndarray - [z, y, x] cube of temperatures q: ndarray - [z, y, x] cube of specific humidity geopotential: ndarray - [z, y, x] cube of geopotential values a: ndarray - [z] vector of a values b: ndarray - [z] vector of b values num_levels: int - integer number of model levels geopotential - The geopotential in units of height times acceleration pressurelvs - The pressure at each of the model levels for each of the input points geoheight - The geopotential heights Source code in RAiDER/utilFcns.py 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 def calcgeoh ( lnsp , t , q , z , a , b , R_d , num_levels ): ''' Calculate pressure, geopotential, and geopotential height from the surface pressure and model levels provided by a weather model. The model levels are numbered from the highest eleveation to the lowest. Args: ---------- lnsp: ndarray - [y, x] array of log surface pressure t: ndarray - [z, y, x] cube of temperatures q: ndarray - [z, y, x] cube of specific humidity geopotential: ndarray - [z, y, x] cube of geopotential values a: ndarray - [z] vector of a values b: ndarray - [z] vector of b values num_levels: int - integer number of model levels Returns: ------- geopotential - The geopotential in units of height times acceleration pressurelvs - The pressure at each of the model levels for each of the input points geoheight - The geopotential heights ''' geopotential = np . zeros_like ( t ) pressurelvs = np . zeros_like ( geopotential ) geoheight = np . zeros_like ( geopotential ) # log surface pressure # Note that we integrate from the ground up, so from the largest model level to 0 sp = np . exp ( lnsp ) if len ( a ) != num_levels + 1 or len ( b ) != num_levels + 1 : raise ValueError ( 'I have here a model with {} levels, but parameters a ' . format ( num_levels ) + 'and b have lengths {} and {} respectively. Of ' . format ( len ( a ), len ( b )) + 'course, these three numbers should be equal.' ) # Integrate up into the atmosphere from *lowest level* z_h = 0 # initial value for lev , t_level , q_level in zip ( range ( num_levels , 0 , - 1 ), t [:: - 1 ], q [:: - 1 ]): # lev is the level number 1-60, we need a corresponding index # into ts and qs # ilevel = num_levels - lev # << this was Ray's original, but is a typo # because indexing like that results in pressure and height arrays that # are in the opposite orientation to the t/q arrays. ilevel = lev - 1 # compute moist temperature t_level = t_level * ( 1 + 0.609133 * q_level ) # compute the pressures (on half-levels) Ph_lev = a [ lev - 1 ] + ( b [ lev - 1 ] * sp ) Ph_levplusone = a [ lev ] + ( b [ lev ] * sp ) pressurelvs [ ilevel ] = Ph_lev # + Ph_levplusone) / 2 # average pressure at half-levels above and below if lev == 1 : dlogP = np . log ( Ph_levplusone / 0.1 ) alpha = np . log ( 2 ) else : dlogP = np . log ( Ph_levplusone ) - np . log ( Ph_lev ) alpha = 1 - (( Ph_lev / ( Ph_levplusone - Ph_lev )) * dlogP ) TRd = t_level * R_d # z_f is the geopotential of this full level # integrate from previous (lower) half-level z_h to the full level z_f = z_h + TRd * alpha + z # Geopotential (add in surface geopotential) geopotential [ ilevel ] = z_f geoheight [ ilevel ] = geopotential [ ilevel ] / g0 # z_h is the geopotential of 'half-levels' # integrate z_h to next half level z_h += TRd * dlogP return geopotential , pressurelvs , geoheight","title":"calcgeoh()"},{"location":"reference/#RAiDER.utilFcns.checkLOS","text":"Check that los is either (1) Zenith, (2) a set of scalar values of the same size as the number of points, which represent the projection value), or (3) a set of vectors, same number as the number of points. Source code in RAiDER/utilFcns.py 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def checkLOS ( los , Npts ): ''' Check that los is either: (1) Zenith, (2) a set of scalar values of the same size as the number of points, which represent the projection value), or (3) a set of vectors, same number as the number of points. ''' from RAiDER.losreader import Zenith # los is a bunch of vectors or Zenith if los is not Zenith : los = los . reshape ( - 1 , 3 ) if los is not Zenith and los . shape [ 0 ] != Npts : raise RuntimeError ( 'Found {} line-of-sight values and only {} points' . format ( los . shape [ 0 ], Npts )) return los","title":"checkLOS()"},{"location":"reference/#RAiDER.utilFcns.checkShapes","text":"Make sure that by the time the code reaches here, we have a consistent set of line-of-sight and position data. Source code in RAiDER/utilFcns.py 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 def checkShapes ( los , lats , lons , hts ): ''' Make sure that by the time the code reaches here, we have a consistent set of line-of-sight and position data. ''' from RAiDER.losreader import Zenith test1 = hts . shape == lats . shape == lons . shape try : test2 = los . shape [: - 1 ] == hts . shape except AttributeError : test2 = los is Zenith if not test1 and test2 : raise ValueError ( 'I need lats, lons, heights, and los to all be the same shape. ' + 'lats had shape {} , lons had shape {} , ' . format ( lats . shape , lons . shape ) + 'heights had shape {} , and los was not Zenith' . format ( hts . shape ))","title":"checkShapes()"},{"location":"reference/#RAiDER.utilFcns.clip_bbox","text":"Clip box to multiple of spacing Source code in RAiDER/utilFcns.py 714 715 716 717 718 719 720 721 def clip_bbox ( bbox , spacing ): \"\"\" Clip box to multiple of spacing \"\"\" return [ np . floor ( bbox [ 0 ] / spacing ) * spacing , np . ceil ( bbox [ 1 ] / spacing ) * spacing , np . floor ( bbox [ 2 ] / spacing ) * spacing , np . ceil ( bbox [ 3 ] / spacing ) * spacing ]","title":"clip_bbox()"},{"location":"reference/#RAiDER.utilFcns.convertLons","text":"Convert lons from 0-360 to -180-180 Source code in RAiDER/utilFcns.py 933 934 935 936 937 938 def convertLons ( inLons ): '''Convert lons from 0-360 to -180-180''' mask = inLons > 180 outLons = inLons outLons [ mask ] = outLons [ mask ] - 360 return outLons","title":"convertLons()"},{"location":"reference/#RAiDER.utilFcns.cosd","text":"Return the cosine of x when x is in degrees. Source code in RAiDER/utilFcns.py 44 45 46 def cosd ( x ): \"\"\"Return the cosine of x when x is in degrees.\"\"\" return np . cos ( np . radians ( x ))","title":"cosd()"},{"location":"reference/#RAiDER.utilFcns.ecef2enu","text":"Convert ECEF xyz to ENU Source code in RAiDER/utilFcns.py 93 94 95 96 97 98 99 100 101 102 def ecef2enu ( xyz , lat , lon , height ): '''Convert ECEF xyz to ENU''' x , y , z = xyz [ ... , 0 ], xyz [ ... , 1 ], xyz [ ... , 2 ] t = cosd ( lon ) * x + sind ( lon ) * y e = - sind ( lon ) * x + cosd ( lon ) * y n = - sind ( lat ) * t + cosd ( lat ) * z u = cosd ( lat ) * t + sind ( lat ) * z return np . stack (( e , n , u ), axis =- 1 )","title":"ecef2enu()"},{"location":"reference/#RAiDER.utilFcns.enu2ecef","text":"float target east ENU coordinate (meters) float target north ENU coordinate (meters) float target up ENU coordinate (meters)","title":"enu2ecef()"},{"location":"reference/#RAiDER.utilFcns.enu2ecef--results","text":"u : float v : float w : float Source code in RAiDER/utilFcns.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def enu2ecef ( east : ndarray , north : ndarray , up : ndarray , lat0 : ndarray , lon0 : ndarray , h0 : ndarray , ): \"\"\" Args: ---------- e1 : float target east ENU coordinate (meters) n1 : float target north ENU coordinate (meters) u1 : float target up ENU coordinate (meters) Results ------- u : float v : float w : float \"\"\" t = cosd ( lat0 ) * up - sind ( lat0 ) * north w = sind ( lat0 ) * up + cosd ( lat0 ) * north u = cosd ( lon0 ) * t - sind ( lon0 ) * east v = sind ( lon0 ) * t + cosd ( lon0 ) * east return np . stack (( u , v , w ), axis =- 1 )","title":"Results"},{"location":"reference/#RAiDER.utilFcns.floorish","text":"Round a value to the lower fractional part Source code in RAiDER/utilFcns.py 34 35 36 def floorish ( val , frac ): '''Round a value to the lower fractional part''' return val - ( val % frac )","title":"floorish()"},{"location":"reference/#RAiDER.utilFcns.getChunkSize","text":"Create a reasonable chunk size Source code in RAiDER/utilFcns.py 979 980 981 982 983 984 985 986 987 988 989 990 def getChunkSize ( in_shape ): '''Create a reasonable chunk size''' minChunkSize = 100 maxChunkSize = 1000 cpu_count = mp . cpu_count () chunkSize = tuple ( max ( min ( maxChunkSize , s // cpu_count ), min ( s , minChunkSize ) ) for s in in_shape ) return chunkSize","title":"getChunkSize()"},{"location":"reference/#RAiDER.utilFcns.getTimeFromFile","text":"Parse a filename to get a date-time Source code in RAiDER/utilFcns.py 480 481 482 483 484 485 486 487 488 489 490 def getTimeFromFile ( filename ): ''' Parse a filename to get a date-time ''' fmt = '%Y_%m_ %d _T%H_%M_%S' p = re . compile ( r '\\d {4} _\\d {2} _\\d {2} _T\\d {2} _\\d {2} _\\d {2} ' ) try : out = p . search ( filename ) . group () return datetime . strptime ( out , fmt ) except BaseException : # TODO: Which error(s)? raise RuntimeError ( 'The filename for {} does not include a datetime in the correct format' . format ( filename ))","title":"getTimeFromFile()"},{"location":"reference/#RAiDER.utilFcns.get_file_and_band","text":"Support file;bandnum as input for filename strings Source code in RAiDER/utilFcns.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 def get_file_and_band ( filestr ): \"\"\" Support file;bandnum as input for filename strings \"\"\" parts = filestr . split ( \";\" ) # Defaults to first band if no bandnum is provided if len ( parts ) == 1 : return filestr . strip (), 1 elif len ( parts ) == 2 : return parts [ 0 ] . strip (), int ( parts [ 1 ] . strip ()) else : raise ValueError ( f \"Cannot interpret { filestr } as valid filename\" )","title":"get_file_and_band()"},{"location":"reference/#RAiDER.utilFcns.nodataToNan","text":"Setting values to nan as needed Source code in RAiDER/utilFcns.py 181 182 183 184 185 186 187 188 def nodataToNan ( inarr , listofvals ): \"\"\" Setting values to nan as needed \"\"\" inarr = inarr . astype ( float ) # nans cannot be integers (i.e. in DEM) for val in listofvals : if val is not None : inarr [ inarr == val ] = np . nan","title":"nodataToNan()"},{"location":"reference/#RAiDER.utilFcns.padLower","text":"add a layer of data below the lowest current z-level at height zmin Source code in RAiDER/utilFcns.py 370 371 372 373 374 375 def padLower ( invar ): ''' add a layer of data below the lowest current z-level at height zmin ''' new_var = _least_nonzero ( invar ) return np . concatenate (( new_var [:, :, np . newaxis ], invar ), axis = 2 )","title":"padLower()"},{"location":"reference/#RAiDER.utilFcns.projectDelays","text":"Project zenith delays to LOS Source code in RAiDER/utilFcns.py 29 30 31 def projectDelays ( delay , inc ): '''Project zenith delays to LOS''' return delay / cosd ( inc )","title":"projectDelays()"},{"location":"reference/#RAiDER.utilFcns.read_hgt_file","text":"Read height data from a comma-delimited file Source code in RAiDER/utilFcns.py 417 418 419 420 421 422 423 def read_hgt_file ( filename ): ''' Read height data from a comma-delimited file ''' data = pd . read_csv ( filename ) hgts = data [ 'Hgt_m' ] . values return hgts","title":"read_hgt_file()"},{"location":"reference/#RAiDER.utilFcns.requests_retry_session","text":"https://www.peterbe.com/plog/best-practice-with-retries-with-requests Source code in RAiDER/utilFcns.py 724 725 726 727 728 729 730 731 732 733 734 735 736 def requests_retry_session ( retries = 10 , session = None ): \"\"\" https://www.peterbe.com/plog/best-practice-with-retries-with-requests \"\"\" import requests from requests.adapters import HTTPAdapter from requests.packages.urllib3.util.retry import Retry # add a retry strategy; https://findwork.dev/blog/advanced-usage-python-requests-timeouts-retries-hooks/ session = session or requests . Session () retry = Retry ( total = retries , read = retries , connect = retries , backoff_factor = 0.3 , status_forcelist = list ( range ( 429 , 505 ))) adapter = HTTPAdapter ( max_retries = retry ) session . mount ( 'http://' , adapter ) session . mount ( 'https://' , adapter ) return session","title":"requests_retry_session()"},{"location":"reference/#RAiDER.utilFcns.rio_extents","text":"Get a bounding box in SNWE from a rasterio profile Source code in RAiDER/utilFcns.py 129 130 131 132 133 134 135 136 137 138 139 def rio_extents ( profile ): \"\"\" Get a bounding box in SNWE from a rasterio profile \"\"\" gt = profile [ \"transform\" ] . to_gdal () xSize = profile [ \"width\" ] ySize = profile [ \"height\" ] if profile [ \"crs\" ] is None or not gt : raise AttributeError ( 'Profile does not contain geotransform information' ) W , E = gt [ 0 ], gt [ 0 ] + ( xSize - 1 ) * gt [ 1 ] + ( ySize - 1 ) * gt [ 2 ] N , S = gt [ 3 ], gt [ 3 ] + ( xSize - 1 ) * gt [ 4 ] + ( ySize - 1 ) * gt [ 5 ] return S , N , W , E","title":"rio_extents()"},{"location":"reference/#RAiDER.utilFcns.robmax","text":"Get the minimum of an array, accounting for empty lists Source code in RAiDER/utilFcns.py 328 329 330 331 332 333 334 335 def robmax ( a ): ''' Get the minimum of an array, accounting for empty lists ''' try : return np . nanmax ( a ) except ValueError : return 'N/A'","title":"robmax()"},{"location":"reference/#RAiDER.utilFcns.robmin","text":"Get the minimum of an array, accounting for empty lists Source code in RAiDER/utilFcns.py 318 319 320 321 322 323 324 325 def robmin ( a ): ''' Get the minimum of an array, accounting for empty lists ''' try : return np . nanmin ( a ) except ValueError : return 'N/A'","title":"robmin()"},{"location":"reference/#RAiDER.utilFcns.round_time","text":"Round a datetime object to any time lapse in seconds dt: datetime.datetime object roundTo: Closest number of seconds to round to, default 1 minute. Source: https://stackoverflow.com/questions/3463930/how-to-round-the-minute-of-a-datetime-object/10854034#10854034 Source code in RAiDER/utilFcns.py 426 427 428 429 430 431 432 433 434 435 def round_time ( dt , roundTo = 60 ): ''' Round a datetime object to any time lapse in seconds dt: datetime.datetime object roundTo: Closest number of seconds to round to, default 1 minute. Source: https://stackoverflow.com/questions/3463930/how-to-round-the-minute-of-a-datetime-object/10854034#10854034 ''' seconds = ( dt . replace ( tzinfo = None ) - dt . min ) . seconds rounding = ( seconds + roundTo / 2 ) // roundTo * roundTo return dt + timedelta ( 0 , rounding - seconds , - dt . microsecond )","title":"round_time()"},{"location":"reference/#RAiDER.utilFcns.sind","text":"Return the sine of x when x is in degrees. Source code in RAiDER/utilFcns.py 39 40 41 def sind ( x ): \"\"\"Return the sine of x when x is in degrees.\"\"\" return np . sin ( np . radians ( x ))","title":"sind()"},{"location":"reference/#RAiDER.utilFcns.transform_bbox","text":"Transform bbox to lat/lon or another CRS for use with rest of workflow Source code in RAiDER/utilFcns.py 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 def transform_bbox ( wesn , dest_crs = 4326 , src_crs = 4326 , margin = 100. ): \"\"\" Transform bbox to lat/lon or another CRS for use with rest of workflow Returns: SNWE \"\"\" # TODO - Handle dateline crossing if isinstance ( src_crs , int ): src_crs = pyproj . CRS . from_epsg ( src_crs ) elif isinstance ( src_crs , str ): src_crs = pyproj . CRS ( src_crs ) # Handle margin for input bbox in degrees if src_crs . axis_info [ 0 ] . unit_name == \"degree\" : margin = margin / 1.0e5 if isinstance ( dest_crs , int ): dest_crs = pyproj . CRS . from_epsg ( dest_crs ) elif isinstance ( dest_crs , str ): dest_crs = pyproj . CRS ( dest_crs ) # If dest_crs is same as src_crs if dest_crs == src_crs : snwe = [ wesn [ 2 ], wesn [ 3 ], wesn [ 0 ], wesn [ 1 ]] return snwe T = Transformer . from_crs ( src_crs , dest_crs , always_xy = True ) xs = np . linspace ( wesn [ 0 ] - margin , wesn [ 1 ] + margin , num = 11 ) ys = np . linspace ( wesn [ 2 ] - margin , wesn [ 3 ] + margin , num = 11 ) X , Y = np . meshgrid ( xs , ys ) # Transform to lat/lon xx , yy = T . transform ( X , Y ) # query_area convention snwe = [ np . nanmin ( yy ), np . nanmax ( yy ), np . nanmin ( xx ), np . nanmax ( xx )] return snwe","title":"transform_bbox()"},{"location":"reference/#RAiDER.utilFcns.transform_coords","text":"Transform coordinates from proj1 to proj2 (can be EPSG or crs from proj). e.g. x, y = transform_coords(4326, 4087, lon, lat) Source code in RAiDER/utilFcns.py 1073 1074 1075 1076 1077 1078 1079 def transform_coords ( proj1 , proj2 , x , y ): \"\"\" Transform coordinates from proj1 to proj2 (can be EPSG or crs from proj). e.g. x, y = transform_coords(4326, 4087, lon, lat) \"\"\" transformer = Transformer . from_crs ( proj1 , proj2 , always_xy = True ) return transformer . transform ( x , y )","title":"transform_coords()"},{"location":"reference/#RAiDER.utilFcns.write2NETCDF4core","text":"The abstract/modular netcdf writer that can be called by a wrapper function to write data to a NETCDF4 file that can be accessed by external programs. The point of doing this is to alleviate some of the memory load of keeping the full model in memory and make it easier to scale up the program. Source code in RAiDER/utilFcns.py 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 def write2NETCDF4core ( nc_outfile , dimension_dict , dataset_dict , tran , mapping_name = 'WGS84' ): ''' The abstract/modular netcdf writer that can be called by a wrapper function to write data to a NETCDF4 file that can be accessed by external programs. The point of doing this is to alleviate some of the memory load of keeping the full model in memory and make it easier to scale up the program. ''' from osgeo import osr if mapping_name == 'WGS84' : epsg = 4326 crs = pyproj . CRS . from_epsg ( epsg ) grid_mapping = 'WGS84' # need to set this as an attribute for the image variables else : crs = pyproj . CRS . from_wkt ( mapping_name ) grid_mapping = 'CRS' datatype = np . dtype ( 'S1' ) dimensions = () var = nc_outfile . createVariable ( grid_mapping , datatype , dimensions , fill_value = None ) # variable made, now add attributes for k , v in crs . to_cf () . items (): var . setncattr ( k , v ) var . setncattr ( 'GeoTransform' , ' ' . join ( str ( x ) for x in tran )) # note this has pixel size in it - set explicitly above for dim in dimension_dict : nc_outfile . createDimension ( dim , dimension_dict [ dim ][ 'length' ]) varname = dimension_dict [ dim ][ 'varname' ] datatype = dimension_dict [ dim ][ 'datatype' ] dimensions = dimension_dict [ dim ][ 'dimensions' ] FillValue = dimension_dict [ dim ][ 'FillValue' ] var = nc_outfile . createVariable ( varname , datatype , dimensions , fill_value = FillValue ) var . setncattr ( 'standard_name' , dimension_dict [ dim ][ 'standard_name' ]) var . setncattr ( 'description' , dimension_dict [ dim ][ 'description' ]) var . setncattr ( 'units' , dimension_dict [ dim ][ 'units' ]) var [:] = dimension_dict [ dim ][ 'dataset' ] . astype ( dimension_dict [ dim ][ 'datatype' ]) for data in dataset_dict : varname = dataset_dict [ data ][ 'varname' ] datatype = dataset_dict [ data ][ 'datatype' ] dimensions = dataset_dict [ data ][ 'dimensions' ] FillValue = dataset_dict [ data ][ 'FillValue' ] ChunkSize = dataset_dict [ data ][ 'ChunkSize' ] var = nc_outfile . createVariable ( varname , datatype , dimensions , fill_value = FillValue , zlib = True , complevel = 2 , shuffle = True , chunksizes = ChunkSize ) # Override with correct name here var . setncattr ( 'grid_mapping' , grid_mapping ) # dataset_dict[data]['grid_mapping']) var . setncattr ( 'standard_name' , dataset_dict [ data ][ 'standard_name' ]) var . setncattr ( 'description' , dataset_dict [ data ][ 'description' ]) if 'units' in dataset_dict [ data ]: var . setncattr ( 'units' , dataset_dict [ data ][ 'units' ]) ndmask = np . isnan ( dataset_dict [ data ][ 'dataset' ]) dataset_dict [ data ][ 'dataset' ][ ndmask ] = FillValue var [:] = dataset_dict [ data ][ 'dataset' ] . astype ( datatype ) return nc_outfile","title":"write2NETCDF4core()"},{"location":"reference/#RAiDER.utilFcns.writeArrayToFile","text":"Write a single-dim array of values to a file Source code in RAiDER/utilFcns.py 276 277 278 279 280 281 282 283 284 def writeArrayToFile ( lats , lons , array , filename , noDataValue =- 9999 ): ''' Write a single-dim array of values to a file ''' array [ np . isnan ( array )] = noDataValue with open ( filename , 'w' ) as f : f . write ( 'Lat,Lon,Hgt_m \\n ' ) for lat , lon , height in zip ( lats , lons , array ): f . write ( ' {} , {} , {} \\n ' . format ( lat , lon , height ))","title":"writeArrayToFile()"},{"location":"reference/#RAiDER.utilFcns.writeArrayToRaster","text":"write a numpy array to a GDAL-readable raster Source code in RAiDER/utilFcns.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 def writeArrayToRaster ( array , filename , noDataValue = 0. , fmt = 'ENVI' , proj = None , gt = None ): ''' write a numpy array to a GDAL-readable raster ''' array_shp = np . shape ( array ) if array . ndim != 2 : raise RuntimeError ( 'writeArrayToRaster: cannot write an array of shape {} to a raster image' . format ( array_shp )) # Data type if \"complex\" in str ( array . dtype ): dtype = np . complex64 elif \"float\" in str ( array . dtype ): dtype = np . float32 else : dtype = np . uint8 # Geotransform trans = None if gt is not None : trans = rasterio . Affine . from_gdal ( * gt ) ## cant write netcdfs with rasterio in a simple way driver = fmt if not fmt == 'nc' else 'GTiff' with rasterio . open ( filename , mode = \"w\" , count = 1 , width = array_shp [ 1 ], height = array_shp [ 0 ], dtype = dtype , crs = proj , nodata = noDataValue , driver = driver , transform = trans ) as dst : dst . write ( array , 1 ) logger . info ( 'Wrote: %s ' , filename ) return","title":"writeArrayToRaster()"},{"location":"reference/#RAiDER.utilFcns.writeDelays","text":"Write the delay numpy arrays to files in the format specified Source code in RAiDER/utilFcns.py 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 def writeDelays ( aoi , wetDelay , hydroDelay , wetFilename , hydroFilename = None , outformat = None , proj = None , gt = None , ndv = 0. ): ''' Write the delay numpy arrays to files in the format specified ''' # Need to consistently handle noDataValues wetDelay [ np . isnan ( wetDelay )] = ndv hydroDelay [ np . isnan ( hydroDelay )] = ndv # Do different things, depending on the type of input if aoi . type () == 'station_file' : try : df = pd . read_csv ( aoi . _filename ) except ValueError : df = pd . read_csv ( aoi . _filename ) df [ 'wetDelay' ] = wetDelay df [ 'hydroDelay' ] = hydroDelay df [ 'totalDelay' ] = wetDelay + hydroDelay df . to_csv ( wetFilename , index = False ) else : writeArrayToRaster ( wetDelay , wetFilename , noDataValue = ndv , fmt = outformat , proj = proj , gt = gt ) writeArrayToRaster ( hydroDelay , hydroFilename , noDataValue = ndv , fmt = outformat , proj = proj , gt = gt )","title":"writeDelays()"},{"location":"reference/#RAiDER.utilFcns.writePnts2HDF5","text":"Write query points to an HDF5 file for storage and access Source code in RAiDER/utilFcns.py 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 def writePnts2HDF5 ( lats , lons , hgts , los , lengths , outName = 'testx.h5' , chunkSize = None , noDataValue = 0. , epsg = 4326 ): ''' Write query points to an HDF5 file for storage and access ''' projname = 'projection' # converts from WGS84 geodetic to WGS84 geocentric t = Transformer . from_crs ( epsg , 4978 , always_xy = True ) checkLOS ( los , np . prod ( lats . shape )) in_shape = lats . shape # create directory if needed os . makedirs ( os . path . abspath ( os . path . dirname ( outName )), exist_ok = True ) # Set up the chunking if chunkSize is None : chunkSize = getChunkSize ( in_shape ) with h5py . File ( outName , 'w' ) as f : f . attrs [ 'Conventions' ] = np . string_ ( \"CF-1.8\" ) x = f . create_dataset ( 'lon' , data = lons , chunks = chunkSize , fillvalue = noDataValue ) y = f . create_dataset ( 'lat' , data = lats , chunks = chunkSize , fillvalue = noDataValue ) z = f . create_dataset ( 'hgt' , data = hgts , chunks = chunkSize , fillvalue = noDataValue ) los = f . create_dataset ( 'LOS' , data = los , chunks = chunkSize + ( 3 ,), fillvalue = noDataValue ) lengths = f . create_dataset ( 'Rays_len' , data = lengths , chunks = x . chunks , fillvalue = noDataValue ) sp_data = np . stack ( t . transform ( lons , lats , hgts ), axis =- 1 ) . astype ( np . float64 ) sp = f . create_dataset ( 'Rays_SP' , data = sp_data , chunks = chunkSize + ( 3 ,), fillvalue = noDataValue ) x . attrs [ 'Shape' ] = in_shape y . attrs [ 'Shape' ] = in_shape z . attrs [ 'Shape' ] = in_shape los . attrs [ 'Shape' ] = in_shape + ( 3 ,) lengths . attrs [ 'Shape' ] = in_shape lengths . attrs [ 'Units' ] = 'm' sp . attrs [ 'Shape' ] = in_shape + ( 3 ,) f . attrs [ 'ChunkSize' ] = chunkSize f . attrs [ 'NoDataValue' ] = noDataValue # CF 1.8 Convention stuff crs = pyproj . CRS . from_epsg ( epsg ) projds = f . create_dataset ( projname , (), dtype = 'i' ) projds [()] = epsg # WGS84 ellipsoid projds . attrs [ 'semi_major_axis' ] = 6378137.0 projds . attrs [ 'inverse_flattening' ] = 298.257223563 projds . attrs [ 'ellipsoid' ] = np . string_ ( \"WGS84\" ) projds . attrs [ 'epsg_code' ] = epsg # TODO - Remove the wkt version after verification projds . attrs [ 'spatial_ref' ] = np . string_ ( crs . to_wkt ( \"WKT1_GDAL\" )) # Geodetic latitude / longitude if epsg == 4326 : # Set up grid mapping projds . attrs [ 'grid_mapping_name' ] = np . string_ ( 'latitude_longitude' ) projds . attrs [ 'longitude_of_prime_meridian' ] = 0.0 x . attrs [ 'standard_name' ] = np . string_ ( \"longitude\" ) x . attrs [ 'units' ] = np . string_ ( \"degrees_east\" ) y . attrs [ 'standard_name' ] = np . string_ ( \"latitude\" ) y . attrs [ 'units' ] = np . string_ ( \"degrees_north\" ) z . attrs [ 'standard_name' ] = np . string_ ( \"height\" ) z . attrs [ 'units' ] = np . string_ ( \"m\" ) else : raise NotImplementedError los . attrs [ 'grid_mapping' ] = np . string_ ( projname ) sp . attrs [ 'grid_mapping' ] = np . string_ ( projname ) lengths . attrs [ 'grid_mapping' ] = np . string_ ( projname ) f . attrs [ 'NumRays' ] = len ( x ) f [ 'Rays_len' ] . attrs [ 'MaxLen' ] = np . nanmax ( lengths )","title":"writePnts2HDF5()"},{"location":"reference/#RAiDER.utilFcns.writeResultsToHDF5","text":"write a 1-D array to a NETCDF5 file Source code in RAiDER/utilFcns.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 def writeResultsToHDF5 ( lats , lons , hgts , wet , hydro , filename , delayType = None ): ''' write a 1-D array to a NETCDF5 file ''' if delayType is None : delayType = \"Zenith\" with h5py . File ( filename , 'w' ) as f : f [ 'lat' ] = lats f [ 'lon' ] = lons f [ 'hgts' ] = hgts f [ 'wetDelay' ] = wet f [ 'hydroDelay' ] = hydro f [ 'wetDelayUnit' ] = \"m\" f [ 'hydroDelayUnit' ] = \"m\" f [ 'hgtsUnit' ] = \"m\" f . attrs [ 'DelayType' ] = delayType","title":"writeResultsToHDF5()"},{"location":"reference/#RAiDER.utilFcns.writeWeatherVars2NETCDF4","text":"By calling the abstract/modular netcdf writer (RAiDER.utilFcns.write2NETCDF4core), write the OpenDAP/PyDAP-retrieved weather model data (GMAO and MERRA-2) to a NETCDF4 file that can be accessed by external programs. The point of doing this is to alleviate some of the memory load of keeping the full model in memory and make it easier to scale up the program. Source code in RAiDER/utilFcns.py 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 def writeWeatherVars2NETCDF4 ( self , lat , lon , h , q , p , t , outName = None , NoDataValue = None , chunk = ( 1 , 91 , 144 ), mapping_name = 'WGS84' ): ''' By calling the abstract/modular netcdf writer (RAiDER.utilFcns.write2NETCDF4core), write the OpenDAP/PyDAP-retrieved weather model data (GMAO and MERRA-2) to a NETCDF4 file that can be accessed by external programs. The point of doing this is to alleviate some of the memory load of keeping the full model in memory and make it easier to scale up the program. ''' import netCDF4 if outName is None : outName = os . path . join ( os . getcwd () + '/weather_files' , self . _Name + datetime . strftime ( self . _time , '_%Y_%m_ %d _T%H_%M_%S' ) + '.nc' ) if NoDataValue is None : NoDataValue = - 9999. self . _time = getTimeFromFile ( outName ) dimidZ , dimidY , dimidX = t . shape chunk_lines_Y = np . min ([ chunk [ 1 ], dimidY ]) chunk_lines_X = np . min ([ chunk [ 2 ], dimidX ]) ChunkSize = [ 1 , chunk_lines_Y , chunk_lines_X ] nc_outfile = netCDF4 . Dataset ( outName , 'w' , clobber = True , format = 'NETCDF4' ) nc_outfile . setncattr ( 'Conventions' , 'CF-1.6' ) nc_outfile . setncattr ( 'datetime' , datetime . strftime ( self . _time , \"%Y_%m_ %d T%H_%M_%S\" )) nc_outfile . setncattr ( 'date_created' , datetime . now () . strftime ( \"%Y_%m_ %d T%H_%M_%S\" )) title = self . _Name + ' weather model data' nc_outfile . setncattr ( 'title' , title ) tran = [ lon [ 0 ], lon [ 1 ] - lon [ 0 ], 0.0 , lat [ 0 ], 0.0 , lat [ 1 ] - lat [ 0 ]] dimension_dict = { 'x' : { 'varname' : 'x' , 'datatype' : np . dtype ( 'float64' ), 'dimensions' : ( 'x' ), 'length' : dimidX , 'FillValue' : None , 'standard_name' : 'longitude' , 'description' : 'longitude' , 'dataset' : lon , 'units' : 'degrees_east' }, 'y' : { 'varname' : 'y' , 'datatype' : np . dtype ( 'float64' ), 'dimensions' : ( 'y' ), 'length' : dimidY , 'FillValue' : None , 'standard_name' : 'latitude' , 'description' : 'latitude' , 'dataset' : lat , 'units' : 'degrees_north' }, 'z' : { 'varname' : 'z' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' ), 'length' : dimidZ , 'FillValue' : None , 'standard_name' : 'model_layers' , 'description' : 'model layers' , 'dataset' : np . arange ( dimidZ ), 'units' : 'layer' } } dataset_dict = { 'h' : { 'varname' : 'H' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'mid_layer_heights' , 'description' : 'mid layer heights' , 'dataset' : h , 'units' : 'm' }, 'q' : { 'varname' : 'QV' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'specific_humidity' , 'description' : 'specific humidity' , 'dataset' : q , 'units' : 'kg kg-1' }, 'p' : { 'varname' : 'PL' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'mid_level_pressure' , 'description' : 'mid level pressure' , 'dataset' : p , 'units' : 'Pa' }, 't' : { 'varname' : 'T' , 'datatype' : np . dtype ( 'float32' ), 'dimensions' : ( 'z' , 'y' , 'x' ), 'grid_mapping' : mapping_name , 'FillValue' : NoDataValue , 'ChunkSize' : ChunkSize , 'standard_name' : 'air_temperature' , 'description' : 'air temperature' , 'dataset' : t , 'units' : 'K' } } nc_outfile = write2NETCDF4core ( nc_outfile , dimension_dict , dataset_dict , tran , mapping_name = 'WGS84' ) nc_outfile . sync () # flush data to disk nc_outfile . close ()","title":"writeWeatherVars2NETCDF4()"},{"location":"reference_td/","text":"tropo_delay RAiDER tropospheric delay calculation This module provides the main RAiDER functionality for calculating tropospheric wet and hydrostatic delays from a weather model. Weather models are accessed as NETCDF files and should have \"wet\" \"hydro\" \"wet_total\" and \"hydro_total\" fields specified. transformPoints ( lats , lons , hgts , old_proj , new_proj ) Transform lat/lon/hgt data to an array of points in a new projection Parameters: Name Type Description Default lats np . ndarray ndarray - WGS-84 latitude (EPSG: 4326) required lons np . ndarray ndarray - ditto for longitude required hgts np . ndarray ndarray - Ellipsoidal height in meters required old_proj CRS CRS - the original projection of the points required new_proj CRS CRS - the new projection in which to return the points required Returns: Name Type Description ndarray np . ndarray the array of query points in the weather model coordinate system (YX) Source code in RAiDER/delay.py 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 def transformPoints ( lats : np . ndarray , lons : np . ndarray , hgts : np . ndarray , old_proj : CRS , new_proj : CRS ) -> np . ndarray : ''' Transform lat/lon/hgt data to an array of points in a new projection Args: lats: ndarray - WGS-84 latitude (EPSG: 4326) lons: ndarray - ditto for longitude hgts: ndarray - Ellipsoidal height in meters old_proj: CRS - the original projection of the points new_proj: CRS - the new projection in which to return the points Returns: ndarray: the array of query points in the weather model coordinate system (YX) ''' t = Transformer . from_crs ( old_proj , new_proj ) # Flags for flipping inputs or outputs if not isinstance ( new_proj , pyproj . CRS ): new_proj = CRS . from_epsg ( new_proj . lstrip ( 'EPSG:' )) if not isinstance ( old_proj , pyproj . CRS ): old_proj = CRS . from_epsg ( old_proj . lstrip ( 'EPSG:' )) in_flip = old_proj . axis_info [ 0 ] . direction out_flip = new_proj . axis_info [ 0 ] . direction if in_flip == 'east' : res = t . transform ( lons , lats , hgts ) else : res = t . transform ( lats , lons , hgts ) if out_flip == 'east' : return np . stack (( res [ 1 ], res [ 0 ], res [ 2 ]), axis =- 1 ) . T else : return np . stack ( res , axis =- 1 ) . T tropo_delay ( dt , weather_model_file , aoi , los , height_levels = None , out_proj = 4326 , cube_spacing_m = None , look_dir = 'right' ) Calculate integrated delays on query points. Parameters: Name Type Description Default dt Datetime - Datetime object for determining when to calculate delays required weather_model_File string - Name of the NETCDF file containing a pre-processed weather model required aoi AOI object - AOI object required los LOS object - LOS object required height_levels List [ float ] list - (optional) list of height levels on which to calculate delays. Only needed for cube generation. None out_proj int | str int,str - (optional) EPSG code for output projection 4326 look_dir str str - (optional) Satellite look direction. Only needed for slant delay calculation 'right' cube_spacing_m int int - (optional) Horizontal spacing in meters when generating cubes None Returns: Type Description xarray Dataset or ndarrays: - wet and hydrostatic delays at the grid nodes / query points. Source code in RAiDER/delay.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def tropo_delay ( dt , weather_model_file : str , aoi , los , height_levels : List [ float ] = None , out_proj : int | str = 4326 , cube_spacing_m : int = None , look_dir : str = 'right' , ): \"\"\" Calculate integrated delays on query points. Args: dt: Datetime - Datetime object for determining when to calculate delays weather_model_File: string - Name of the NETCDF file containing a pre-processed weather model aoi: AOI object - AOI object los: LOS object - LOS object height_levels: list - (optional) list of height levels on which to calculate delays. Only needed for cube generation. out_proj: int,str - (optional) EPSG code for output projection look_dir: str - (optional) Satellite look direction. Only needed for slant delay calculation cube_spacing_m: int - (optional) Horizontal spacing in meters when generating cubes Returns: xarray Dataset *or* ndarrays: - wet and hydrostatic delays at the grid nodes / query points. \"\"\" # get heights if height_levels is None : with xarray . load_dataset ( weather_model_file ) as ds : height_levels = ds . z . values #TODO: expose this as library function ds = _get_delays_on_cube ( dt , weather_model_file , aoi . bounds (), height_levels , los , out_proj = out_proj , cube_spacing_m = cube_spacing_m , look_dir = look_dir ) if ( aoi . type () == 'bounding_box' ) or ( aoi . type () == 'Geocube' ): return ds , None else : # CRS can be an int, str, or CRS object try : out_proj = CRS . from_epsg ( out_proj ) except pyproj . exceptions . CRSError : out_proj = out_proj pnt_proj = CRS . from_epsg ( 4326 ) lats , lons = aoi . readLL () hgts = aoi . readZ () pnts = transformPoints ( lats , lons , hgts , pnt_proj , out_proj ) if pnts . ndim == 3 : pnts = pnts . transpose ( 1 , 2 , 0 ) elif pnts . ndim == 2 : pnts = pnts . T ifWet , ifHydro = getInterpolators ( ds , 'ztd' ) # the cube from get_delays_on_cube calls the total delays 'wet' and 'hydro' wetDelay = ifWet ( pnts ) hydroDelay = ifHydro ( pnts ) # return the delays (ZTD or STD) if los . is_Projected (): los . setTime ( dt ) los . setPoints ( lats , lons , hgts ) wetDelay = los ( wetDelay ) hydroDelay = los ( hydroDelay ) return wetDelay , hydroDelay","title":"Delay Calculation"},{"location":"reference_td/#RAiDER.delay.transformPoints","text":"Transform lat/lon/hgt data to an array of points in a new projection Parameters: Name Type Description Default lats np . ndarray ndarray - WGS-84 latitude (EPSG: 4326) required lons np . ndarray ndarray - ditto for longitude required hgts np . ndarray ndarray - Ellipsoidal height in meters required old_proj CRS CRS - the original projection of the points required new_proj CRS CRS - the new projection in which to return the points required Returns: Name Type Description ndarray np . ndarray the array of query points in the weather model coordinate system (YX) Source code in RAiDER/delay.py 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 def transformPoints ( lats : np . ndarray , lons : np . ndarray , hgts : np . ndarray , old_proj : CRS , new_proj : CRS ) -> np . ndarray : ''' Transform lat/lon/hgt data to an array of points in a new projection Args: lats: ndarray - WGS-84 latitude (EPSG: 4326) lons: ndarray - ditto for longitude hgts: ndarray - Ellipsoidal height in meters old_proj: CRS - the original projection of the points new_proj: CRS - the new projection in which to return the points Returns: ndarray: the array of query points in the weather model coordinate system (YX) ''' t = Transformer . from_crs ( old_proj , new_proj ) # Flags for flipping inputs or outputs if not isinstance ( new_proj , pyproj . CRS ): new_proj = CRS . from_epsg ( new_proj . lstrip ( 'EPSG:' )) if not isinstance ( old_proj , pyproj . CRS ): old_proj = CRS . from_epsg ( old_proj . lstrip ( 'EPSG:' )) in_flip = old_proj . axis_info [ 0 ] . direction out_flip = new_proj . axis_info [ 0 ] . direction if in_flip == 'east' : res = t . transform ( lons , lats , hgts ) else : res = t . transform ( lats , lons , hgts ) if out_flip == 'east' : return np . stack (( res [ 1 ], res [ 0 ], res [ 2 ]), axis =- 1 ) . T else : return np . stack ( res , axis =- 1 ) . T","title":"transformPoints()"},{"location":"reference_td/#RAiDER.delay.tropo_delay","text":"Calculate integrated delays on query points. Parameters: Name Type Description Default dt Datetime - Datetime object for determining when to calculate delays required weather_model_File string - Name of the NETCDF file containing a pre-processed weather model required aoi AOI object - AOI object required los LOS object - LOS object required height_levels List [ float ] list - (optional) list of height levels on which to calculate delays. Only needed for cube generation. None out_proj int | str int,str - (optional) EPSG code for output projection 4326 look_dir str str - (optional) Satellite look direction. Only needed for slant delay calculation 'right' cube_spacing_m int int - (optional) Horizontal spacing in meters when generating cubes None Returns: Type Description xarray Dataset or ndarrays: - wet and hydrostatic delays at the grid nodes / query points. Source code in RAiDER/delay.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def tropo_delay ( dt , weather_model_file : str , aoi , los , height_levels : List [ float ] = None , out_proj : int | str = 4326 , cube_spacing_m : int = None , look_dir : str = 'right' , ): \"\"\" Calculate integrated delays on query points. Args: dt: Datetime - Datetime object for determining when to calculate delays weather_model_File: string - Name of the NETCDF file containing a pre-processed weather model aoi: AOI object - AOI object los: LOS object - LOS object height_levels: list - (optional) list of height levels on which to calculate delays. Only needed for cube generation. out_proj: int,str - (optional) EPSG code for output projection look_dir: str - (optional) Satellite look direction. Only needed for slant delay calculation cube_spacing_m: int - (optional) Horizontal spacing in meters when generating cubes Returns: xarray Dataset *or* ndarrays: - wet and hydrostatic delays at the grid nodes / query points. \"\"\" # get heights if height_levels is None : with xarray . load_dataset ( weather_model_file ) as ds : height_levels = ds . z . values #TODO: expose this as library function ds = _get_delays_on_cube ( dt , weather_model_file , aoi . bounds (), height_levels , los , out_proj = out_proj , cube_spacing_m = cube_spacing_m , look_dir = look_dir ) if ( aoi . type () == 'bounding_box' ) or ( aoi . type () == 'Geocube' ): return ds , None else : # CRS can be an int, str, or CRS object try : out_proj = CRS . from_epsg ( out_proj ) except pyproj . exceptions . CRSError : out_proj = out_proj pnt_proj = CRS . from_epsg ( 4326 ) lats , lons = aoi . readLL () hgts = aoi . readZ () pnts = transformPoints ( lats , lons , hgts , pnt_proj , out_proj ) if pnts . ndim == 3 : pnts = pnts . transpose ( 1 , 2 , 0 ) elif pnts . ndim == 2 : pnts = pnts . T ifWet , ifHydro = getInterpolators ( ds , 'ztd' ) # the cube from get_delays_on_cube calls the total delays 'wet' and 'hydro' wetDelay = ifWet ( pnts ) hydroDelay = ifHydro ( pnts ) # return the delays (ZTD or STD) if los . is_Projected (): los . setTime ( dt ) los . setPoints ( lats , lons , hgts ) wetDelay = los ( wetDelay ) hydroDelay = los ( hydroDelay ) return wetDelay , hydroDelay","title":"tropo_delay()"},{"location":"tutorials/","text":"Tutorials R aytracing A tmospher i c D elay E stimator for R ADAR - RAiDER RAiDER is a package in Python which contains tools to calculate tropospheric corrections for Radar using a raytracing implementation. Its development was funded under the NASA Sea-level Change Team (NSLCT) program, the Earth Surface and Interior (ESI) program, and the NISAR Science Team (NISAR-ST) (NTR-51433). U.S. Government sponsorship acknowledged. Copyright (c) 2019-2022, California Institute of Technology (\"Caltech\"). All rights reserved. THIS IS RESEARCH CODE PROVIDED TO YOU \"AS IS\" WITH NO WARRANTIES OF CORRECTNESS. USE AT YOUR OWN RISK. Getting Started Quick Start To get started, run the following lines in a terminal: conda env create --name RAiDER -c conda-forge raider jupyterlab conda activate RAiDER Then download or clone this repository to your working directory, and run jupyter lab navigate to one of the tutorial notebooks, and open it. Other ways to install Defining Custom Weather Models Tutorials Pandas tutorial for GNSS delay manipulation RAiDER tutorial RAiDER library access in Python Tutorial downloading GNSS tropospheric delays raiderStats tutorial","title":"Overview"},{"location":"tutorials/#tutorials","text":"","title":"Tutorials"},{"location":"tutorials/#raytracing-atmospheric-delay-estimator-for-radar-raider","text":"RAiDER is a package in Python which contains tools to calculate tropospheric corrections for Radar using a raytracing implementation. Its development was funded under the NASA Sea-level Change Team (NSLCT) program, the Earth Surface and Interior (ESI) program, and the NISAR Science Team (NISAR-ST) (NTR-51433). U.S. Government sponsorship acknowledged. Copyright (c) 2019-2022, California Institute of Technology (\"Caltech\"). All rights reserved. THIS IS RESEARCH CODE PROVIDED TO YOU \"AS IS\" WITH NO WARRANTIES OF CORRECTNESS. USE AT YOUR OWN RISK.","title":"Raytracing Atmospheric Delay Estimator for RADAR - RAiDER"},{"location":"tutorials/#getting-started","text":"","title":"Getting Started"},{"location":"tutorials/#quick-start","text":"To get started, run the following lines in a terminal: conda env create --name RAiDER -c conda-forge raider jupyterlab conda activate RAiDER Then download or clone this repository to your working directory, and run jupyter lab navigate to one of the tutorial notebooks, and open it. Other ways to install Defining Custom Weather Models","title":"Quick Start"},{"location":"tutorials/#tutorials_1","text":"Pandas tutorial for GNSS delay manipulation RAiDER tutorial RAiDER library access in Python Tutorial downloading GNSS tropospheric delays raiderStats tutorial","title":"Tutorials"}]}